---
title         : "You've Gotta Keep em' Separated: Examining the Efficacy of Proximal Remedies for Method Variance"
shorttitle    : "PROXIMAL METHOD VARIANCE REMEDIES"

author:
- address: 906 East 1st. St., Thibodaux, LA 70301
  affiliation: '1'
  corresponding: yes
  email: chris_castille@mac.com
  name: Christopher M. Castille
- affiliation: '2'
  name: Wayne S. Crawford
- affilitation: '3'
  name: Marcia J. Simmering
affiliation:
- id: '1'
  institution: Nicholls State University
- id: '2'
  institution: University of Texas at Arlington
- id: '3'
  institution: Louisiana Tech University

output            : papaja::apa6_word

class: "man"
figsintext: no
figurelist: no
footnotelist: no
tablelist: no

lang: "english"
lineno: no

author_note: "A previous version of this manuscript received both a Research Methods Division Best Paper Award and Best Conference Paper Award from the 2017 annual conference for the Southern Management Association. We'd like to thank several individuals for providing friendly reviewer feedback on the development of this manuscript, including Hettie Richardson, Larry Williams, Alyssa McGonagle, and Chris Rosen. Additionally, we'd like to thank members of the lavaan user group, specifically Terrence Jorgenson and Ed Rigon for providing helpful feedback as we were troubleshooting the analytics. All data and code have been made publicly available for re-analysis at https://osf.io/wdskv/."

abstract: "Scholars have argued that method factors, such as shared measurement context effects, bias estimates of covariation for same-source and single time-point investigations. To address such method bias, scholars have proposed procedural remedies for such proximal causes of method variance. Recommended remedies include (1) presenting participants with a cover story to disguise the purpose of the survey (which addresses respondents' ability to produce data consistent with researchers' hypotheses), (2) randomizing item and (3) scale presentation around filler scales (which addresses item and scale context effects), and (4) introducing a brief temporal separation (which addresses respondents' momentary mood). There are no studies that experimentaally examine whether data gathered using these remedies produce results that differ from those where remedies are not utilized. Here, we present the findings from three field experiments utilizing the same measurement model (proactive personality, in-role behavior, and OCB) but either gathering data via bundling remedies (study 1), using single remedies (study 2), or inserting a one-week separation between predictor and outcome data collection. Our results demonstrate that these proximal remedies do, indeed, produce different results. Furthermore, in several instances correlations were rendered statistically non-significant when remedies were applied."    

header-includes:
  - \raggedbottom
  - \usepackage{caption}
  
keywords: "method variance, procedural remedies, statistical remedies"

wordcount: X

bibliography: r-references.bib
---
```{r Load Packages, include=FALSE}
packages <- c("papaja", "apaTables","citr","tidyverse","haven","readr","MOTE","frequencies","mice","psych","car","boot","lavaan","semTools","semPlot","searcher","fastDummies","gtools","rmdrive","packrat","scales","kableExtra","huxtable")
lapply(packages, library, character.only = TRUE)
set.seed(19)
```
# Introduction

Few methodological problems have been discussed more frequently than the presence and impact of common method variance [CMV; @PodsakoffCommonmethodbiases2003; @PodsakoffSourcesMethodBias2012; @spectorNewPerspectiveMethod2017]. Though long debated over the past several decades [e.g., @CampbellConvergentDiscriminantValidation1959; @LanceUseindependentmeasures2015; @RichardsonTaleThreePerspectives2009; @SpectorMethodVarianceOrganizational2006], the problem of CMV is often viewed as a serious one in the organizational sciences, where researchers often rely upon observations made using sources believed to share common method variance [i.e., variation in observations that is attributable to a common cause, such mood held by respondents; see @PodsakoffCommonmethodbiases2003]. To guard against CMV, procedural remedies, such as separating the collection of predictor and criterion measures in some fashion [@PodsakoffCommonmethodbiases2003; @PodsakoffSourcesMethodBias2012], have been proposed. The proposed solutions (e.g., use a cover story to disguise the purpose of a study, counterbalance item presentation, and separate measurement by an interval of time) and routinely encouraged by journal editors [e.g., @AshkanasySubmittingyourmanuscript2008].

While it has been suggested that procedural remedies control for CMV [@PodsakoffCommonmethodbiases2003; @PodsakoffSourcesMethodBias2012], we know of no research that has experimentally demonstrated this to be the case. Knowing whether procedural remedies control for CMV is important for scholars who are planning their research and hope to produce findings that are unbiased by method variance. Therefore, we set out to test the effectiveness of proximal remedies for CMV. Furthermore, by testing procedural remedies for method variance, we have the opportunity to speak to the larger debate regarding the nature of method variance (i.e., whether or not it exists and what kinds of effects method variance has).

In three studies, we contribute to the literature by experimentally examining the effects of using proximal remedies for method variance in single-source designs. We examine several proximal remedies for method variance for single-source designs because these designs are likely to be the most prevalent in the organizational sciences due to lower cost and reduced administrative burden. Single-source designs should be ripe for method variance because the item responses processes may not be addressed unless a remedy is applied [see @PodsakoffCommonmethodbiases2003; @PodsakoffSourcesMethodBias2012]. In study 1, we bundle a series of proximal remedies for method variance (i.e., cover story, item and scale randomization, using a filler scale) to increase the separation (and therefore the biasing effects of CMV) between a focal predictor measure (i.e., proactive personality) and a set of outcomes (i.e., in-role behavior, organizational citizenship). In study 2, we examine the unique effects of each of these procedural remedies in a follow-up experiment. In study 3, we examine the efficacy of a one-week temporal separation of measurement between our predictor and outcomes [@spectorNewPerspectiveMethod2017]. 

# Theoretical Overview of Method Variance

Differing views exist regarding the terms 'method,' 'method variance,' and 'method bias. @PodsakoffSourcesMethodBias2012 argued that method encompasses several abstract elements (e.g., taking a paper-and-pencil instrument, responding using Likert scales, characteristics of the examiner, mood, social desirability) and that when these elements are shared across methods or measures in the same investigation there will be a convergence resulting in bias. According to Podsakoff and colleagues' view, method variance is synonymous with method bias such that if a methodological aspect is shared across studies (e.g., self-reported data are obtained on both predictors and outcomes) then the study is biased. In other words, studies that contain overlapping methods (e.g., self-reports of predictor and outcomes) are biased, whereas studies that contain procedural remedies for method variance, or include design features intended to reduce the number of overlapping elements (e.g., self-reports of predictors and other-reports on outcomes) are superior and assumed to be either less biased or unbiased. We believe this view is the one that is more commonly held across the social sciences, particularly among editors and reviewers of major academic journals.

By contrast to Podsakoff and colleagues' view, @LanceMethodEffectsMeasurement2010 defined 'method' as alternative means of enumerating observations (e.g., self- vs. other-report, explicit vs. implicit assessment) to indicate standings on latent traits. This view focuses on the impact of a methodological aspect of a study on particular observation(s) (e.g., self-reports resulting in overly positive reports of workplace behavior). Lance and colleagues' view implies that the impact of these aspects may (or may not) be so strong as to cause parameter estimates to differ systematically. While this view has been criticized as omitting common rater sources of method variance (e.g., mood), such sources have been included in theoretical elaborations by other scholars. Specifically, @spectorNewPerspectiveMethod2017 suggested that method variance can be unique to a set of observations within a data set (e.g., halo contaminates self-appraisals of performance), specific to a set of observations within a data set (e.g., halo contaminates supervisor appraisals of employee performance, but not subordinate variables), and may or may not rise to such levels as to cause differences in latent construct covariances (i.e., the substantive correlations of interest may or may not be biased by methodological elements). @spectorNewPerspectiveMethod2017 encourage researchers to adopt a measure-centric view of method variance and clearly articulate the cause of method variance likely to be at play with regard to one's measures.

In our manuscript, we examine how parameter estimates for a measure of proactive personality, in-role behavior, and organizational citizenship, vary across conditions where proximal remedies have and have not been applied. Our remedies were aimed at reducing method variance believed to inflate correlations linking proactive personality (predictor) to in-role and organizational citizenship behavior (outcomes). Single-source designs should be ripe for method variance because the cognitive processes of retrieval (i.e., remembering responses to previously answered items rather than the whole of one's experience), judgment (i.e., falsely judging that one has effectively retrieved relevant memories), and in some instances reporting (i.e., modifying one's responses to be consistent) may not be addressed unless a remedy is applied [see @PodsakoffCommonmethodbiases2003; @PodsakoffSourcesMethodBias2012]. We decided to study  proactive personality-workplace behavior relationships for two reasons that are important for both organizational scholars and method variance testing in general. First, these relationships are often described in the literature [e.g.,@FullerJr.Changedrivennature2009; @GrantReversingExtravertedLeadership2011], suggesting that the relationship is important to a broad audience of organizational scholars. Second, prior research suggests that these relationships may be a function of method variance [e.g., @FullerJr.Changedrivennature2009]. A meta-analysis by @FullerJr.Changedrivennature2009 demonstrated that these relationships could be inflated by common method variance, particularly when observations are gathered using the same source rather than multiple sources. Specifically, they observed that the relationships between proactive personality and workplace behaviors were inflated  by between 129% and 308% when designs used a common rather than distinct sources. Though these percentages likely misstate the impact of method variance by failing to account for uncommon method variance [e.g., @LanceUseindependentmeasures2015; @spectorNewPerspectiveMethod2017], they nevertheless suggest that methodological factors could play a role. To resolve these disparities, we tested the same measurement model in our three studies, examining the effects of various hypothesized common method factors (e.g., momentary mood) and uncommon method factors (e.g., negative item wording). Figure 1 illustrates the substantive and methodological factors of interest.

# ---------------
# Insert Figure 1 about here
# ---------------

## Common Rater Effects

Common rater effects are any nuisance covariation introduced by gathering observations on or from the same rater [@PodsakoffCommonmethodbiases2003]. Of the many common rater effects, there are four that are particularly relevant for same-source single time-point investigations: consistency motifs, implicit theories, mood states, and trait affectivity factors. Consistency motifs refer to the propensity for respondents to maintain consistency in their responses to questions, in which the desire to appear rational should affect response reporting [@PodsakoffCommonmethodbiases2003]. Research indicates that consistency motifs contaminate measures and may even bias results [@HarrisonContextCognitionCommon1996]. Similarly, implicit theories refer to respondents’ beliefs about the covariation among particular traits, behaviors, and or outcomes that may not accurately reflect reality [@SternbergPeopleconceptionsintelligence1981]. These biases are believed to affect judgments of response appropriateness [@PodsakoffCommonmethodbiases2003]. In research settings, if participants are aware of the study purpose, it could equip them with an understanding of the hypotheses, inclining them to respond in a manner that is consistent with these hypotheses. To address consistency motifs and implicit theories, researchers have used cover stories [@PodsakoffCommonmethodbiases2003], which are designed to both disguise the hypotheses under investigation and dispel implicit theories held by the participants [@PodsakoffCommonmethodbiases2003]. 

Different from consistency motifs and implicit theories are the interrelated notions of mood states and trait affectivity, both of which may be a common source of method variance. Mood states refer to respondents’ momentary or brief mood state, which can influence the contents of what is recalled from memory by priming affectively similar material stored in memory [cf. @BlaneyAffectmemoryreview1986; @ParrottMoodmemorynatural1990]. Sometimes this has a desirable effect, such as when mood-congruence facilitates recall [@BowerMoodmemory1981; @IsenPositiveaffectfactor1991]. Mood state should also be related to general affectivity, which is a respondent's propensity to view themselves or their environments in positive or negative ways [@PodsakoffCommonmethodbiases2003]. Similar to mood, trait affectivity should exert comparable effects on the item response process and may explain the effect of momentary mood state on the item response process. @BriefShouldnegativeaffectivity1988 observed that negative affectivity contaminates observations of stress, job and life satisfaction, and depression, resulting in biased estimates of covariation [see also @ChenNegativeaffectivityunderlying1991, @Jeximpactnegativeaffectivity1996, and @Williamsalternativeapproachmethod1994]. To address the role of mood states, researchers recommend separating measures temporally to increase the amount of time between when observations of predictor and criterion variables are made [@PodsakoffCommonmethodbiases2003; @PodsakoffSourcesMethodBias2012]. Though it involves separation by time, this remedy addresses a proximal cause of method variance (i.e., momentary mood state). Despite wide use of these procedural remedies, there is no experimental evidence that they are effective.

## Relevant Context Effects

Context effects refer to any nuisance covariation among observations caused by an item or scale’s location or relationship to other items or scales in a survey (e.g., item/scale priming and embeddedness effects). @PodsakoffCommonmethodbiases2003 proposed that item context can affect retrieval and judgments of response appropriateness. @TourangeauAttitudestructurebelief1991 demonstrated that participants respond more quickly to similar items placed closer together rather than further apart. Tourangeau et al.’s (1991) findings suggest that item inter-correlations may be a function of the consistent ordering of items within a particular instrument (i.e., serial-order effects). Essentially, if responses made to previous items or scales are maintained in short-term memory, these responses may influence responses to future items or scales. @SteinbergContextserialordereffects1994 found that one item in a 20-item scale became slightly more discriminating when it was presented later in the scale rather than as the first item, which suggests that a systematic item context effect can artificially increase the systematic variance explained by a latent trait. Furthermore, when negative scales were presented first, estimated inter-scale correlations were much higher, suggesting the presence of scale embeddedness effects. 

To address item priming and item embeddedness effects, researchers have proposed counterbalancing or randomizing the order of items or scales within a survey or including one or more filler scales in-between the administration of a predictor and outcome scale [@PodsakoffCommonmethodbiases2003]. Randomizing items within a scale essentially eliminates the influence of a serial item-order effect that would apply to a study that presents all items within a scale in the same order. Randomizing scale presentation within a survey should have the same influence but at the level of the scale, thus eliminating a serial scale-serial-order effect that would apply to a study that presents all scales within a survey in the same order. Lastly, introducing filler scales makes it more difficult to recall responses to prior scales, reducing the influence of recall effects. No research has examined these remedies.

# General Purpose

Across three studies, we examine the effectiveness of proximal remedies for method variance. Procedural remedies for method variance are tactics designed to reduce any nuisance covariation introduced by gathering observations on (or from) a common source [@PodsakoffCommonmethodbiases2003]. Of the many remedies proposed by @PodsakoffCommonmethodbiases2003, those that are relevant for same-source single time-point are our focus. These remedies include such methodological aspects as (i) using a cover story to introduce a psychological separation of measurement to eliminate hypothesis guessing effects, (ii) randomizing items and scales to eliminate any method effects related to item and scale locations within a survey such as item priming effects and item/scale embeddedness effects [i.e., measurement context effects; see @HarrisonContextCognitionCommon1996; @SteinbergContextserialordereffects1994; @WeinbergerItemsContextEffects2006], (iii) introducing a filler scale in-between the predictor and outcome measures within a survey to eliminate recall and consistency motif effects [see @SalancikExaminationNeedSatisfactionModels1977], and (iv) introducing a temporal separation of measurement, such as a one-week separation of measurement, between the collection of predictor and criterion measures. Collectively, we refer to these remedies as proximal remedies for method variance. 

Given that proximal causes of method variance inflate the correlation among same-source single-time-point observations [@PodsakoffCommonmethodbiases2003], applying a proximal remedy to target these proximal causes of method variance should result in lower correlations between the substantive factors of interest. Therefore, each study tests the following hypothesis:

*Hypothesis 1: Data gathered using proximal remedies (i.e., using of a cover story, randomizing item- and scale-order, using a filler scale, separating collection of predictor data from criterion data by one week) will result in lower correlations between substantive factors of interest.*

Pyschometrically, when method variance is common to a measurement model, latent factor approaches will mistake method variance for trait variance [@@PodsakoffCommonmethodbiases2003]. However, when a remedy has been applied, such method variance should not be correlated with the latent trait in question but rather be forced into the error terms. For instance, maintaining a consistent item-order in one's survey introduces a serial-item order effect that will inflate the factor loadings for a measurement model. Conversely, randomizing the presentation of items such that participants receive unique orderings of items introduces unsystematic variation into the group-level measurement (i.e., item-order effects are largely confined to the item residuals). As the reliability of observed data can be decomposed into modeled sources of variance, both method and non-method [@WilliamsMethodVarianceMarker2010], we can examine how methodological conditions influence estimates of substantive variance across conditions. As proximal causes of method variance operate within non-remedied data, studies that do not use proximal remedies should result in an inflated impression of the amount of substantive variance that is captured by a measurement model [@@PodsakoffCommonmethodbiases2003]. In other words, in comparing the amount of substantive and method variance captured by a measurement model across studies that do and do not apply remedies, non-remedied data should reflect more (and remedied data should reflect less) substantive variance. Therefore, each study tests the following hypothesis:

*Hypothesis 2: Data gathered using proximal remedies (i.e., using of a cover story, randomizing item- and scale-order, using a filler scale) will result in lower estimates of substantive (i.e., non-method) variance for the substantive factors of interest.*

Affective traits should (and appear to) bare some substantive relationship with general descriptions of workplace behavior, such as in-role behavior and OCB [e.g., via behavioral approach and inhibition systems; see [@kaplanRolePositiveNegative2009]. Therefore, viewing affective tendencies as methodological contaminants per se, and seeking to control them statitistically, would be inadvisable [@spectorWhyNegativeAffectivity2000]. [^1] However, momentary mood, though a possible cause of workplace behavior within a specific performance episode [@bealEpisodicProcessModel2005], should be independent from general descriptions of workplace behavior. Any relationships linking momentary mood to substantive item indicators could, rather, be interpreted as a methodological contaminant [@spectorNewPerspectiveMethod2017; @WilliamsMethodVarianceMarker2010]. Therefore, we examined the contaminating role of mood across all study conditions but in study 3 examined the efficacy of a procedural remedy (i.e., gathering predictor and criterion data with one-week separating these measurements) to address these effects.

[^1]: We'd like to thank Chris Rosen for pointing this out in an earlier version of our work.

# Study 1 - Bundling Proximal Remedies 

In study 1, we examined the efficacy of bundling proximal remedies for method variance. More specifically, we examined how bundling a cover story, randomizing items within scales, and randomizing the presentation of scales around a filler scale affects the parameter estimates for our substantive measures. Participants were randomly assigned to a condition where observations were obtained with or without these remedies. For our non-remedied (i.e., control) condition, an online self-reported survey was designed such that all items and scales appeared in the same order, but were separated by different web pages. This was a conventional survey design. For our remedied condition, we used a cover story that was designed to both blind participants to the purpose of our study and dispel implicit theories regarding the substantive intention guiding a study such as ours: test for positive relationships linking proactive personality to workplace behaviors. Also, the item- and scale-order were randomized around a series of filler scales. These filler scales measured other potential sources of method variance that were not addressed by our procedural remedies (i.e., positive and negative affectivity, and momentary mood) and a variety of other scales commonly used to capture method effects (i.e., attitudes toward the color blue, preference for brand label clothing, seeing a survey as valuable, and seeing a survey as enjoyable). 

# Methods - Study 1

## Sample and Procedure
```{r Load Study 1 Data, include=FALSE}
#Load data
data <- read_sav("Data/Study 1.sav")

#Count of individuals who agree to participate (Q42=1 OR Q44=1). Note: 1 is control/non-remedied and 2 is treatment/experimental/remedied.
N1 <- as.numeric(freq_vect(data$Q42)[1,"Count"])
N2 <- as.numeric(freq_vect(data$Q44)[1,"Count"])
Nall <- sum(N1,N2)

#Subset in those who agreed to participate and passed the manipulation check question (Q42=1 OR Q44=1 & correctly answered attention checks).
Agree <- subset(data, Q42 == 1 & Q47 == 1 | Q44 == 1 & Q79 == 2)

#Get success rates (SR) for responding correctly across both conditions.
n1 <- as.numeric(freq_vect(Agree$Q42)[1,"Count"])
n2 <- as.numeric(freq_vect(Agree$Q42)[2,"Count"])
n <- n1+n2
sr1 <- percent(n1/N1)
sr2 <- percent(n2/N2)

#Filter in attentive responders.
Inattent <- subset(Agree, Q26_11 == 1 | Q105_11 ==  1)

#Calculate sum of inattentive responders (iar) that have been screened out of the Inattent dataset for either incorrectly or failing to answer "strongly disagree" to an attention check question. 
iar1 <- freq_vect(Agree$Q26_11)
iar1[3,1] <- "2" 
iar1[4,1] <- "2" 
iar1[5,1] <- "2" 
iar1 <- freq_vect(iar1$data)
iar1 <- as.numeric(iar1[2,"Count"])
iar2 <- freq_vect(Agree$Q105_11)
iar2[2,1] <- "2" 
iar2[3,1] <- "2" 
iar2 <- freq_vect(iar2$data)
iar2 <- as.numeric(iar2[2,"Count"])
iar <- iar1+iar2+(nrow(Agree) - (sum(iar1,iar2)) - nrow(Inattent))

#Delete cases who did not complete the demographic questionnaire and barely completed any tests.
Inattent <- Inattent[-c(1, 275, 276, 277),]

#Setup demographics.
#Rename variables.
Inattent$COND <- Inattent$Q42 
Inattent$AGE <- Inattent$Q33
Inattent$RACE <- Inattent$Q34
Inattent$GENDER <- Inattent$Q35
Inattent$EDUCAT <- Inattent$Q36.0
Inattent$EMPLOYED <- Inattent$Q37
Inattent$JOBTENURE <- Inattent$Q38
Inattent$USREGION <- Inattent$Q40
Inattent$PROFESSION <- Inattent$Q39

#Then, combine data from separate surveys in order to form the final cleaned and well-structured dataset. Note: there were two surveys setup on Qualtrics, hence why data needs to be combined. You'll see redundant items moving forward.
##Convert NAs to 0 to allow merging. Later, you'll need to reasign NAs to conduct a missing data analysis. 
Inattent[is.na(Inattent)] <- 0

#Proactive personality
Inattent$PP1	<-	Inattent$Q26_1	+Inattent$Q105_1 #Wherever I have been, I have been a powerful force for change.
Inattent$PP2	<-	Inattent$Q26_2	+Inattent$Q105_2 #I am constantly on the lookout for new ways to improve my life.
Inattent$PP3	<-	Inattent$Q26_3	+Inattent$Q105_3 #If I see something I don't like, I fix it.
Inattent$PP4	<-	Inattent$Q26_4	+Inattent$Q105_4 #I am always looking for better ways to do things.
Inattent$PP5	<-	Inattent$Q26_5	+Inattent$Q105_5 #No matter what the odds, if I believe in something, I will make it happen.
Inattent$PP6	<-	Inattent$Q26_6	+Inattent$Q105_6 #Nothing is more exciting than seeing my ideas turn into reality.
Inattent$PP7	<-	Inattent$Q26_7	+Inattent$Q105_7 #I love being a champion for my ideas, even against others' opposition.
Inattent$PP8	<-	Inattent$Q26_8	+Inattent$Q105_8 #I excel at identifying opportunities.
Inattent$PP9	<-	Inattent$Q26_9	+Inattent$Q105_9 #If I believe in an idea, no obstacle will prevent me.
Inattent$PP10	<-	Inattent$Q26_10	+Inattent$Q105_10 #I can spot a good opportunity long before others can.

#Voice
Inattent$VC1	<-	Inattent$Q27_1	+	Inattent$Q106_1 #I develop and make recommendations concerning issues that affect my work group.
Inattent$VC2	<-	Inattent$Q27_2	+	Inattent$Q106_2 #I speak up and encourage others in my group to get involved in issues that affect the group.
Inattent$VC3	<-	Inattent$Q27_3	+	Inattent$Q106_3 #I communicate my opinions about work issues to others in my group even if my opinion is different and others in the group disagree with me.
Inattent$VC4	<-	Inattent$Q27_4	+	Inattent$Q106_4 #I keep well informed about issues where my opinion might be useful to my work group.
Inattent$VC5	<-	Inattent$Q27_5	+	Inattent$Q106_5 #I get involved in issues that affect the quality of work life here in my group.
Inattent$VC6	<-	Inattent$Q27_6	+	Inattent$Q106_6 #I speak up in my group with ideas for new projects or changes in procedures.

#Taking Charge
Inattent$TC1	<-	Inattent$Q28_1	+	Inattent$Q106_8  #I often try to adopt improved procedures for doing my job.
Inattent$TC2	<-	Inattent$Q28_2	+	Inattent$Q106_9  #I often try to change how my job is executed in order to be more effective.
Inattent$TC3	<-	Inattent$Q28_3	+	Inattent$Q106_10 #I often try to bring about improved procedures for the work unit or department.
Inattent$TC4	<-	Inattent$Q28_4	+	Inattent$Q106_11 #I often try to institute new work methods that are more effective for this company.
Inattent$TC5	<-	Inattent$Q28_5	+	Inattent$Q106_12 #I often try to change organizational rules or policies that are nonproductive or counterproductive.
Inattent$TC6	<-	Inattent$Q28_6	+	Inattent$Q106_13 #I often make constructive suggestions for imprving how things operate within the organization.
Inattent$TC7	<-	Inattent$Q28_7	+	Inattent$Q106_14 #I often try to correct a faulty procedure or practice.
Inattent$TC8	<-	Inattent$Q28_8	+	Inattent$Q106_15 #I often try to eliminate redundant or unnecessary procedures.
Inattent$TC9	<-	Inattent$Q28_9	+	Inattent$Q106_16 #I often try to implement solutions to pressing organizational problems.
Inattent$TC10	<-	Inattent$Q28_10	+	Inattent$Q106_17 #I often try to introduce new structures, technologies, or approaches to improve efficiency

#OCBI
Inattent$OCBI1	<-	Inattent$Q29_1	+	Inattent$Q106_19 #I help others who have been absent.
Inattent$OCBI2	<-	Inattent$Q29_2	+	Inattent$Q106_20 #I help others who have heavy work loads.
Inattent$OCBI3  <-	Inattent$Q29_3	+	Inattent$Q106_21 #I assist my supervisor with his/her work load (when not asked).
Inattent$OCBI4	<-	Inattent$Q29_4	+	Inattent$Q106_22 #I take time to listen to co-workers' problems and worries.
Inattent$OCBI5	<-	Inattent$Q29_5	+	Inattent$Q106_23 #I go out of my way to help new employees.
Inattent$OCBI6	<-	Inattent$Q29_6	+	Inattent$Q106_24 #I take a personal interest in other employees.
Inattent$OCBI7	<-	Inattent$Q29_7	+	Inattent$Q106_25 #I pass along information to co-workers.

#OCBO
Inattent$OCBO1	<-	Inattent$Q30_1	+	Inattent$Q106_27 #My attendance at work is above the norm.
Inattent$OCBO2	<-	Inattent$Q30_2	+	Inattent$Q106_28 #I give advance notice when I'm unable to come to work.
Inattent$OCBO3	<-	Inattent$Q30_3	+	Inattent$Q106_29 #I take undeserved work breaks. (r) 
Inattent$OCBO4	<-	Inattent$Q30_4	+	Inattent$Q106_30 #I spend a great deal of time with personal phone conversations. (r)
Inattent$OCBO5	<-	Inattent$Q30_5	+	Inattent$Q106_31 #I complain about insignificant things at work. (r)
Inattent$OCBO6	<-	Inattent$Q30_6	+	Inattent$Q106_32 #I conserve and protect organizational property.
Inattent$OCBO7	<-	Inattent$Q30_7	+	Inattent$Q106_33 #I adhere to informal rules devised to maintain order. 

#IRB
Inattent$IRB1	<-	Inattent$Q31_1	+	Inattent$Q106_35 #I adepquately complete assigned duties.
Inattent$IRB2	<-	Inattent$Q31_2	+	Inattent$Q106_36 #I fulfill responsibilities specific in my job description.
Inattent$IRB3	<-	Inattent$Q31_3	+	Inattent$Q106_37 #I perform tasks that are expected of me.
Inattent$IRB4	<-	Inattent$Q31_4	+	Inattent$Q106_38 #I meet formal performance requirements of the job.
Inattent$IRB5	<-	Inattent$Q31_5	+	Inattent$Q106_39 #I engage in activities that will directly affect my performance.
Inattent$IRB6	<-	Inattent$Q31_6	+	Inattent$Q106_40 #I nelgect aspects of my job that I'm obligated to perform. (r)
Inattent$IRB7	<-	Inattent$Q31_7	+	Inattent$Q106_41 #I fail to perform essential job duties. (r)

#Consistency Motif
Inattent$CM1 <- Inattent$Q27_7 + Inattent$Q106_7   #"I am a brave person"
Inattent$CM2 <- Inattent$Q28_11 + Inattent$Q106_18 #"I am a courageous person"  
Inattent$CM3 <- Inattent$Q29_8 + Inattent$Q106_26  #"I am a talkative person"
Inattent$CM4 <- Inattent$Q30_8 + Inattent$Q106_34  #"I am a silent person"
Inattent$CM5 <- Inattent$Q33_6 + Inattent$Q112_6   #"I am an optimistic person"
Inattent$CM6 <- Inattent$Q41_7 + Inattent$Q116_7   #"I am a pessimistic person"
Inattent$CM7 <- Inattent$Q31_8 + Inattent$Q106_42  #"I seldom feel blue."
Inattent$CM8 <- Inattent$Q32_5 + Inattent$Q111_5   #"I often feel blue."

###Create PANAS items.
##PA
Inattent$PA1 <- Inattent$Q34_1 + Inattent$Q113_1   #Interested
Inattent$PA2 <- Inattent$Q34_3 + Inattent$Q113_3   #Excited
Inattent$PA3 <- Inattent$Q34_5 + Inattent$Q113_5   #Strong
Inattent$PA4 <- Inattent$Q34_9 + Inattent$Q113_9   #Enthusiastic
Inattent$PA5 <- Inattent$Q34_10 + Inattent$Q113_10 #Proud
Inattent$PA6 <- Inattent$Q35_2 + Inattent$Q114_2   #Alert
Inattent$PA7 <- Inattent$Q35_4 + Inattent$Q114_4   #Inspired
Inattent$PA8 <- Inattent$Q35_6 + Inattent$Q114_6   #Determined
Inattent$PA9 <- Inattent$Q35_7 + Inattent$Q114_7   #Attentive
Inattent$PA10 <- Inattent$Q35_9 + Inattent$Q114_9  #Active

##NA
Inattent$NA1 <- Inattent$Q34_2 + Inattent$Q113_2    #Distressed
Inattent$NA2 <- Inattent$Q34_4 + Inattent$Q113_4    #Upset
Inattent$NA3 <- Inattent$Q34_6 + Inattent$Q113_6    #Guilty
Inattent$NA4 <- Inattent$Q34_7 + Inattent$Q113_7    #Scared
Inattent$NA5 <- Inattent$Q34_8 + Inattent$Q113_8    #Hostile
Inattent$NA6 <- Inattent$Q35_1 + Inattent$Q114_1    #Irritable
Inattent$NA7 <- Inattent$Q35_3 + Inattent$Q114_3    #Ashamed
Inattent$NA8 <- Inattent$Q35_5 + Inattent$Q114_5    #Nervous
Inattent$NA9 <- Inattent$Q35_8 + Inattent$Q114_8    #Jittery
Inattent$NA10 <- Inattent$Q35_10 + Inattent$Q114_10 #Afraid

##Mood
Inattent$MOOD <- Inattent$Q36 + Inattent$Q115 # “My mood today can best be described as...”

#Recode 0 values to missing.
l <- Inattent[c(224:299)]
l <- as.data.frame(apply(l, 2, function(x) {x <- recode(x,"0=NA"); x}))

#Retain only variables used for testing purposes.
data1 <- cbind(Inattent[c(215:223)],l)
N <- as.numeric(nrow(data1))
n1f <- as.numeric(freq_vect(data1$COND)[2,"Count"])
n2f <- as.numeric(freq_vect(data1$COND)[1,"Count"])

#Missing data analysis using 'sapply(data1, function(x) sum(is.na(x)))' revealed some missing likert data. 
init <- mice(data1, maxit = 0)
meth <- init$method
predM <- init$predictorMatrix
meth[c("PP5")]="norm"
meth[c("OCBI5")]="norm"
meth[c("IRB4")]="norm"
imputed <- mice(data1, method=meth, predictorMatrix=predM, m=5)
data1 <- complete(imputed)

#Round imputed data to nearest whole number for estimation purposes. 
data1$PP5 <- ceiling(data1$PP5)
data1$OCBI5 <- ceiling(data1$OCBI5)
data1$IRB4 <- ceiling(data1$IRB4)

#Calculate descriptives
data1$AGE <- as.numeric(data1$AGE)
M <- mean(data1$AGE, na.rm = TRUE)
SD <- sd(data1$AGE, na.rm = TRUE)

#Calculate frequencies
##Recode single "f" to female. 
female.n <- as.numeric(freq_vect(data1$GENDER)[1,"Count"])
female.p <- as.numeric(freq_vect(data1$GENDER)[1,"Percentage"])
white.n <- as.numeric(freq_vect(data1$RACE)[2,"Count"])
white.p <- as.numeric(freq_vect(data1$RACE)[2,"Percentage"])
fulltime.n <- as.numeric(freq_vect(data1$EMPLOYED)[4,"Count"])
fulltime.p <- as.numeric(freq_vect(data1$EMPLOYED)[4,"Percentage"])

# Create response style indicators from observed data (Falk & Cai, 2016).
# Recode all Likert measures (l)
l <- data1[c(10:56)]
## Extreme Response Style (ERS)
ERS <- as.data.frame(apply(l, 2, function(x) {x <- recode(x,"1=1; 2=0; 3=0; 4=0; 5=1"); x}))
ERS <- as.data.frame(round(rowSums(ERS)/ncol(ERS),2))
## Midpoint Response Style(MRS)
MRS <- as.data.frame(apply(l, 2, function(x) {x <- recode(x,"1=0; 2=0; 3=1; 4=0; 5=0"); x}))
MRS <- as.data.frame(round(rowSums(MRS)/ncol(MRS),2))
## Acquiescence as a tendency to respond above the midpoint
ARS <- as.data.frame(apply(l, 2, function(x) {x <- recode(x,"1=0; 2=0; 3=0; 4=1; 5=1"); x}))
ARS <- as.data.frame(round(rowSums(ARS)/ncol(ARS),2))
## Dis-acquiescence as a tendency to respond above the midpoint
DRS <- as.data.frame(apply(l, 2, function(x) {x <- recode(x,"1=1; 2=1; 3=0; 4=0; 5=0"); x}))
DRS <- as.data.frame(round(rowSums(DRS)/ncol(DRS),2))
## Socially desirable responding. Requires separating datasets into positive and negatively worded items and keying for socially desirable resonding.
lp <- data1[c(10:44,48:54)]
SDR1 <- as.data.frame(apply(l, 2, function(x) {x <- recode(x,"1=0; 2=0; 3=0; 4=1; 5=0"); x}))
ln <- data1[c(45:47,55,56)]
SDR2 <- as.data.frame(apply(l, 2, function(x) {x <- recode(x,"1=0; 2=1; 3=0; 4=0; 5=0"); x}))
SDR <- cbind(SDR1,SDR2)
SDR<- as.data.frame(round(rowSums(SDR)/ncol(SDR),2))

## Merge all response style variables.
RS <- cbind(ERS,MRS,ARS,DRS,SDR)

## Merge response style measures with main dataset.
data1 <- cbind(data1,RS)

# Rename response style variables.
colnames(data1)[colnames(data1)== "round(rowSums(ERS)/ncol(ERS), 2)"] <- "ERS"
colnames(data1)[colnames(data1)== "round(rowSums(MRS)/ncol(MRS), 2)"] <- "MRS"
colnames(data1)[colnames(data1)== "round(rowSums(ARS)/ncol(ARS), 2)"] <- "ARS"
colnames(data1)[colnames(data1)== "round(rowSums(DRS)/ncol(DRS), 2)"] <- "DRS"
colnames(data1)[colnames(data1)== "round(rowSums(SDR)/ncol(SDR), 2)"] <- "SDR"
```
  
Six hundred and twenty-one workers from Amazon's Mechanical Turk were paid $1.30 for completing a survey, of which `r apa(N,0,T)` agreed to participate in our study after reading our informed consent form. These `r apa(N,0,T)` participants were randomly assigned to one of two conditions: non-remedied (n = `r apa(N1,0,T)`) or remedied (n = `r apa(N2,0,T)`). Approximately `r apa(sr1,2,T)` (n = `r apa(n1,0,T)`) in the non-remedied and approximately `r apa(sr2,2,T)` (n = `r apa(n2,0,T)`) in the remedied condition correctly responded to the manipulation check of proper interpretation of the study purpose and were allowed to continue. Respondents' data were eliminated due to incorrectly or not responding to an attention check item asking individuals to "Click on the first circle indicating 'Strongly Disagree?'" (n = `r apa(iar,0,T)`) [@MeadeIdentifyingcarelessresponses2012], or not reporting demographics and abandoning the survey (n = 4). This filtering process resulted in a final sample of `r apa(N,0,T)` individuals (`r apa(n1f,0,T)` in the control and `r apa(n2f,0,T)` in the experimental condition). Notwithstanding missing demographic data, the sample was female biased (n = `r apa(female.n,0,T)`), predominantly Caucasian, and  the average age was *M* = `r apa(M,2,T)` (*SD* = `r apa(SD,2,T)`). The majority (n = `r apa(fulltime.n,0,T)`) of respondents worked full-time. Three cases of missing item level data were handled using the normal model approach [see @WuComparisonImputationStrategies2015].
  
Following guidance by @PodsakoffCommonmethodbiases2003, we sought to create a psychological separation of measurement using a cover story. In the experimental condition where the proximal separation of measurement remedy was used, participants were given a cover story designed to disguise the purpose of the study:"In this study, you will be asked to respond to statements about yourself and how you behave at work. Separate researchers built the content of this survey for separate purposes, and so the questions may or may not relate to one another. As such, there are no right or wrong answers, so please provide honest responses." By comparison, in the control condition participants were given a message to disguise the purpose of the study transparent: "The purpose of this study is to test for relationships between proactive personality and workplace behaviors (including taking charge at work, having a voice in the workplace, organizational citizenship behavior at work, and job performance)." These participants were also given a survey with all items and scales presented in the same order (demographics were presented last). Following the randomly assigned cover story, we asked our participants to respond to the following item indicating whether they understood the purpose of our study: "Before you take our survey, please tell us which of the following correctly describes the purpose of this study." Three response options were given: (a) "The purpose of this study is to test for relationships between personality and workplace behaviors. As such, there is a clear purpose to the study." (b) "Separate researchers built the content of this survey for separate purposes, and so the questions may or may not relate to one another. As such, there is no single clear purpose to this study." and (c) "The purpose of this study is to measure emotional intelligence and workplace behaviors." In addition to the use of a cover story, we following guidance by @PodsakoffCommonmethodbiases2003 and included other procedural remedies for method variance within the experimental condition. We placed filler scales in between the administration of the proactive personality and outcomes scales. Several filler scales were included, which contained the positive and negative affective schedule and the consistency motif scales. Additionally, to address scale-order effects, we randomized the placement of proactive personality and outcomes scales such that one would be placed first before the other. To address item-order effects, we randomized items within all scales. 

## Measures
```{r Study 1: Cronbach Alphas, include = FALSE}
#Create scale scores
my.keys.list <- list(PP=c("PP1","PP2","PP3","PP4","PP5","PP6","PP7","PP8","PP9","PP10"),
                     IRB=c("IRB1","IRB2","IRB3","IRB4","IRB5","-IRB6","-IRB7"), 
                     OCBI=c("OCBI1","OCBI2","OCBI3","OCBI4","OCBI5","OCBI6","OCBI7"),
                     OCBO=c("OCBO1","OCBO2", "-OCBO3","-OCBO4","-OCBO5","OCBO6","OCBO7"),
                     PA=c("PA1","PA2","PA3","PA4","PA5","PA6","PA7","PA8","PA9","PA10"))
my.scales <- scoreItems(my.keys.list,data1)
PP.alpha <- round(my.scales[["alpha"]][1],2)
IRB.alpha <- round(my.scales[["alpha"]][2],2)
OCBI.alpha <- round(my.scales[["alpha"]][3],2)
OCBO.alpha <- round(my.scales[["alpha"]][4],2)
PA.alpha <- round(my.scales[["alpha"]][5],2)
```

### Proactive personality

We used the 10-item proactive personality scale was used to capture proactive personality [@SeibertProactivepersonalitycareer1999]. Example items include: "I am constantly on the lookout for new ways to improve," "If I see something I don't like, I fix it," and "I excel at identifying opportunities." This, and all scales included in this study, utilized a five-point agreement Likert rating scale (1 = strongly disagree, 5 = strongly agree) ($\alpha$ = `r apa(PP.alpha,2,T)`).

### In-role and organizational citizenship behavior 

@Williamsalternativeapproachmethod1994 In-Role Performance Behavior (IRB) and Organizational Citizenship Behavior (OCB) scales, the latter of which includes OCBI (OCB directed at the individual) and OCBO (OCB directed at the organization), were used as outcome measures The items were initially written to reflect a supervisor's perspective and so were adjusted here to be self-referent. Example items (and Cronbach alphas) for each respective scale: "I perform tasks that are expected of me" ($\alpha$ = `r apa(IRB.alpha,2,T)`), "I take a personal interest in other employees" ($\alpha$ = `r apa(OCBI.alpha,2,T)`), and "I conserve and protect organizational property" ($\alpha$ = `r apa(OCBO.alpha,2,T)`). [^2] [^3] 

[^2]: Given the presence of negatively-keyed items in these measures, we included a negative item wording content factor that explained variance in the negatively-keyed items only [see @dalalNegativelyWordedItems2015; @ZhangImprovingFactorStructure2016]. This negative method factor was theoretically independent from IRB and OCBOO [@ConwayWhatReviewersShould2010].

[^3]: We also included self-report measures of voice [@vandyneHelppingVoiceExtrarole1998] and taking charge [@morrisonTakingChargeWork1999] but did not include these in our reports as (i) they were not included in all studies and (ii) the procedural remedies were not applied to interrelations linking proactive personality to these outcomes.

### Positive affectivity

We administered the positive and negative affect schedule [PANAS; see @WatsonDevelopmentValidationBrief1988] and asked individuals to describe their how they generally feel. Data were modeled using a bi-factor approach [see @LeuePANASstructurerevisited2011] and only positive affectivity trait scores were retained for our purposes ($\alpha$ = `r apa(PA.alpha,2,T)`) to reduce the number of parameters we estimated. We modeled positive affectivity as a single-item latent construct using guidance provided by @PetrescuMarketingresearchusing2013. 

### Momentary mood state 

Momentary mood state was measured with a single item (“My mood today can best be described as...”) with a seven-point scale (1 = unpleasant; 7 = pleasant), which was used to model a latent momentary mood construct. Following guidance on modeling single-item latent constructs [@PetrescuMarketingresearchusing2013], we set $\lambda$ conservatively as .95 * variance. We also calculated the error variance as the sample variance of this item * (1 - scale reliability) and the scale reliability was set at .85. [^4]

[^4]: We also included a variety of other potential measured method effects in our study (e.g., consistency motif, response styles, negative affect, attitudes toward the color blue, survey enjoyment, and seeing value in a survey). However, we did not include these in our analysis for the following reasons. To measure a consistency motif, we employed psychometric synonyms and antonyms that have been used in the literature [@goldbergPredictionSemanticConsistency1985]. Specifically, we asked individuals to respond to the following items with a 5-point agreement scale: [synonyms] (1a) "I am a brave person", (1b) "I am a courageous person"; [antonyms] (2a) "I am a talkative person", (2b) "I am a silent person", (3a) " I am an optimistic person" (3b) "I am a pessimistic person", (4a) "I am seldom blue", and (4b) "I am often blue". Unfortunately, we could not build a model capturing a consistency motif (it also clearly taps into extroversion and emotional stability). An index describing how well individuals agreed to these statements was also unreliable ($\alpha$ < .3). Response style variables were also created whereby all responses to substantive items were re-keyed in the form discussed by @falkFlexibleFullinformationApproach2016. Other response styles (i.e., extreme response style, midpoint response style, acquiescence, dis-acquiescence) were calculated in their respective forms [see @falkFlexibleFullinformationApproach2016]. However, we elected to not include these factors in our analysis as these styles, such as social desirability, are substantive traits likely to be correlated with both proactive personality and our outcomes, and therefore less like to be solely a methodological contaminant, per se [see @spectorNewPerspectiveMethod2017; @spectorWhyNegativeAffectivity2000]. In an earlier analysis, negative affect played a negligible contaminating role in our data, and so we elected to ignore it moving forward. The remaining measures could not be linked conceptually to a possible cause of method variance as specified by @PodsakoffCommonmethodbiases2003. We have made all data available for reanalysis.

```{r Study 1: Construct Validity for Affect, include = FALSE}
mood.cfa <- '
#Affectivity
PA =~ PA1 + PA2 + PA3 + PA4 + PA5 + PA6 + PA7 + PA8 + PA9 + PA10 
Na =~ NA1 + NA2 + NA3 + NA4 + NA5 + NA6 + NA7 + NA8 + NA9 + NA10 
AP =~ PA1 + PA2 + PA3 + PA4 + PA5 + PA6 + PA7 + PA8 + PA9 + PA10 + NA1 + NA2 + NA3 + NA4 + NA5 + NA6 + NA7 + NA8 + NA9 + NA10 

#Constrain bifactor covariances
PA ~~ 0*Na
PA ~~ 0*AP
Na ~~ 0*AP
'
mood <- cfa(mood.cfa, data = data1, estimator = "DWLS", parameterization = "theta", information = "expected",  std.lv=TRUE, test = "Satorra-Bentler")
summary(mood, standardized = TRUE, fit.measures =TRUE)
affect <- as.data.frame(predict(mood))

#Bind to data
data1 <- cbind(data1,affect[c(1)])
```

## Analytical Approach

We conducted a multiple group analysis with 'lavaan' in R. Specifically, a model was tested whereby all latent construct covariances were freely estimated with certain exceptions. Negative item wording, as a content factor, was modeled as uncorrelated with all other factors. Positive affectivity was modeled as a cause of the momentary mood state [@spectorNewPerspectiveMethod2017; @watsonDevelopmentValidationBrief1988]. Importantly, mood was modeled as a contaminant of substantive item responses whereas positive affectivity was modeled as a correlate of the substantive factors of interest [@kaplanRolePositiveNegative2009]. This approach allowed us to capture the contaminating influences of mood independent of the substantive influences of positive affectivity [for a similar approach, see @spectorNewPerspectiveMethod2017]. As our scales are Likert-type (i.e., have only five response categories, cannot be normally distributed), we used the diagonal weighted least squares (DWLS) estimator [@beauducelPerformanceMaximumLikelihood2006]. As DWLS estimation was used, model fit was assessed by means of the Satorra-Bentler scaled chi-square test statistics and robust variants of the root-mean-square error of approximation (RMSEA), the standardized root-mean-square residual (SRMR), and the comparative fit index (CFI) [see @beauducelSimulationStudyFit2005]. According to @huCutoffCriteriaFit1999, RMSEA of ~.06 indicates a close fit, whereas an RMSEA of ~.08 indicates acceptable fit. For the SRMR, a cutoff close to .08 or below has been recommended [@huCutoffCriteriaFit1999]. The CFI should be greater than .95. Across all studies, we fit the exact same statistical model to the data.

# Results - Study 1
```{r Study 1: Build for Descriptives, include = FALSE}
#Create Descriptives Table
scales <- my.scales[["scores"]]
scales <- cbind(data1[c(1,2,4)],scales,data1[c(85)])
table1 <- apa.cor.table(scales, table.number = 1, landscape = TRUE, show.conf.interval = TRUE)
```
```{r results="asis"}
#table1 
```
```{r Study 1: Model Testing, include = FALSE}
# Create pvalue function to conver values < .001 to "<.001".
pvalr <- function(pvals, sig.limit = .001, digits = 3, html = FALSE) {

  roundr <- function(x, digits = 1) {
    res <- sprintf(paste0('%.', digits, 'f'), x)
    zzz <- paste0('0.', paste(rep('0', digits), collapse = ''))
    res[res == paste0('-', zzz)] <- zzz
    res
  }

  sapply(pvals, function(x, sig.limit) {
    if (x < sig.limit)
      if (html)
        return(sprintf('&lt; %s', format(sig.limit))) else
          return(sprintf('< %s', format(sig.limit)))
    if (x > .1)
      return(roundr(x, digits = 2)) else
        return(roundr(x, digits = digits))
  }, sig.limit = sig.limit)
}

m1.cfa <- ' 
# Substantive factors
PP =~ c(pp1a, pp1b)*PP1 + c(pp2a, pp2b)*PP2 + c(pp3a,pp3b)*PP3 + c(pp4a,pp4b)*PP4 + c(pp5a,pp5b)*PP5 + c(pp6a,pp6b)*PP6 + c(pp7a,pp7b)*PP7 + c(pp8a,pp8b)*PP8 + c(pp9a,pp9b)*PP9 + c(pp10a,pp10b)*PP10
IRB =~ c(irb1a,irb1b)*IRB1 + c(irb2a,irb2b)*IRB2 + c(irb3a,irb3b)*IRB3 + c(irb4a,irb4b)*IRB4 + c(irb5a,irb5b)*IRB5 + c(irb6a,irb6b)*IRB6 + c(irb7a,irb7b)*IRB7
OCBI =~ c(ocbi1a,ocbi1b)*OCBI1 + c(ocbi2a,ocbi2b)*OCBI2 + c(ocbi3a,ocbi3b)*OCBI3 + c(ocbi4a,ocbi4b)*OCBI4 + c(ocbi5a,ocbi5b)*OCBI5 + c(ocbi6a,ocbi6b)*OCBI6 + c(ocbi7a,ocbi7b)*OCBI7
OCBO =~ c(ocbo1a,ocbo1b)*OCBO1 + c(ocbo2a,ocbo2b)*OCBO2 + c(ocbo3a,ocbo3b)*OCBO3 + c(ocbo4a,ocbo4b)*OCBO4 + c(ocbo5a,ocbo5b)*OCBO5 + c(ocbo6a,ocbo6b)*OCBO6 + c(ocbo7a,ocbo7b)*OCBO7

# Negative Item Content Factor
NW =~ c(nw1a, nw1b)*IRB6 + c(nw2a, nw2b)*IRB7 + c(nw3a, nw3b)*OCBO3 + c(nw4a, nw4b)*OCBO4 + c(nw5a, nw5b)*OCBO5
NW ~~ c(1,1)*NW
NW ~ c(0,0)*1
NW ~~ c(0,0)*IRB
NW ~~ c(0,0)*OCBO
NW ~~ c(0,0)*PP
NW ~~ c(0,0)*OCBI

#Factor variances are fixed to 1 to allow estimation.
###Substantive factors
PP ~~ c(1,1)*PP
IRB ~~ c(1,1)*IRB
OCBI ~~ c(1,1)*OCBI
OCBO ~~ c(1,1)*OCBO

##Factor covariances labels 
PP ~~ c(PPIRB1,PPIRB2)*IRB
PP ~~ c(PPOCBI1,PPOCBI2)*OCBI
PP ~~ c(PPOCBO1,PPOCBO2)*OCBO
IRB ~~ c(IRBOCBI1,IRBOCBI2)*OCBI
IRB ~~ c(IRBOCBO1,IRBOCBO2)*OCBO
OCBI ~~ c(OCBIO1,OCBIO2)*OCBO

#Factor means of both groups are fixed at zero to allow identification.
##Substantive factors
PP ~ c(0,0)*1
IRB ~ c(0,0)*1
OCBI ~ c(0,0)*1
OCBO ~ c(0,0)*1

# Add in "Mood" as a single-item latent construct that is an uncorrelated with all factors.
Mood =~ .95*MOOD + c(mpp1a, mpp1b)*PP1 + c(mpp2a, mpp2b)*PP2 + c(mpp3a,mpp3b)*PP3 + c(mpp4a,mpp4b)*PP4 + c(mpp5a,mpp5b)*PP5 + c(mpp6a,mpp6b)*PP6 + c(mpp7a,mpp7b)*PP7 + c(mpp8a,mpp8b)*PP8 + c(mpp9a,mpp9b)*PP9 + c(mpp10a,mpp10b)*PP10 + c(mirb1a,mirb1b)*IRB1 + c(mirb2a,mirb2b)*IRB2 + c(mirb3a,mirb3b)*IRB3 + c(mirb4a,mirb4b)*IRB4 + c(mirb5a,mirb5b)*IRB5 + c(mirb6a,mirb6b)*IRB6 + c(mirb7a,mirb7b)*IRB7 + c(mocbi1a,mocbi1b)*OCBI1 + c(mocbi2a,mocbi2b)*OCBI2 + c(mocbi3a,mocbi3b)*OCBI3 + c(mocbi4a,mocbi4b)*OCBI4 + c(mocbi5a,mocbi5b)*OCBI5 + c(mocbi6a,mocbi6b)*OCBI6 + c(mocbi7a,mocbi7b)*OCBI7 + c(mocbo1a,mocbo1b)*OCBO1 + c(mocbo2a,mocbo2b)*OCBO2 + c(mocbo3a,mocbo3b)*OCBO3 + c(mocbo4a,mocbo4b)*OCBO4 + c(mocbo5a,mocbo5b)*OCBO5 + c(mocbo6a,mocbo6b)*OCBO6 + c(mocbo7a,mocbo7b)*OCBO7
 # (see Anderson & Gerbing, 1988; Sorbom & Joreskig, 1982; as cited by Petrescu, 2013)
MOOD ~~ (1.593178)*(1-.85)*MOOD

# Add in PA as a single-item latent construct that is an uncorrelated with all factors.
PosAff =~ .95*PA
PA ~~ (0.9241972)*(1-.92)*PA

# Make uncorrelated with NW
NW ~~ 0*PosAff

# Add in causal sructure for PA and Mood
Mood ~ PosAff # PA causes mood

# Mood as a proximal cause of method variance
Mood ~~ 0*PP # Mood uncorrelated with proactive personality
Mood ~~ 0*IRB # Mood uncorrelated with IRB
Mood ~~ 0*OCBI # Mood uncorreated with OCBI
Mood ~~ 0*OCBO # Mood uncorrelated with OCBO
NW ~~ 0*Mood
'

m1 <- cfa(m1.cfa, data = data1, group = "COND", estimator = "DWLS", parameterization = "theta", information = "expected", std.lv=TRUE,test = "Satorra-Bentler")
summary(m1, standardized = TRUE, fit.measures = TRUE)
mod.m <- modindices(m1, minimum.value = 10, sort = TRUE)
fitm1.m <- fitmeasures(m1)
mod.m <- modindices(m1, minimum.value = 10, sort = TRUE)
fitm1.m <- fitmeasures(m1)

#Gather stats
csq.ms1 <- round(as.numeric(fitm1.m[c(6)]), digits = 2) ##Chi-Square
df.ms1 <- round(as.numeric(fitm1.m[c(7)]), digits = 2) ##df
cdfratio.ms1 <- round(csq.ms1/df.ms1,2) ## Chi-square / df ratio
p.ms1 <- pvalr(as.numeric(fitm1.m[c(8)]), digits = 2) ##Chi-Square p value
cssf.ms1 <- round(as.numeric(fitm1.m[c(9)]), digits = 2) ##Chisq.scaling factor
cfi.ms1 <- round(as.numeric(fitm1.m[c(27)]), digits = 2) #CFI robust
remsea.ms1 <- round(as.numeric(fitm1.m[c(44)]), digits = 3) ##RMSEA robust
remseal.ms1 <- round(as.numeric(fitm1.m[c(45)]), digits = 3) #RMSEA lower robust
remseau.ms1 <- round(as.numeric(fitm1.m[c(46)]), digits = 3) #RMSEA upper robust
srmr.ms1<- round(as.numeric(fitm1.m[c(50)]), digits = 3) #SRMR
```
```{r Study 1: Build Latent Correlation and Reliability Decomposition Tables, include = FALSE}
# Build table for latent covariances
factors <- c("Proactive Personality","")
table <- as.data.frame(cbind(factors,"Condition","In-Role Behavior","","OCBI","","OCBO",""))
colnames(table)[1] <- "Predictor"
colnames(table)[2] <- "Condition"
colnames(table)[3] <- "In-Role Behavior"
colnames(table)[4] <- "In-Role Behaviorp"
colnames(table)[5] <- "OCBI"
colnames(table)[6] <- "OCBIp"
colnames(table)[7] <- "OCBO"
colnames(table)[8] <- "OCBOp"
table$Condition <- as.character(table$Condition)
table$`In-Role Behavior` <- as.numeric(table$`In-Role Behavior`)
table$`In-Role Behaviorp` <- as.character(table$`In-Role Behavior`)
table$`OCBI` <- as.numeric(table$`OCBI`)
table$OCBIp <- as.character(table$OCBIp)
table$`OCBO` <- as.numeric(table$`OCBO`)
table$OCBOp <- as.character(table$OCBOp)
table$Condition[1] <- "No Remedies"
table$Condition[2] <- "Remedies Bundled"

# Extract pvalues and significance levels for mapping purposes.
data <- parameterEstimates(m1, standardized=TRUE)%>% 
  dplyr::filter(op == "~~") %>%
  dplyr::filter(label == "PPIRB1" | label == "PPOCBI1" | label == "PPOCBO1" | label == "PPIRB2" | label == "PPOCBI2" | label == "PPOCBO2") %>%
  select(label, est, pvalue) %>%
  mutate_if(is.numeric, ~round(., 2)) 
data$p <- stars.pval(data$pvalue)

# Extract the latent covariances loadings, map on significance levels, and place into table.
m1.covariances<- inspect(m1,"std.all")
table$`In-Role Behavior`[1] <- m1.covariances[["1"]][["psi"]][1,2]  # PP-IRBnr
table$`In-Role Behaviorp`[1] <- data %>% dplyr::filter(label == "PPIRB2") %>% select(p) 
table$`In-Role Behavior`[2] <- m1.covariances[["0"]][["psi"]][1,2]  # PP-IRBr
table$`In-Role Behaviorp`[2] <- data %>% dplyr::filter(label == "PPIRB1") %>% select(p) 
table$OCBI[1] <- m1.covariances[["1"]][["psi"]][1,3]  # PP-OCBInr
table$OCBIp[1] <- data %>% dplyr::filter(label == "PPOCBI2") %>% select(p) 
table$OCBI[2] <- m1.covariances[["0"]][["psi"]][1,3]  # PP-OCBIr
table$OCBIp[2] <- data %>% dplyr::filter(label == "PPOCBI1") %>% select(p) 
table$OCBO[1] <- m1.covariances[["1"]][["psi"]][1,4]  # PP-OCBOnr
table$OCBOp[1] <- data %>% dplyr::filter(label == "PPOCBO2") %>% select(p) 
table$OCBO[2] <- m1.covariances[["0"]][["psi"]][1,4]  # PP-OCBOr
table$OCBOp[2] <- data %>% dplyr::filter(label == "PPOCBO1") %>% select(p) 

# round
table <- table %>% 
  mutate_if(is.numeric, ~round(., 2))

# Combine columns
table$`In-Role Behavior` <- paste(table$`In-Role Behavior`,table$`In-Role Behaviorp`)
table$OCBI <- paste(table$OCBI,table$OCBIp)
table$OCBO <- paste(table$OCBO,table$OCBOp)

# Name table 2
table2 <- table %>%
  select(-one_of("In-Role Behaviorp","OCBIp","OCBOp"))

# Build reliability decomposition table 
factors <- c("Proactive Personality","","In-Role Behavior", "","OCBI","", "OCBO", "")
table <- as.data.frame(cbind(factors,"Condition","Total Reliability","Substantive Reliability","Mood Reliability","% Mood Reliability","Negative Item Wording Reliability","% Negative Item Wording Reliability"))
colnames(table)[1] <- "Latent Variable"
colnames(table)[2] <- "Condition"
colnames(table)[3] <- "Total Reliability"
colnames(table)[4] <- "Substantive Reliability"
colnames(table)[5] <- "Mood Reliability"
colnames(table)[6] <- "% Mood Reliability"
colnames(table)[7] <- "Negative Item Wording Reliability"
colnames(table)[8] <- "% Negative Item Wording Reliability"
table$Condition <- as.character(table$Condition)
table$`Total Reliability` <- as.numeric(table$`Total Reliability`)
table$`Substantive Reliability` <- as.numeric(table$`Substantive Reliability`)
table$`Mood Reliability` <- as.numeric(table$`Mood Reliability`)
table$`% Mood Reliability` <- as.numeric(table$`% Mood Reliability`)
table$`Negative Item Wording Reliability` <- as.numeric(table$`Negative Item Wording Reliability`)
table$`% Negative Item Wording Reliability` <- as.numeric(table$`% Negative Item Wording Reliability`)
table$Condition[1] <- "No Remedies"
table$Condition[2] <- "Remedies Bundled"
table$Condition[3] <- "No Remedies"
table$Condition[4] <- "Remedies Bundled"
table$Condition[5] <- "No Remedies"
table$Condition[6] <- "Remedies Bundled"
table$Condition[7] <- "No Remedies"
table$Condition[8] <- "Remedies Bundled"

# Extract the factor loadings
m1.factorloadings <- inspect(m1,"est")

# Focus on the remedied group
PP <- as.data.frame(m1.factorloadings[["0"]][["lambda"]])  %>%
  abs() %>%
  rownames_to_column %>%
  slice(1:10) %>%
  mutate(Sub.Rel = PP^2)%>%
  mutate(Mood.Rel = Mood^2)%>%
  mutate(NW.Rel = NW^2) %>%
  as.data.frame() %>%
  mutate(Error = (1-(Sub.Rel+Mood.Rel+NW.Rel)))

table$`Substantive Reliability`[2] <- (sum(PP$PP)^2)/((sum(PP$PP)^2)+(sum(PP$Mood)^2)+sum(PP$Error))
table$`Mood Reliability`[2] <- (sum(PP$Mood)^2)/((sum(PP$PP)^2)+(sum(PP$Mood)^2)+sum(PP$Error))
table$`Negative Item Wording Reliability`[2] <- (sum(PP$NW)^2)/((sum(PP$PP)^2)+(sum(PP$Mood)^2)+(sum(PP$NW)^2)+sum(PP$Error))
table$`Total Reliability`[2] <- (table$`Substantive Reliability`[2]+table$`Mood Reliability`[2]+table$`Negative Item Wording Reliability`[2] )
table$`% Mood Reliability`[2] <- (table$`Mood Reliability`[2]/table$`Total Reliability`[2])
table$`% Negative Item Wording Reliability`[2] <- (table$`Negative Item Wording Reliability`[2]/table$`Total Reliability`[2])

IRB <- as.data.frame(m1.factorloadings[["0"]][["lambda"]])  %>%
  abs() %>%
  rownames_to_column %>%
  slice(11:17) %>%
  mutate(Sub.Rel = IRB^2) %>%
  mutate(Mood.Rel = Mood^2) %>%
  mutate(NW.Rel = NW^2) %>%
  as.data.frame() %>%
  mutate(Error = (1-(Sub.Rel+Mood.Rel+NW.Rel)))

table$`Substantive Reliability`[4] <- (sum(IRB$IRB)^2)/((sum(IRB$IRB)^2)+(sum(IRB$Mood)^2)+(sum(IRB$NW)^2)+sum(IRB$Error))
table$`Mood Reliability`[4] <- (sum(IRB$Mood)^2)/((sum(IRB$IRB)^2)+(sum(IRB$Mood)^2)+(sum(IRB$NW)^2)+sum(IRB$Error))
table$`Negative Item Wording Reliability`[4] <- (sum(IRB$NW)^2)/((sum(IRB$IRB)^2)+(sum(IRB$Mood)^2)+(sum(IRB$NW)^2)+sum(IRB$Error))
table$`Total Reliability`[4] <- (table$`Substantive Reliability`[4]+table$`Mood Reliability`[4]+table$`Negative Item Wording Reliability`[4] )
table$`% Mood Reliability`[4] <- (table$`Mood Reliability`[4]/table$`Total Reliability`[4])
table$`% Negative Item Wording Reliability`[4] <- (table$`Negative Item Wording Reliability`[4]/table$`Total Reliability`[4])

OCBI <- as.data.frame(m1.factorloadings[["0"]][["lambda"]])  %>%
  abs() %>%
  rownames_to_column %>%
  slice(18:24) %>%
  mutate(Sub.Rel = OCBI^2) %>%
  mutate(Mood.Rel = Mood^2) %>%
  mutate(NW.Rel = NW^2) %>%
  as.data.frame() %>%
  mutate(Error = (1-(Sub.Rel+Mood.Rel+NW.Rel)))

table$`Substantive Reliability`[6] <- (sum(OCBI$OCBI)^2)/((sum(OCBI$OCBI)^2)+(sum(OCBI$Mood)^2)+(sum(OCBI$NW)^2)+sum(OCBI$Error))
table$`Mood Reliability`[6] <- (sum(OCBI$Mood)^2)/((sum(OCBI$OCBI)^2)+(sum(OCBI$Mood)^2)+(sum(OCBI$NW)^2)+sum(OCBI$Error))
table$`Negative Item Wording Reliability`[6] <- (sum(OCBI$NW)^2)/((sum(OCBI$OCBI)^2)+(sum(OCBI$Mood)^2)+(sum(OCBI$NW)^2)+sum(OCBI$Error))
table$`Total Reliability`[6] <- (table$`Substantive Reliability`[6]+table$`Mood Reliability`[6]+table$`Negative Item Wording Reliability`[6])
table$`% Mood Reliability`[6] <- (table$`Mood Reliability`[6]/table$`Total Reliability`[6])
table$`% Negative Item Wording Reliability`[6] <- (table$`Negative Item Wording Reliability`[6]/table$`Total Reliability`[6])

OCBO <- as.data.frame(m1.factorloadings[["0"]][["lambda"]])  %>%
  abs() %>%
  rownames_to_column %>%
  slice(25:32) %>%
  mutate(Sub.Rel = OCBO^2) %>%
  mutate(Mood.Rel = Mood^2) %>%
  mutate(NW.Rel = NW^2) %>%
  as.data.frame() %>%
  mutate(Error = (1-(Sub.Rel+Mood.Rel+NW.Rel)))

table$`Substantive Reliability`[8] <- (sum(OCBO$OCBO)^2)/((sum(OCBO$OCBO)^2)+(sum(OCBO$Mood)^2)+(sum(OCBO$NW)^2)+sum(OCBO$Error))
table$`Mood Reliability`[8] <- (sum(OCBO$Mood)^2)/((sum(OCBO$OCBO)^2)+(sum(OCBO$Mood)^2)+(sum(OCBO$NW)^2)+sum(OCBO$Error))
table$`Negative Item Wording Reliability`[8] <- (sum(OCBO$NW)^2)/((sum(OCBO$OCBO)^2)+(sum(OCBO$Mood)^2)+(sum(OCBO$NW)^2)+sum(OCBO$Error))
table$`Total Reliability`[8] <- (table$`Substantive Reliability`[8]+table$`Mood Reliability`[8]+table$`Negative Item Wording Reliability`[8])
table$`% Mood Reliability`[8] <- (table$`Mood Reliability`[8]/table$`Total Reliability`[8])
table$`% Negative Item Wording Reliability`[8] <- (table$`Negative Item Wording Reliability`[8]/table$`Total Reliability`[8])

# Focus on non-remedied
PP <- as.data.frame(m1.factorloadings[["1"]][["lambda"]])  %>%
  abs() %>%
  rownames_to_column %>%
  slice(1:10) %>%
  mutate(Sub.Rel = PP^2)%>%
  mutate(Mood.Rel = Mood^2)%>%
  mutate(NW.Rel = NW^2) %>%
  as.data.frame() %>%
  mutate(Error = (1-(Sub.Rel+Mood.Rel+NW.Rel)))

table$`Substantive Reliability`[1] <- (sum(PP$PP)^2)/((sum(PP$PP)^2)+(sum(PP$Mood)^2)+sum(PP$Error))
table$`Mood Reliability`[1] <- (sum(PP$Mood)^2)/((sum(PP$PP)^2)+(sum(PP$Mood)^2)+sum(PP$Error))
table$`Negative Item Wording Reliability`[1] <- (sum(PP$NW)^2)/((sum(PP$PP)^2)+(sum(PP$Mood)^2)+(sum(PP$NW)^2)+sum(PP$Error))
table$`Total Reliability`[1] <- (table$`Substantive Reliability`[1]+table$`Mood Reliability`[1]+table$`Negative Item Wording Reliability`[1])
table$`% Mood Reliability`[1] <- (table$`Mood Reliability`[1]/table$`Total Reliability`[1])
table$`% Negative Item Wording Reliability`[1] <- (table$`Negative Item Wording Reliability`[1]/table$`Total Reliability`[1])

IRB <- as.data.frame(m1.factorloadings[["1"]][["lambda"]])  %>%
  abs() %>%
  rownames_to_column %>%
  slice(11:17) %>%
  mutate(Sub.Rel = IRB^2) %>%
  mutate(Mood.Rel = Mood^2) %>%
  mutate(NW.Rel = NW^2) %>%
  as.data.frame() %>%
  mutate(Error = (1-(Sub.Rel+Mood.Rel+NW.Rel)))

table$`Substantive Reliability`[3] <- (sum(IRB$IRB)^2)/((sum(IRB$IRB)^2)+(sum(IRB$Mood)^2)+(sum(IRB$NW)^2)+sum(IRB$Error))
table$`Mood Reliability`[3] <- (sum(IRB$Mood)^2)/((sum(IRB$IRB)^2)+(sum(IRB$Mood)^2)+(sum(IRB$NW)^2)+sum(IRB$Error))
table$`Negative Item Wording Reliability`[3] <- (sum(IRB$NW)^2)/((sum(IRB$IRB)^2)+(sum(IRB$Mood)^2)+(sum(IRB$NW)^2)+sum(IRB$Error))
table$`Total Reliability`[3] <- (table$`Substantive Reliability`[3]+table$`Mood Reliability`[3]+table$`Negative Item Wording Reliability`[3])
table$`% Mood Reliability`[3] <- (table$`Mood Reliability`[3]/table$`Total Reliability`[3])
table$`% Negative Item Wording Reliability`[3] <- (table$`Negative Item Wording Reliability`[3]/table$`Total Reliability`[3])

OCBI <- as.data.frame(m1.factorloadings[["1"]][["lambda"]])  %>%
  abs() %>%
  rownames_to_column %>%
  slice(18:24) %>%
  mutate(Sub.Rel = OCBI^2) %>%
  mutate(Mood.Rel = Mood^2) %>%
  mutate(NW.Rel = NW^2) %>%
  as.data.frame() %>%
  mutate(Error = (1-(Sub.Rel+Mood.Rel+NW.Rel)))

table$`Substantive Reliability`[5] <- (sum(OCBI$OCBI)^2)/((sum(OCBI$OCBI)^2)+(sum(OCBI$Mood)^2)+(sum(OCBI$NW)^2)+sum(OCBI$Error))
table$`Mood Reliability`[5] <- (sum(OCBI$Mood)^2)/((sum(OCBI$OCBI)^2)+(sum(OCBI$Mood)^2)+(sum(OCBI$NW)^2)+sum(OCBI$Error))
table$`Negative Item Wording Reliability`[5] <- (sum(OCBI$NW)^2)/((sum(OCBI$OCBI)^2)+(sum(OCBI$Mood)^2)+(sum(OCBI$NW)^2)+sum(OCBI$Error))
table$`Total Reliability`[5] <- (table$`Substantive Reliability`[5]+table$`Mood Reliability`[5]+table$`Negative Item Wording Reliability`[5])
table$`% Mood Reliability`[5] <- (table$`Mood Reliability`[5]/table$`Total Reliability`[5])
table$`% Negative Item Wording Reliability`[5] <- (table$`Negative Item Wording Reliability`[5]/table$`Total Reliability`[5])

OCBO <- as.data.frame(m1.factorloadings[["1"]][["lambda"]])  %>%
  abs() %>%
  rownames_to_column %>%
  slice(25:32) %>%
  mutate(Sub.Rel = OCBO^2) %>%
  mutate(Mood.Rel = Mood^2) %>%
  mutate(NW.Rel = NW^2) %>%
  as.data.frame() %>%
  mutate(Error = (1-(Sub.Rel+Mood.Rel+NW.Rel)))

table$`Substantive Reliability`[7] <- (sum(OCBO$OCBO)^2)/((sum(OCBO$OCBO)^2)+(sum(OCBO$Mood)^2)+(sum(OCBO$NW)^2)+sum(OCBO$Error))
table$`Mood Reliability`[7] <- (sum(OCBO$Mood)^2)/((sum(OCBO$OCBO)^2)+(sum(OCBO$Mood)^2)+(sum(OCBO$NW)^2)+sum(OCBO$Error))
table$`Negative Item Wording Reliability`[7] <- (sum(OCBO$NW)^2)/((sum(OCBO$OCBO)^2)+(sum(OCBO$Mood)^2)+(sum(OCBO$NW)^2)+sum(OCBO$Error))
table$`Total Reliability`[7] <- (table$`Substantive Reliability`[7]+table$`Mood Reliability`[7]+table$`Negative Item Wording Reliability`[7])
table$`% Mood Reliability`[7] <- (table$`Mood Reliability`[7]/table$`Total Reliability`[7])
table$`% Negative Item Wording Reliability`[7] <- (table$`Negative Item Wording Reliability`[7]/table$`Total Reliability`[7])

# Round columsn to nearest 100th
table <- table %>% 
  mutate_if(is.numeric, ~round(., 2))

# Call table 3
table3 <- table
```
```{r Table 1, results = 'asis', tab.cap = NULL, echo = FALSE, include = FALSE}
table2 %>%
  knitr::kable(format = "latex", booktabs=T ,escape = T) %>%
  kable_styling(latex_options = c("scale_down", "hold_position"),
                 repeat_header_method = c("replace")) %>%
  footnote(general = "* p < .05; ** p < .01; *** p < .001. ") %>%
  landscape() %>%
  kable_styling(full_width = T) %>%
  save_kable("Tables/Table1.pdf")
```
```{r Table 2, results = 'asis', tab.cap = NULL, echo = TRUE, include = FALSE}
table3 %>%
  knitr::kable(format = "latex", booktabs=T ,escape = T) %>%
  kable_styling(latex_options = c("scale_down", "hold_position"),
                 repeat_header_method = c("replace")) %>%
  footnote(general = "Total reliability is a sum of the substantive and method reliability components (see Williams et al., 2010). ") %>%
  landscape() %>%
  kable_styling(full_width = T) %>%
  save_kable("Tables/Table2.pdf")
```

Our proposed model had a close fit ($\chi^2$(`r df.ms1`) = `r csq.ms1`, *p* = `r p.ms1`, $\chi^2$/df = `r cdfratio.ms1`, CFI = `r cfi.ms1`, RMSEA = `r remsea.ms1`, SRMR = `r srmr.ms1`) and so we moved to interpreting the results. \Chunk9 contains the estimates for the latent construct correlations. A clear trend emerged. Specifically, all of the latent construct correlations were larger under the non-remedied condition relative to the remedied condition, indicating support for hypothesis 1. Crucially, whereas all of the substantive correlations were significant in the non-remedied condition (*p* < .001), this was not the case in the remedied condition (all *p*s were >= .05). 

# ---------------
# Insert Tables 1 and 2 about here
# ---------------

Table 2 contains the reliability decomposition for our key study variables. Another clear trend emerged. Specifically, the substantive reliability estimates were all smaller in the remedied condition compared to the non-remedied condition. This indicates that proximal causes of method variance inflated the estimate of substantive variance in the non-remedied condition and that using remedies reduces such common method variance, supporting hypothesis 2. 

# Discussion Study 1

Our first study provided strong evidence for the efficacy of bundling proximal remedies to address proximal causes of method variance. We found that in a condition where proximal remedies were bundled, the estimated construct correlations were all weakened. Crucially, these reductions were dramatic in that they rendered these correlations statistically non-significant, suggesting the these substantive linkages (as assessed by same-source designs) linking proactive personality to in-role behavior and OCB can be explained by measurement context and mood effects. In other words, for designs relying on single-sources, proximal causes of method variance account for the correlation linking self-descriptions of proactive personality and workplace behavior (i.e., in-role behavior, OCBI, OCBO). 

Additionally, an unexpected trend emerged regarding mood (see table 3). As the amount of the substantive variance captured was lower in the remedied condition, we also observed more variance attributable to mood. This finding suggests that when researchers use a non-remedied design and estimate the amount of method variance using a measured variable approach, they might *underestimate* the amount of method variance at play in their data. In other words, using procedural remedies for method variance might allow researchers to observe stronger measured method effects.

# Study 2 - Efficacy of Individual Proximal Remedies

With this study, we sought to isolate the effects of the specific remedies used in study one by running another experiment whereby participants were randomly assigned to a single proximal remedy for method variance. [^5] The same measurement model was used but individuals were randomly assigned to receive one of the proximal remedies tested in study 1. In other words, individuals were assigned to (a) a condition where a cover story was used to disguise the purpose of the study, (b) a condition where the item-order within scales was randomized, (c) a condition where the scale-order was randomized, (d) a condition where filler scales where included in between measures of the predictor and outcomes, or (e) a condition where whereby no remedies were applied. This allowed us to examine the effects of specific remedies. 

[^5]: A friendly reviewer, Alyssa McGonagle, recommended that we pursue this course of action.

# Methods - Study 2

## Sample, Procedure, and Analytical Approach
```{r Load Study 2 Data, message = FALSE, warning = FALSE, include = FALSE}
data <- read_sav("Data/Study 2.sav")
N <- as.numeric(nrow(data))

#Count of individuals who agree to participate.
N1 <- as.numeric(freq_vect(data$IC1)[1,"Count"])  #Control
N2 <- as.numeric(freq_vect(data$IC2)[1,"Count"])  #Coverstory
N3 <- as.numeric(freq_vect(data$IC3)[1,"Count"])  #Randomized Scales
N4 <- as.numeric(freq_vect(data$FOR4)[1,"Count"]) #Randomized Items Within Scales
N5 <- as.numeric(freq_vect(data$FOR5)[1,"Count"]) #Filler Scales Used
Nall <- sum(N1,N2,N3,N4,N5)

#Recode consent and manipulation check so that only those who passed have their data examined. 
data$IC1[data$IC1 =="I Agree"] <- 1
data$IC2[data$IC2 =="I Agree"] <- 1
data$IC3[data$IC3 =="I Agree"] <- 1
data$FOR4[data$FOR4 =="I Agree"] <- 1
data$FOR5[data$FOR5 =="I Agree"] <- 1

#Subset in those individuals who paid attention and understood the purpose of the survey as it was presented to them.
Likert <- subset(data, IC1 == "1" & Dupe1 == "1" | IC2 == "1" & Dupe1 == "2" | IC3 == "1" & Dupe1 == "1" | FOR4 == "1" & Dupe1 == "1" |  FOR5 == "1" & Dupe1 == "1")

#Delete cases who did not complete the demographic questionnaire.
Likert <- Likert[complete.cases(Likert[ , 376:380]),]

#Get success rates (SR) for responding correctly across both conditions.
n1 <- as.numeric(freq_vect(Likert$IC1)[1,"Count"])  #Control
n2 <- as.numeric(freq_vect(Likert$IC2)[1,"Count"])  #Coverstory
n3 <- as.numeric(freq_vect(Likert$IC3)[1,"Count"])  #Randomized Scales
n4 <- as.numeric(freq_vect(Likert$FOR4)[1,"Count"]) #Randomized Items Within Scales
n5 <- as.numeric(freq_vect(Likert$FOR5)[1,"Count"]) #Filler Scales Used
n <- n1+n2+n2+n4+n5
sr1 <- round(n1/N1,2)
sr2 <- round(n2/N2,2)
sr3 <- round(n3/N3,2)
sr4 <- round(n4/N4,2)
sr5 <- round(n5/N5,2)
srall <- round(n/(N1+N2+N3+N4+N5),2)

##Collapse like data across columns.
###Conditions; build independent data sets and then rename the condition variable.
###1 Control
Likert$IC1[is.na(Likert$IC1)] <- 0
Likert$IC1 <- as.numeric(Likert$IC1)
###2 Cover Story Manipulation
Likert$IC2 <- ifelse(Likert$IC2 == 1, c("2"))
Likert$IC2[is.na(Likert$IC2)] <- 0
Likert$IC2 <- as.numeric(Likert$IC2)
###3 Randomized Scales
Likert$IC3 <- ifelse(Likert$IC3 == 1, c("3"))
Likert$IC3[is.na(Likert$IC3)] <- 0
Likert$IC3 <- as.numeric(Likert$IC3)
###4 Randomized Items
Likert$FOR4 <- ifelse(Likert$FOR4 == 1, c("4"))
Likert$FOR4[is.na(Likert$FOR4)] <- 0
Likert$FOR4 <- as.numeric(Likert$FOR4)
###5 Filler Scales
Likert$FOR5 <- ifelse(Likert$FOR5 == 1, c("5"))
Likert$FOR5[is.na(Likert$FOR5)] <- 0
Likert$FOR5 <- as.numeric(Likert$FOR5)
#Combine condition Likert into single variable column
Likert$COND <- Likert$IC1 + Likert$IC2+ Likert$IC3 + Likert$FOR4 + Likert$FOR5
#Apply value lables. 
Likert$COND <- factor(Likert$COND, levels = c(1,2,3,4,5), labels = c("Control", "CSManipulation","ScaleRand","ItemRand","FillerScales"))

#Consolidate item response measures
##Form Likert datasets, re-lable values, convert to numeric data, consolidate measures, and delete data.
agree <- Likert[c(21:60,89:138,157:196,225:236,262:289,293:332)]
#agree <- ifelse(agree == "Strongly Disagree", 1, ifelse(agree == "Disagree", 2, ifelse(agree == "Neither Agree nor Disagree", 3, ifelse(agree == "Agree", 4, ifelse(agree == "Strongly Agree", 5,0)))))
agree <- as.data.frame(agree)
agree[is.na(agree)] <- 0

#####Proactive Personality 
agree$PP1	<-	agree$PP1_1	+	agree$PP2_1	+	agree$PP5_1	+	agree$PP6_1	+	agree$PPx_1
agree$PP2	<-	agree$PP1_2	+	agree$PP2_2	+	agree$PP5_2	+	agree$PP6_2	+	agree$PPx_2
agree$PP3	<-	agree$PP1_3	+	agree$PP2_3	+	agree$PP5_3	+	agree$PP6_3	+	agree$PPx_3
agree$PP4	<-	agree$PP1_4	+	agree$PP2_4	+	agree$PP5_4	+	agree$PP6_4	+	agree$PPx_4
agree$PP5	<-	agree$PP1_5	+	agree$PP2_5	+	agree$PP5_5	+	agree$PP6_5	+	agree$PPx_5
agree$PP6	<-	agree$PP1_6	+	agree$PP2_6	+	agree$PP5_6	+	agree$PP6_6	+	agree$PPx_6
agree$PP7	<-	agree$PP1_7	+	agree$PP2_7	+	agree$PP5_7	+	agree$PP6_7	+	agree$PPx_7
agree$PP8	<-	agree$PP1_8	+	agree$PP2_8	+	agree$PP5_8	+	agree$PP6_8	+	agree$PPx_8
agree$PP9	<-	agree$PP1_9	+	agree$PP2_9	+	agree$PP5_9	+	agree$PP6_9	+	agree$PPx_9
agree$PP10	<-	agree$PP1_10	+	agree$PP2_10	+	agree$PP5_10	+	agree$PP6_10	+	agree$PPx_10

#####OCBI
agree$OCBI1	<-	agree$OCBI1_1	+	agree$OCBI2_1	+	agree$OCBI5_1	+	agree$OCBI6_1	+	agree$OCBIx_1
agree$OCBI2	<-	agree$OCBI1_3	+	agree$OCBI2_3	+	agree$OCBI5_3	+	agree$OCBI6_3	+	agree$OCBIx_3
agree$OCBI3	<-	agree$OCBI1_4	+	agree$OCBI2_4	+	agree$OCBI5_4	+	agree$OCBI6_4	+	agree$OCBIx_4
agree$OCBI4	<-	agree$OCBI1_5	+	agree$OCBI2_5	+	agree$OCBI5_5	+	agree$OCBI6_5	+	agree$OCBIx_5
agree$OCBI5	<-	agree$OCBI1_6	+	agree$OCBI2_6	+	agree$OCBI5_6	+	agree$OCBI6_6	+	agree$OCBIx_6
agree$OCBI6	<-	agree$OCBI1_7	+	agree$OCBI2_7	+	agree$OCBI5_7	+	agree$OCBI6_7	+	agree$OCBIx_7
agree$OCBI7	<-	agree$OCBI1_8	+	agree$OCBI2_8	+	agree$OCBI5_8	+	agree$OCBI6_8	+	agree$OCBIx_8

#####OCBO
agree$OCBO1	<-	agree$OCBO1_1	+	agree$OCBO2_1	+	agree$OCBO5_1	+	agree$OCBO6_1	+	agree$OCBOx_1
agree$OCBO2	<-	agree$OCBO1_2	+	agree$OCBO2_2	+	agree$OCBO5_2	+	agree$OCBO6_2	+	agree$OCBOx_2
agree$OCBO3	<-	agree$OCBO1_3	+	agree$OCBO2_3	+	agree$OCBO5_3	+	agree$OCBO6_3	+	agree$OCBOx_3
agree$OCBO4	<-	agree$OCBO1_5	+	agree$OCBO2_5	+	agree$OCBO5_5	+	agree$OCBO6_5	+	agree$OCBOx_5
agree$OCBO5	<-	agree$OCBO1_6	+	agree$OCBO2_6	+	agree$OCBO5_6	+	agree$OCBO6_6	+	agree$OCBOx_6
agree$OCBO6	<-	agree$OCBO1_7	+	agree$OCBO2_7	+	agree$OCBO5_7	+	agree$OCBO6_7	+	agree$OCBOx_7
agree$OCBO7	<-	agree$OCBO1_8	+	agree$OCBO2_8	+	agree$OCBO5_8	+	agree$OCBO6_8	+	agree$OCBOx_8

#####IRB
agree$IRB1	<-	agree$IRB1_1	+	agree$IRB2_1	+	agree$IRB5_1	+	agree$IRB6_1	+	agree$IRBx_1
agree$IRB2	<-	agree$IRB1_3	+	agree$IRB2_3	+	agree$IRB5_3	+	agree$IRB6_3	+	agree$IRBx_3
agree$IRB3	<-	agree$IRB1_4	+	agree$IRB2_4	+	agree$IRB5_4	+	agree$IRB6_4	+	agree$IRBx_4
agree$IRB4	<-	agree$IRB1_5	+	agree$IRB2_5	+	agree$IRB5_5	+	agree$IRB6_5	+	agree$IRBx_5
agree$IRB5	<-	agree$IRB1_6	+	agree$IRB2_6	+	agree$IRB5_6	+	agree$IRB6_6	+	agree$IRBx_6
agree$IRB6	<-	agree$IRB1_8	+	agree$IRB2_8	+	agree$IRB5_8	+	agree$IRB6_8	+	agree$IRBx_8
agree$IRB7	<-	agree$IRB1_9	+	agree$IRB2_9	+	agree$IRB5_9	+	agree$IRB6_9	+	agree$IRBx_9

#####Inattentive Responding
agree$IR	<-	agree$PP1_11	+	agree$PP2_11	+	agree$PP5_11	+	agree$PP6_11	+	agree$PPx_11

#####Consistency Motif
agree$CM1	<-	agree$PP1_12	+	agree$PP2_12	+	agree$PP5_12	+	agree$PP6_12	+	agree$PPx_12 #Brave
agree$CM2	<-	agree$IRB1_2	+	agree$IRB2_2	+	agree$IRB5_2	+	agree$IRB6_2	+	agree$IRBx_2 #Courageous
agree$CM3	<-	agree$OCBI1_9	+	agree$OCBI2_9	+	agree$OCBI5_9	+	agree$OCBI6_9	+	agree$OCBIx_9 #Talkative
agree$CM4	<-	agree$OCBO1_9	+	agree$OCBO2_9	+	agree$OCBO5_9	+	agree$OCBO6_9	+	agree$OCBOx_9 #Silent Person
agree$CM5	<-	agree$IRB1_7	+	agree$IRB2_7	+	agree$IRB5_7	+	agree$IRB6_7	+	agree$IRBx_7 #Optimistic
agree$CM6	<-	agree$OCBO1_4	+	agree$OCBO2_4	+	agree$OCBO5_4	+	agree$OCBO6_4	+	agree$OCBOx_4 #Pessimistic Person
agree$CM7	<-	agree$IRB1_10	+	agree$IRB2_10	+	agree$IRB5_10	+	agree$IRB6_10	+	agree$IRBx_10 #Seldom feel blue.
agree$CM8	<-	agree$OCBI1_2	+	agree$OCBI2_2	+	agree$OCBI5_2	+	agree$OCBI6_2	+	agree$OCBIx_2 #Often feel blue.

#Consolidate agree data
agree <- agree[c(211:250)]

#PANAS
PANAS <- Likert[c(65:84,133:152,201:220,241:260,337:356)]
#PANAS <- ifelse(PANAS == "Very slightly or not at all", 1, ifelse(PANAS == "A little", 2, ifelse(PANAS == "Moderately", 3, ifelse(PANAS == "Quite a bit", 4, ifelse(PANAS == "Extremely", 5, 0)))))
PANAS <- as.data.frame(PANAS)
PANAS[is.na(PANAS)] <- 0
PANAS$PA1	<-	PANAS$PAff1_1	+	PANAS$PAff2_1	+	PANAS$PAff5_1	+	PANAS$PAff6_1	+	PANAS$PAffx_1
PANAS$PA2	<-	PANAS$PAff1_2	+	PANAS$PAff2_2	+	PANAS$PAff5_2	+	PANAS$PAff6_2	+	PANAS$PAffx_2
PANAS$PA3	<-	PANAS$PAff1_3	+	PANAS$PAff2_3	+	PANAS$PAff5_3	+	PANAS$PAff6_3	+	PANAS$PAffx_3
PANAS$PA4	<-	PANAS$PAff1_4	+	PANAS$PAff2_4	+	PANAS$PAff5_4	+	PANAS$PAff6_4	+	PANAS$PAffx_4
PANAS$PA5	<-	PANAS$PAff1_5	+	PANAS$PAff2_5	+	PANAS$PAff5_5	+	PANAS$PAff6_5	+	PANAS$PAffx_5
PANAS$PA6	<-	PANAS$PAff1_6	+	PANAS$PAff2_6	+	PANAS$PAff5_6	+	PANAS$PAff6_6	+	PANAS$PAffx_6
PANAS$PA7	<-	PANAS$PAff1_7	+	PANAS$PAff2_7	+	PANAS$PAff5_7	+	PANAS$PAff6_7	+	PANAS$PAffx_7
PANAS$PA8	<-	PANAS$PAff1_8	+	PANAS$PAff2_8	+	PANAS$PAff5_8	+	PANAS$PAff6_8	+	PANAS$PAffx_8
PANAS$PA9	<-	PANAS$PAff1_9	+	PANAS$PAff2_9	+	PANAS$PAff5_9	+	PANAS$PAff6_9	+	PANAS$PAffx_9
PANAS$PA10	<-	PANAS$PAff1_10	+	PANAS$PAff2_10	+	PANAS$PAff5_10	+	PANAS$PAff6_10	+	PANAS$PAffx_10
PANAS$NA1	<-	PANAS$NAff1_1	+	PANAS$NAff2_1	+	PANAS$NAff5_1	+	PANAS$NAff6_1	+	PANAS$NAffx_1
PANAS$NA2	<-	PANAS$NAff1_2	+	PANAS$NAff2_2	+	PANAS$NAff5_2	+	PANAS$NAff6_2	+	PANAS$NAffx_2
PANAS$NA3	<-	PANAS$NAff1_3	+	PANAS$NAff2_3	+	PANAS$NAff5_3	+	PANAS$NAff6_3	+	PANAS$NAffx_3
PANAS$NA4	<-	PANAS$NAff1_4	+	PANAS$NAff2_4	+	PANAS$NAff5_4	+	PANAS$NAff6_4	+	PANAS$NAffx_4
PANAS$NA5	<-	PANAS$NAff1_5	+	PANAS$NAff2_5	+	PANAS$NAff5_5	+	PANAS$NAff6_5	+	PANAS$NAffx_5
PANAS$NA6	<-	PANAS$NAff1_6	+	PANAS$NAff2_6	+	PANAS$NAff5_6	+	PANAS$NAff6_6	+	PANAS$NAffx_6
PANAS$NA7	<-	PANAS$NAff1_7	+	PANAS$NAff2_7	+	PANAS$NAff5_7	+	PANAS$NAff6_7	+	PANAS$NAffx_7
PANAS$NA8	<-	PANAS$NAff1_8	+	PANAS$NAff2_8	+	PANAS$NAff5_8	+	PANAS$NAff6_8	+	PANAS$NAffx_8
PANAS$NA9	<-	PANAS$NAff1_9	+	PANAS$NAff2_9	+	PANAS$NAff5_9	+	PANAS$NAff6_9	+	PANAS$NAffx_9
PANAS$NA10	<-	PANAS$NAff1_10	+	PANAS$NAff2_10	+	PANAS$NAff5_10	+	PANAS$NAff6_10	+	PANAS$NAffx_10
#Consolidate "PANAS" dataset.
PANAS <- PANAS[c(101:120)]

#Mood
Mood <- as.data.frame(Likert[c(85,153,221,261,357)])
Mood[is.na(Mood)] <- 0
Mood$MOOD	<-	Mood$Mood1	+	Mood$MOOD2	+	Mood$MOOD5	+	Mood$Mood6	+	Mood$MOODx
#Consolidate Mood
MOOD <- Mood[c(6)]

#####HALO
Halo <- as.data.frame(Likert[c(61:64,129:132,197:200,237:240,333:336)])
Halo[is.na(Halo)] <- 0
Halo$HALO1	<-	Halo$HALO1_1	+	Halo$HALO2_1	+	Halo$HALO5_1	+	Halo$HALO6_1	+	Halo$HALOx_1 # Facial attractiveness
Halo$HALO2	<-	Halo$HALO1_2	+	Halo$HALO2_2	+	Halo$HALO5_2	+	Halo$HALO6_2	+	Halo$HALOx_2 # Intelligence
Halo$HALO3	<-	Halo$HALO1_3	+	Halo$HALO2_3	+	Halo$HALO5_3	+	Halo$HALO6_3	+	Halo$HALOx_3 # Atheletic ability
Halo$HALO4	<-	Halo$HALO1_4	+	Halo$HALO2_4	+	Halo$HALO5_4	+	Halo$HALO6_4	+	Halo$HALOx_4 # Trivia knowledge
#Consolidate Halo
HALO <- Halo[c(21:24)]

#Recode 0 values to missing.
l <- cbind(agree,PANAS,MOOD,HALO,Likert[c(358:365)])
l <- as.data.frame(apply(l, 2, function(x) {x <- recode(x,"0=NA"); x}))

#Missing data analysis using 'sapply(data2, function(x) sum(is.na(x)))' revealed some missing likert data. 
init <- mice(l[c(-8)], maxit = 0)
meth <- init$method
predM <- init$predictorMatrix
meth[c("IRB3")]="norm"
meth[c("NA3")]="norm"
imputed <- mice(l, method=meth, predictorMatrix=predM, m=5)
l <- complete(imputed)
#Round imputed data to nearest whole number for estimation purposes. 
l$IRB3 <- ceiling(l$IRB3)
l$NA3 <- ceiling(l$NA3)

#Retain only variables used for testing purposes.
data2 <- cbind(Likert[c(376:386)],l,Likert[c(365:375)])
n <- as.numeric(nrow(data2))

#Setup demographics.
#Rename variables.
colnames(data2)[colnames(data2)=="Age"] <- "AGE"
colnames(data2)[colnames(data2)=="Race"] <- "RACE"
colnames(data2)[colnames(data2)=="Gender"] <- "GENDER"
colnames(data2)[colnames(data2)=="Education"] <- "EDUCAT"
colnames(data2)[colnames(data2)=="EmployStat"] <- "EMPLOYED"
colnames(data2)[colnames(data2)=="EmployStatYrs"] <- "JOBTENURE"
colnames(data2)[colnames(data2)=="JobTitle"] <- "PROF_SPECIFY"

#Calculate descriptives
M <- mean(data2$AGE, na.rm = TRUE)
SD <- sd(data2$AGE, na.rm = TRUE)

#Calculate frequencies
##Recode single "f" to female. 
#data2$GENDER[data2$GENDER=="f"] <- 1
female.n <- as.numeric(freq_vect(data2$GENDER)[1,"Count"])
female.p <- as.numeric(freq_vect(data2$GENDER)[1,"Percentage"])
white.n <- as.numeric(freq_vect(data2$RACE)[7,"Count"])
white.p <- as.numeric(freq_vect(data2$RACE)[7,"Percentage"])
fulltime.n <- as.numeric(freq_vect(data2$EMPLOYED)[3,"Count"])
fulltime.p <- as.numeric(freq_vect(data2$EMPLOYED)[3,"Percentage"])

# Dummy code the conodition variable for lavaan.
data2 <- dummy_cols(data2, select_columns = "COND")
```
```{r Study 2: Construct Validity for Affect, include = FALSE}
mood.cfa <- '
#Affectivity
PA =~ PA1 + PA2 + PA3 + PA4 + PA5 + PA6 + PA7 + PA8 + PA9 + PA10 
Na =~ NA1 + NA2 + NA3 + NA4 + NA5 + NA6 + NA7 + NA8 + NA9 + NA10 
AP =~ PA1 + PA2 + PA3 + PA4 + PA5 + PA6 + PA7 + PA8 + PA9 + PA10 + NA1 + NA2 + NA3 + NA4 + NA5 + NA6 + NA7 + NA8 + NA9 + NA10 

#Constrain bifactor covariances
PA ~~ 0*Na
PA ~~ 0*AP
Na ~~ 0*AP
'
mood <- cfa(mood.cfa, data = data2, estimator = "DWLS", parameterization = "theta", information = "expected",  std.lv=TRUE, test = "Satorra-Bentler")
summary(mood, standardized = TRUE, fit.measures =TRUE)
affect <- as.data.frame(predict(mood))

#Bind to data
data2 <- cbind(data2,affect[c(1)])
```
```{r Study 2: Cronbach Alphas, include = FALSE}
#Create scale scores
my.scales <- scoreItems(my.keys.list,data2)
PP.alpha <- round(my.scales[["alpha"]][1],2)
IRB.alpha <- round(my.scales[["alpha"]][2],2)
OCBI.alpha <- round(my.scales[["alpha"]][3],2)
OCBO.alpha <- round(my.scales[["alpha"]][4],2)
PA.alpha <- round(my.scales[["alpha"]][5],2)
```

Among the `r apa(N,0,T)` workers from Amazon's Mechanical Turk who considered participating in our study, `r apa(Nall,0,T)` consented to participating in our study. These `r apa(Nall,0,T)` participants were randomly assigned to one of five conditions: a control or non-remedied condition (n = `r apa(N1,0,T)`), a condition where a cover story was used to blind the purpose of the study (n = `r apa(N2,0,T)`), a condition where scale order was randomized (n = `r apa(N3,0,T)`), a condition where items within scales were randomized (n = `r apa(N4,0,T)`), or a condition wherein filler scales were employed (n = `r apa(N5,0,T)`). Respondents who incorrectly responded to an attention check item were excluded from the survey during administration and so their data were not collected. Five more cases were removed for failing to report demographics and abandoning the survey. The rates for successfully passing our manipulation check for the study purpose were between `r apa(sr5,2,T)` and `r apa(sr4,2,T)` with an overall success rate of `r apa(srall,2,T)` and were much higher than study 1. [^6] This filtering process resulted in a final sample of `r apa(n1,0,T)` individuals in the control condition, `r apa(n2,0,T)` in the cover story condition, `r apa(n3,0,T)` in the randomized scales condition, `r apa(n4,0,T)` in the randomized items within scales condition, and `r apa(n5,0,T)` in the filler scales condition. The sample was female biased (n = `r apa(female.n,0,T)`), predominantly Caucasian, and the average age was *M* = `r apa(M,2,T)` (*SD* = `r apa(SD,2,T)`).  The majority (n = `r apa(fulltime.n,0,T)`) of respondents worked full-time. 

[^6]: Our relatively higher success rates, compared to study 1, might be attributed tentatively to our using both a video to communicate the purpose of the survey and/or requiring participants to restate the purpose of our study in their own terms in writing prior to participating in our study.

The same measures and modeling approach used in study 1 were used in study 2. Cronbach alphas ranged from `r apa(OCBO.alpha)` for OCBO to `r apa(PA.alpha)` for positive affectivity. Again, cases of missing item level data were handled using the normal model approach [see @WuComparisonImputationStrategies2015]. 

# Results - Study 2
```{r Study 2: Build for Descriptive Statistics, include=FALSE}
#Create Descriptives Table
scales <- my.scales[["scores"]]
scales <- cbind(data2[c("COND_Control","COND_CSManipulation","COND_FillerScales","COND_ScaleRand","COND_ItemRand","AGE","GENDER")],scales,data2[c("MOOD")])
table4 <- apa.cor.table(scales, table.number = 1, landscape = FALSE)
```
```{r Study 2: Table 4: Descriptives, results="asis"}
#table4
```
```{r Study 2: Model Testing, include = FALSE}
m2.cfa <- ' 
##Substantive factors
PP =~ c(pp1a, pp1b, pp1c, pp1d, pp1e)*PP1 + c(pp2a, pp2b, pp2c, pp2d, pp2e)*PP2 + c(pp3a,pp3b, pp3c, pp3d, pp3e)*PP3 + c(pp4a,pp4b, pp4c, pp4d, pp4e)*PP4 + c(pp5a,pp5b, pp5c, pp5d, pp5e)*PP5 + c(pp6a,pp6b, pp6c, pp6d, pp6e)*PP6 + c(pp7a,pp7b, pp7c, pp7d, pp7e)*PP7 + c(pp8a,pp8b, pp8c, pp8d, pp8e)*PP8 + c(pp9a,pp9b, pp9c, pp9d, pp9e)*PP9 + c(pp10a,pp10b, pp10c, pp10d, pp10e)*PP10
IRB =~ c(irb1a,irb1b,irb1c,irb1d,irb1e)*IRB1 + c(irb2a,irb2b,irb2c,irb2d,irb2e)*IRB2 + c(irb3a,irb3b,irb3c,irb3d,irb3e)*IRB3 + c(irb4a,irb4b,irb4c,irb4d,irb4e)*IRB4 + c(irb5a,irb5b,irb5c,irb5d,irb5e)*IRB5 + c(irb6a,irb6b,irb6c,irb6d,irb6e)*IRB6 + c(irb7a,irb7b,irb7c,irb7d,irb7e)*IRB7
OCBI =~ c(ocbi1a,ocbi1b,ocbi1c,ocbi1d,ocbi1e)*OCBI1 + c(ocbi2a,ocbi2b,ocbi2c,ocbi2d,ocbi2e)*OCBI2 + c(ocbi3a,ocbi3b,ocbi3c,ocbi3d,ocbi3e)*OCBI3 + c(ocbi4a,ocbi4b,ocbi4c,ocbi4d,ocbi4e)*OCBI4 + c(ocbi5a,ocbi5b,ocbi5c,ocbi5d,ocbi5e)*OCBI5 + c(ocbi6a,ocbi6b,ocbi6c,ocbi6d,ocbi6e)*OCBI6 + c(ocbi7a,ocbi7b,ocbi7c,ocbi7d,ocbi7e)*OCBI7
OCBO =~ c(ocbo1a,ocbo1b,ocbo1c,ocbo1d,ocbo1e)*OCBO1 + c(ocbo2a,ocbo2b,ocbo2c,ocbo2d,ocbo2e)*OCBO2 + c(ocbo3a,ocbo3b,ocbo3c,ocbo3d,ocbo3e)*OCBO3 + c(ocbo4a,ocbo4b,ocbo4c,ocbo4d,ocbo4e)*OCBO4 + c(ocbo5a,ocbo5b,ocbo5c,ocbo5d,ocbo5e)*OCBO5 + c(ocbo6a,ocbo6b,ocbo6c,ocbo6d,ocbo6e)*OCBO6 + c(ocbo7a,ocbo7b,ocbo7c,ocbo7d,ocbo7e)*OCBO7

# Negative Item Content Factor
NW =~ c(nw1a, nw1b, nw1c, nw1d, nw1e)*IRB6 + c(nw2a, nw2b, nw2c, nw2d, nw2e)*IRB7 + c(nw3a, nw3b, nw3c, nw3d, nw3e)*OCBO3 + c(nw4a, nw4b, nw4c, nw4d, nw4e)*OCBO4 + c(nw5a, nw5b, nw5c, nw5d, nw5e)*OCBO5
NW ~~ c(1,1,1,1,1)*NW
NW ~ c(0,0,0,0,0)*1
NW ~~ c(0,0,0,0,0)*IRB
NW ~~ c(0,0,0,0,0)*OCBO
NW ~~ c(0,0,0,0,0)*PP
NW ~~ c(0,0,0,0,0)*OCBI

#Factor variances are fixed to 1 to allow estimation.
PP ~~ c(1,1,1,1,1)*PP
IRB ~~ c(1,1,1,1,1)*IRB
OCBI ~~ c(1,1,1,1,1)*OCBI
OCBO ~~ c(1,1,1,1,1)*OCBO

#Factor means of latent factors for all groups are fixed at zero to allow identification.
PP ~ c(0,0,0,0,0)*1
IRB ~ c(0,0,0,0,0)*1
OCBI ~ c(0,0,0,0,0)*1
OCBO ~ c(0,0,0,0,0)*1

##Factor covariances labeled.
PP ~~ c(ppirba,ppirbb,ppirbc,ppirbd,ppirbe)*IRB
PP ~~ c(ppocbia,ppocbib,ppocbic,ppocbid,ppocbie)*OCBI
PP ~~ c(ppocboa,ppocbob,ppocboc,ppocbod,ppocboe)*OCBO
IRB ~~ c(irbocbia,irbocbib,irbocbic,irbocbid,irbocbie)*OCBI
IRB ~~ c(irbocboa,irbocbob,irbocboc,irbocbod,irbocboe)*OCBO
OCBI ~~ c(ocbioa,ocbiob,ocbioc,ocbiod,ocbioe)*OCBO

# Add in "Mood" as a single-item latent construct that is an uncorrelated with all factors.
Mood =~ .95*MOOD + c(mpp1a, mpp1b, mpp1c, mpp1d, mpp1e)*PP1 + c(mpp2a, mpp2b, mpp2c, mpp2d, mpp2e)*PP2 + c(mpp3a,mpp3b,mpp3c,mpp3d,mpp3e)*PP3 + c(mpp4a,mpp4b,mpp4c,mpp4d,mpp4e)*PP4 + c(mpp5a,mpp5b,mpp5c,mpp5d,mpp5e)*PP5 + c(mpp6a,mpp6b,mpp6c,mpp6d,mpp6e)*PP6 + c(mpp7a,mpp7b,mpp7c,mpp7d,mpp7e)*PP7 + c(mpp8a,mpp8b,mpp8c,mpp8d,mpp8e)*PP8 + c(mpp9a,mpp9b,mpp9c,mpp9d,mpp9e)*PP9 + c(mpp10a,mpp10b,mpp10c,mpp10d,mpp10e)*PP10 + c(mirb1a,mirb1b,mirb1c,mirb1d,mirb1e)*IRB1 + c(mirb2a,mirb2b,mirb2c,mirb2d,mirb2e)*IRB2 + c(mirb3a,mirb3b,mirb3c,mirb3d,mirb3e)*IRB3 + c(mirb4a,mirb4b,mirb4c,mirb4d,mirb4e)*IRB4 + c(mirb5a,mirb5b,mirb5c,mirb5d,mirb5e)*IRB5 + c(mirb6a,mirb6b,mirb6c,mirb6d,mirb6e)*IRB6 + c(mirb7a,mirb7b,mirb7c,mirb7d,mirb7e)*IRB7 + c(mocbi1a,mocbi1b,mocbi1c,mocbi1d,mocbi1e)*OCBI1 + c(mocbi2a,mocbi2b,mocbi2c,mocbi2d,mocbi2e)*OCBI2 + c(mocbi3a,mocbi3b,mocbi3c,mocbi3d,mocbi3e)*OCBI3 + c(mocbi4a,mocbi4b,mocbi4c,mocbi4d,mocbi4e)*OCBI4 + c(mocbi5a,mocbi5b,mocbi5c,mocbi5d,mocbi5e)*OCBI5 + c(mocbi6a,mocbi6b,mocbi6c,mocbi6d,mocbi6e)*OCBI6 + c(mocbi7a,mocbi7b,mocbi7c,mocbi7d,mocbi7e)*OCBI7 + c(mocbo1a,mocbo1b,mocbo1c,mocbo1d,mocbo1e)*OCBO1 + c(mocbo2a,mocbo2b,mocbo2c,mocbo2d,mocbo2e)*OCBO2 + c(mocbo3a,mocbo3b,mocbo3c,mocbo3d,mocbo3e)*OCBO3 + c(mocbo4a,mocbo4b,mocbo4c,mocbo4d,mocbo4e)*OCBO4 + c(mocbo5a,mocbo5b,mocbo5c,mocbo5d,mocbo5e)*OCBO5 + c(mocbo6a,mocbo6b,mocbo6c,mocbo6d,mocbo6e)*OCBO6 + c(mocbo7a,mocbo7b,mocbo7c,mocbo7d,mocbo7e)*OCBO7
 # (see Anderson & Gerbing, 1988; Sorbom & Joreskig, 1982; as cited by Petrescu, 2013)
MOOD ~~ (1.580939)*(1-.85)*MOOD

# Add in PA as a single-item latent construct that is an uncorrelated with all factors.
PosAff =~ .95*PA
PA ~~ (0.7321512)*(1-.92)*PA

# Make uncorrelated with NW
NW ~~ 0*PosAff

# Add in causal sructure for PA and Mood
Mood ~ PosAff # PA causes mood

# Mood as a proximal cause of method variance
Mood ~~ 0*PP # Mood uncorrelated with proactive personality
Mood ~~ 0*IRB # Mood uncorrelated with IRB
Mood ~~ 0*OCBI # Mood uncorreated with OCBI
Mood ~~ 0*OCBO # Mood uncorrelated with OCBO
NW ~~ 0*Mood # Mood uncorrelated with NW
'

m2 <- cfa(m2.cfa, data = data2, group = "COND", estimator = "DWLS", parameterization = "theta", information = "expected", std.lv=TRUE,test = "Satorra-Bentler")
summary(m2, standardized = TRUE, fit.measures = TRUE)
fitm2.m <- fitmeasures(m2)

#Gather stats
csq.ms2 <- round(as.numeric(fitm2.m[c(6)]), digits = 2) ##Chi-Square
df.ms2 <- round(as.numeric(fitm2.m[c(7)]), digits = 2) ##df
cdfratio.ms2 <- round(csq.ms2/df.ms2,2) ## Chi-square / df ratio
p.ms2 <- pvalr(as.numeric(fitm2.m[c(8)]), digits = 2) ##Chi-Square p value
cssf.ms2 <- round(as.numeric(fitm2.m[c(9)]), digits = 2) ##Chisq.scaling factor
cfi.ms2 <- round(as.numeric(fitm2.m[c(27)]), digits = 2) #CFI robust
remsea.ms2 <- round(as.numeric(fitm2.m[c(44)]), digits = 3) ##RMSEA robust
remseal.ms2 <- round(as.numeric(fitm2.m[c(45)]), digits = 3) #RMSEA lower robust
remseau.ms2 <- round(as.numeric(fitm2.m[c(46)]), digits = 3) #RMSEA upper robust
srmr.ms2<- round(as.numeric(fitm2.m[c(50)]), digits = 3) #SRMR
```
```{r Study 2: Build Latent Correlation and Reliability Decomposition Tables, include = FALSE}
# Build table for latent covariances
factors <- c("Proactive Personality","","","","")
table <- as.data.frame(cbind(factors,"Condition","In-Role Behavior","","OCBI","","OCBO",""))
colnames(table)[1] <- "Predictor"
colnames(table)[2] <- "Condition"
colnames(table)[3] <- "In-Role Behavior"
colnames(table)[4] <- "In-Role Behaviorp"
colnames(table)[5] <- "OCBI"
colnames(table)[6] <- "OCBIp"
colnames(table)[7] <- "OCBO"
colnames(table)[8] <- "OCBOp"
table$Condition <- as.character(table$Condition)
table$`In-Role Behavior` <- as.numeric(table$`In-Role Behavior`)
table$`In-Role Behaviorp` <- as.character(table$`In-Role Behavior`)
table$`OCBI` <- as.numeric(table$`OCBI`)
table$OCBIp <- as.character(table$OCBIp)
table$`OCBO` <- as.numeric(table$`OCBO`)
table$OCBOp <- as.character(table$OCBOp)
table$Condition[1] <- "No Remedies"
table$Condition[2] <- "Cover Story"
table$Condition[3] <- "Filler Scales"
table$Condition[4] <- "Scale Randomization"
table$Condition[5] <- "Item Randomization"

# Extract pvalues and significance levels for mapping purposes.
data <- parameterEstimates(m2, standardized=TRUE) %>% 
  dplyr::filter(op == "~~") %>%
  dplyr::filter(label == "ppirba" | label == "ppirbb" | label == "ppirbc" | label == "ppirbd" | label == "ppirbe" | label == "ppocbia" | label == "ppocbib" | label == "ppocbic" | label == "ppocbid" | label == "ppocbie" | label == "ppocboa" | label == "ppocbob" | label == "ppocboc" | label == "ppocbod" | label == "ppocboe") %>%
  select(label, est, pvalue) %>%
  mutate_if(is.numeric, ~round(., 2)) 
data$p <- stars.pval(data$pvalue)

# Extract the latent covariances, map on significance levels, and place into table.
m2.covariances<- inspect(m2,"std.all")
table$`In-Role Behavior`[1] <- m2.covariances[["Control"]][["psi"]][1,2]  # PP-IRBControl
table$`In-Role Behaviorp`[1] <- data %>% dplyr::filter(label == "ppirbb") %>% select(p) 
table$`In-Role Behavior`[2] <- m2.covariances[["CSManipulation"]][["psi"]][1,2]  # PP-IRBCSManipulation
table$`In-Role Behaviorp`[2] <- data %>% dplyr::filter(label == "ppirba") %>% select(p) 
table$`In-Role Behavior`[3] <- abs(m2.covariances[["FillerScales"]][["psi"]][1,2])  # PP-IRBFillerScalesItemRand
table$`In-Role Behaviorp`[3] <- data %>% dplyr::filter(label == "ppirbd") %>% select(p) 
table$`In-Role Behavior`[4] <- m2.covariances[["ScaleRand"]][["psi"]][1,2]  # PP-IRBScaleRand
table$`In-Role Behaviorp`[4] <- data %>% dplyr::filter(label == "ppirbe") %>% select(p) 
table$`In-Role Behavior`[5] <- abs(m2.covariances[["ItemRand"]][["psi"]][1,2])  # PP-IRBItemRandS
table$`In-Role Behaviorp`[5] <- data %>% dplyr::filter(label == "ppirbc") %>% select(p) 
table$OCBI[1] <- m2.covariances[["Control"]][["psi"]][1,3]  
table$OCBIp[1] <- data %>% dplyr::filter(label == "ppocbib") %>% select(p) 
table$OCBI[2] <- m2.covariances[["CSManipulation"]][["psi"]][1,3]  
table$OCBIp[2] <- data %>% dplyr::filter(label == "ppocbia") %>% select(p) 
table$OCBI[3] <- abs(m2.covariances[["FillerScales"]][["psi"]][1,3])  
table$OCBIp[3] <- data %>% dplyr::filter(label == "ppocbid") %>% select(p) 
table$OCBI[4] <- m2.covariances[["ScaleRand"]][["psi"]][1,3]  
table$OCBIp[4] <- data %>% dplyr::filter(label == "ppocbie") %>% select(p) 
table$OCBI[5] <- abs(m2.covariances[["ItemRand"]][["psi"]][1,3]) 
table$OCBIp[5] <- data %>% dplyr::filter(label == "ppocbic") %>% select(p) 
table$OCBO[1] <- m2.covariances[["Control"]][["psi"]][1,4]  
table$OCBOp[1] <- data %>% dplyr::filter(label == "ppocbob") %>% select(p) 
table$OCBO[2] <- m2.covariances[["CSManipulation"]][["psi"]][1,4]  
table$OCBOp[2] <- data %>% dplyr::filter(label == "ppocboa") %>% select(p) 
table$OCBO[3] <- abs(m2.covariances[["FillerScales"]][["psi"]][1,4]) 
table$OCBOp[3] <- data %>% dplyr::filter(label == "ppocbod") %>% select(p) 
table$OCBO[4] <- m2.covariances[["ScaleRand"]][["psi"]][1,4]  
table$OCBOp[4] <- data %>% dplyr::filter(label == "ppocboe") %>% select(p) 
table$OCBO[5] <- abs(m2.covariances[["ItemRand"]][["psi"]][1,4])  
table$OCBOp[5] <- data %>% dplyr::filter(label == "ppocboc") %>% select(p) 

# round
table <- table %>% 
  mutate_if(is.numeric, ~round(., 2))

# Combine columns
table$`In-Role Behavior` <- paste(table$`In-Role Behavior`,table$`In-Role Behaviorp`)
table$OCBI <- paste(table$OCBI,table$OCBIp)
table$OCBO <- paste(table$OCBO,table$OCBOp)

# Name table 5
table <- table %>%
  select(-one_of("In-Role Behaviorp","OCBIp","OCBOp"))

# Call table 5.
table <- table %>% 
  mutate_if(is.numeric, ~round(., 2))
table5 <- table

# Build reliability decomposition table 
factors <- c("Proactive Personality","","","","","In-Role Behavior", "","","","","OCBI","", "","","","OCBO", "", "", "", "")
table <- as.data.frame(cbind(factors,"Condition","Total Reliability","Substantive Reliability","Mood Reliability","% Mood Reliability","Negative Item Wording Reliability","% Negative Item Wording Reliability"))
colnames(table)[1] <- "Latent Variable"
colnames(table)[2] <- "Condition"
colnames(table)[3] <- "Total Reliability"
colnames(table)[4] <- "Substantive Reliability"
colnames(table)[5] <- "Mood Reliability"
colnames(table)[6] <- "% Mood Reliability"
colnames(table)[7] <- "Negative Item Wording Reliability"
colnames(table)[8] <- "% Negative Item Wording Reliability"
table$Condition <- as.character(table$Condition)
table$`Total Reliability` <- as.numeric(table$`Total Reliability`)
table$`Substantive Reliability` <- as.numeric(table$`Substantive Reliability`)
table$`Mood Reliability` <- as.numeric(table$`Mood Reliability`)
table$`% Mood Reliability` <- as.numeric(table$`% Mood Reliability`)
table$`Negative Item Wording Reliability` <- as.numeric(table$`Negative Item Wording Reliability`)
table$`% Negative Item Wording Reliability` <- as.numeric(table$`% Negative Item Wording Reliability`)
table$Condition[1] <- "No Remedies"
table$Condition[2] <- "Cover Story"
table$Condition[3] <- "Filler Scales"
table$Condition[4] <- "Scale Randomization"
table$Condition[5] <- "Item Randomization"
table$Condition[6] <- "No Remedies"
table$Condition[7] <- "Cover Story"
table$Condition[8] <- "Filler Scales"
table$Condition[9] <- "Scale Randomization"
table$Condition[10] <- "Item Randomization"
table$Condition[11] <- "No Remedies"
table$Condition[12] <- "Cover Story"
table$Condition[13] <- "Filler Scales"
table$Condition[14] <- "Scale Randomization"
table$Condition[15] <- "Item Randomization"
table$Condition[16] <- "No Remedies"
table$Condition[17] <- "Cover Story"
table$Condition[18] <- "Filler Scales"
table$Condition[19] <- "Scale Randomization"
table$Condition[20] <- "Item Randomization"

# Extract the factor loadings
m2.factorloadings <- inspect(m2,"est")

# Focus on Control
PP <- as.data.frame(m2.factorloadings[["Control"]][["lambda"]])  %>%
  abs() %>%
  rownames_to_column %>%
  slice(1:10) %>%
  mutate(Sub.Rel = PP^2)%>%
  mutate(Mood.Rel = Mood^2)%>%
  mutate(NW.Rel = NW^2) %>%
  as.data.frame() %>%
  mutate(Error = (1-(Sub.Rel+Mood.Rel+NW.Rel)))

table$`Substantive Reliability`[1] <- (sum(PP$PP)^2)/((sum(PP$PP)^2)+(sum(PP$Mood)^2)+sum(PP$Error))
table$`Mood Reliability`[1] <- (sum(PP$Mood)^2)/((sum(PP$PP)^2)+(sum(PP$Mood)^2)+sum(PP$Error))
table$`Negative Item Wording Reliability`[1] <- (sum(PP$NW)^2)/((sum(PP$PP)^2)+(sum(PP$Mood)^2)+(sum(PP$NW)^2)+sum(PP$Error))
table$`Total Reliability`[1] <- (table$`Substantive Reliability`[1]+table$`Mood Reliability`[1]+table$`Negative Item Wording Reliability`[1])
table$`% Mood Reliability`[1] <- (table$`Mood Reliability`[1]/table$`Total Reliability`[1])
table$`% Negative Item Wording Reliability`[1] <- (table$`Negative Item Wording Reliability`[1]/table$`Total Reliability`[1])

IRB <- as.data.frame(m2.factorloadings[["Control"]][["lambda"]])  %>%
  abs() %>%
  rownames_to_column %>%
  slice(11:17) %>%
  mutate(Sub.Rel = IRB^2) %>%
  mutate(Mood.Rel = Mood^2) %>%
  mutate(NW.Rel = NW^2) %>%
  as.data.frame() %>%
  mutate(Error = (1-(Sub.Rel+Mood.Rel+NW.Rel)))

table$`Substantive Reliability`[6] <- (sum(IRB$IRB)^2)/((sum(IRB$IRB)^2)+(sum(IRB$Mood)^2)+(sum(IRB$NW)^2)+sum(IRB$Error))
table$`Mood Reliability`[6] <- (sum(IRB$Mood)^2)/((sum(IRB$IRB)^2)+(sum(IRB$Mood)^2)+(sum(IRB$NW)^2)+sum(IRB$Error))
table$`Negative Item Wording Reliability`[6] <- (sum(IRB$NW)^2)/((sum(IRB$IRB)^2)+(sum(IRB$Mood)^2)+(sum(IRB$NW)^2)+sum(IRB$Error))
table$`Total Reliability`[6] <- (table$`Substantive Reliability`[6]+table$`Mood Reliability`[6]+table$`Negative Item Wording Reliability`[6])
table$`% Mood Reliability`[6] <- (table$`Mood Reliability`[6]/table$`Total Reliability`[6])
table$`% Negative Item Wording Reliability`[6] <- (table$`Negative Item Wording Reliability`[6]/table$`Total Reliability`[6])

OCBI <- as.data.frame(m2.factorloadings[["Control"]][["lambda"]])  %>%
  abs() %>%
  rownames_to_column %>%
  slice(18:24) %>%
  mutate(Sub.Rel = OCBI^2) %>%
  mutate(Mood.Rel = Mood^2) %>%
  mutate(NW.Rel = NW^2) %>%
  as.data.frame() %>%
  mutate(Error = (1-(Sub.Rel+Mood.Rel+NW.Rel)))

table$`Substantive Reliability`[11] <- (sum(OCBI$OCBI)^2)/((sum(OCBI$OCBI)^2)+(sum(OCBI$Mood)^2)+(sum(OCBI$NW)^2)+sum(OCBI$Error))
table$`Mood Reliability`[11] <- (sum(OCBI$Mood)^2)/((sum(OCBI$OCBI)^2)+(sum(OCBI$Mood)^2)+(sum(OCBI$NW)^2)+sum(OCBI$Error))
table$`Negative Item Wording Reliability`[11] <- (sum(OCBI$NW)^2)/((sum(OCBI$OCBI)^2)+(sum(OCBI$Mood)^2)+(sum(OCBI$NW)^2)+sum(OCBI$Error))
table$`Total Reliability`[11] <- (table$`Substantive Reliability`[11]+table$`Mood Reliability`[11]+table$`Negative Item Wording Reliability`[11])
table$`% Mood Reliability`[11] <- (table$`Mood Reliability`[11]/table$`Total Reliability`[11])
table$`% Negative Item Wording Reliability`[11] <- (table$`Negative Item Wording Reliability`[11]/table$`Total Reliability`[11])

OCBO <- as.data.frame(m2.factorloadings[["Control"]][["lambda"]])  %>%
  abs() %>%
  rownames_to_column %>%
  slice(25:32) %>%
  mutate(Sub.Rel = OCBO^2) %>%
  mutate(Mood.Rel = Mood^2) %>%
  mutate(NW.Rel = NW^2) %>%
  as.data.frame() %>%
  mutate(Error = (1-(Sub.Rel+Mood.Rel+NW.Rel)))

table$`Substantive Reliability`[16] <- (sum(OCBO$OCBO)^2)/((sum(OCBO$OCBO)^2)+(sum(OCBO$Mood)^2)+(sum(OCBO$NW)^2)+sum(OCBO$Error))
table$`Mood Reliability`[16] <- (sum(OCBO$Mood)^2)/((sum(OCBO$OCBO)^2)+(sum(OCBO$Mood)^2)+(sum(OCBO$NW)^2)+sum(OCBO$Error))
table$`Negative Item Wording Reliability`[16] <- (sum(OCBO$NW)^2)/((sum(OCBO$OCBO)^2)+(sum(OCBO$Mood)^2)+(sum(OCBO$NW)^2)+sum(OCBO$Error))
table$`Total Reliability`[16] <- (table$`Substantive Reliability`[16]+table$`Mood Reliability`[16]+table$`Negative Item Wording Reliability`[16])
table$`% Mood Reliability`[16] <- (table$`Mood Reliability`[16]/table$`Total Reliability`[16])
table$`% Negative Item Wording Reliability`[16] <- (table$`Negative Item Wording Reliability`[16]/table$`Total Reliability`[16])

# Focus on the CSManpulation
PP <- as.data.frame(m2.factorloadings[["CSManipulation"]][["lambda"]])  %>%
  abs() %>%
  rownames_to_column %>%
  slice(1:10) %>%
  mutate(Sub.Rel = PP^2)%>%
  mutate(Mood.Rel = Mood^2)%>%
  mutate(NW.Rel = NW^2) %>%
  as.data.frame() %>%
  mutate(Error = (1-(Sub.Rel+Mood.Rel+NW.Rel)))

table$`Substantive Reliability`[2] <- (sum(PP$PP)^2)/((sum(PP$PP)^2)+(sum(PP$Mood)^2)+sum(PP$Error))
table$`Mood Reliability`[2] <- (sum(PP$Mood)^2)/((sum(PP$PP)^2)+(sum(PP$Mood)^2)+sum(PP$Error))
table$`Negative Item Wording Reliability`[2] <- (sum(PP$NW)^2)/((sum(PP$PP)^2)+(sum(PP$Mood)^2)+(sum(PP$NW)^2)+sum(PP$Error))
table$`Total Reliability`[2] <- (table$`Substantive Reliability`[2]+table$`Mood Reliability`[2]+table$`Negative Item Wording Reliability`[2])
table$`% Mood Reliability`[2] <- (table$`Mood Reliability`[2]/table$`Total Reliability`[2])
table$`% Negative Item Wording Reliability`[2] <- (table$`Negative Item Wording Reliability`[2]/table$`Total Reliability`[2])

IRB <- as.data.frame(m2.factorloadings[["CSManipulation"]][["lambda"]])  %>%
  abs() %>%
  rownames_to_column %>%
  slice(11:17) %>%
  mutate(Sub.Rel = IRB^2) %>%
  mutate(Mood.Rel = Mood^2) %>%
  mutate(NW.Rel = NW^2) %>%
  as.data.frame() %>%
  mutate(Error = (1-(Sub.Rel+Mood.Rel+NW.Rel)))

table$`Substantive Reliability`[7] <- (sum(IRB$IRB)^2)/((sum(IRB$IRB)^2)+(sum(IRB$Mood)^2)+(sum(IRB$NW)^2)+sum(IRB$Error))
table$`Mood Reliability`[7] <- (sum(IRB$Mood)^2)/((sum(IRB$IRB)^2)+(sum(IRB$Mood)^2)+(sum(IRB$NW)^2)+sum(IRB$Error))
table$`Negative Item Wording Reliability`[7] <- (sum(IRB$NW)^2)/((sum(IRB$IRB)^2)+(sum(IRB$Mood)^2)+(sum(IRB$NW)^2)+sum(IRB$Error))
table$`Total Reliability`[7] <- (table$`Substantive Reliability`[7]+table$`Mood Reliability`[7]+table$`Negative Item Wording Reliability`[7])
table$`% Mood Reliability`[7] <- (table$`Mood Reliability`[7]/table$`Total Reliability`[7])
table$`% Negative Item Wording Reliability`[7] <- (table$`Negative Item Wording Reliability`[7]/table$`Total Reliability`[7])

OCBI <- as.data.frame(m2.factorloadings[["CSManipulation"]][["lambda"]])  %>%
  abs() %>%
  rownames_to_column %>%
  slice(18:24) %>%
  mutate(Sub.Rel = OCBI^2) %>%
  mutate(Mood.Rel = Mood^2) %>%
  mutate(NW.Rel = NW^2) %>%
  as.data.frame() %>%
  mutate(Error = (1-(Sub.Rel+Mood.Rel+NW.Rel)))

table$`Substantive Reliability`[12] <- (sum(OCBI$OCBI)^2)/((sum(OCBI$OCBI)^2)+(sum(OCBI$Mood)^2)+(sum(OCBI$NW)^2)+sum(OCBI$Error))
table$`Mood Reliability`[12] <- (sum(OCBI$Mood)^2)/((sum(OCBI$OCBI)^2)+(sum(OCBI$Mood)^2)+(sum(OCBI$NW)^2)+sum(OCBI$Error))
table$`Negative Item Wording Reliability`[12] <- (sum(OCBI$NW)^2)/((sum(OCBI$OCBI)^2)+(sum(OCBI$Mood)^2)+(sum(OCBI$NW)^2)+sum(OCBI$Error))
table$`Total Reliability`[12] <- (table$`Substantive Reliability`[12]+table$`Mood Reliability`[12]+table$`Negative Item Wording Reliability`[12])
table$`% Mood Reliability`[12] <- (table$`Mood Reliability`[12]/table$`Total Reliability`[12])
table$`% Negative Item Wording Reliability`[12] <- (table$`Negative Item Wording Reliability`[12]/table$`Total Reliability`[12])

OCBO <- as.data.frame(m2.factorloadings[["CSManipulation"]][["lambda"]])  %>%
  abs() %>%
  rownames_to_column %>%
  slice(25:32) %>%
  mutate(Sub.Rel = OCBO^2) %>%
  mutate(Mood.Rel = Mood^2) %>%
  mutate(NW.Rel = NW^2) %>%
  as.data.frame() %>%
  mutate(Error = (1-(Sub.Rel+Mood.Rel+NW.Rel)))

table$`Substantive Reliability`[17] <- (sum(OCBO$OCBO)^2)/((sum(OCBO$OCBO)^2)+(sum(OCBO$Mood)^2)+(sum(OCBO$NW)^2)+sum(OCBO$Error))
table$`Mood Reliability`[17] <- (sum(OCBO$Mood)^2)/((sum(OCBO$OCBO)^2)+(sum(OCBO$Mood)^2)+(sum(OCBO$NW)^2)+sum(OCBO$Error))
table$`Negative Item Wording Reliability`[17] <- (sum(OCBO$NW)^2)/((sum(OCBO$OCBO)^2)+(sum(OCBO$Mood)^2)+(sum(OCBO$NW)^2)+sum(OCBO$Error))
table$`Total Reliability`[17] <- (table$`Substantive Reliability`[17]+table$`Mood Reliability`[17]+table$`Negative Item Wording Reliability`[17])
table$`% Mood Reliability`[17] <- (table$`Mood Reliability`[17]/table$`Total Reliability`[17])
table$`% Negative Item Wording Reliability`[17] <- (table$`Negative Item Wording Reliability`[17]/table$`Total Reliability`[17])

# Focus on the FillerScales
PP <- as.data.frame(m2.factorloadings[["FillerScales"]][["lambda"]])  %>%
  abs() %>%
  rownames_to_column %>%
  slice(1:10) %>%
  mutate(Sub.Rel = PP^2)%>%
  mutate(Mood.Rel = Mood^2)%>%
  mutate(NW.Rel = NW^2) %>%
  as.data.frame() %>%
  mutate(Error = (1-(Sub.Rel+Mood.Rel+NW.Rel)))

table$`Substantive Reliability`[3] <- (sum(PP$PP)^2)/((sum(PP$PP)^2)+(sum(PP$Mood)^2)+sum(PP$Error))
table$`Mood Reliability`[3] <- (sum(PP$Mood)^2)/((sum(PP$PP)^2)+(sum(PP$Mood)^2)+sum(PP$Error))
table$`Negative Item Wording Reliability`[3] <- (sum(PP$NW)^2)/((sum(PP$PP)^2)+(sum(PP$Mood)^2)+(sum(PP$NW)^2)+sum(PP$Error))
table$`Total Reliability`[3] <- (table$`Substantive Reliability`[3]+table$`Mood Reliability`[3]+table$`Negative Item Wording Reliability`[3])
table$`% Mood Reliability`[3] <- (table$`Mood Reliability`[3]/table$`Total Reliability`[3])
table$`% Negative Item Wording Reliability`[3] <- (table$`Negative Item Wording Reliability`[3]/table$`Total Reliability`[3])

IRB <- as.data.frame(m2.factorloadings[["FillerScales"]][["lambda"]])  %>%
  abs() %>%
  rownames_to_column %>%
  slice(11:17) %>%
  mutate(Sub.Rel = IRB^2) %>%
  mutate(Mood.Rel = Mood^2) %>%
  mutate(NW.Rel = NW^2) %>%
  as.data.frame() %>%
  mutate(Error = (1-(Sub.Rel+Mood.Rel+NW.Rel)))

table$`Substantive Reliability`[8] <- (sum(IRB$IRB)^2)/((sum(IRB$IRB)^2)+(sum(IRB$Mood)^2)+(sum(IRB$NW)^2)+sum(IRB$Error))
table$`Mood Reliability`[8] <- (sum(IRB$Mood)^2)/((sum(IRB$IRB)^2)+(sum(IRB$Mood)^2)+(sum(IRB$NW)^2)+sum(IRB$Error))
table$`Negative Item Wording Reliability`[8] <- (sum(IRB$NW)^2)/((sum(IRB$IRB)^2)+(sum(IRB$Mood)^2)+(sum(IRB$NW)^2)+sum(IRB$Error))
table$`Total Reliability`[8] <- (table$`Substantive Reliability`[8]+table$`Mood Reliability`[8]+table$`Negative Item Wording Reliability`[8])
table$`% Mood Reliability`[8] <- (table$`Mood Reliability`[8]/table$`Total Reliability`[8])
table$`% Negative Item Wording Reliability`[8] <- (table$`Negative Item Wording Reliability`[8]/table$`Total Reliability`[8])

OCBI <- as.data.frame(m2.factorloadings[["FillerScales"]][["lambda"]])  %>%
  abs() %>%
  rownames_to_column %>%
  slice(18:24) %>%
  mutate(Sub.Rel = OCBI^2) %>%
  mutate(Mood.Rel = Mood^2) %>%
  mutate(NW.Rel = NW^2) %>%
  as.data.frame() %>%
  mutate(Error = (1-(Sub.Rel+Mood.Rel+NW.Rel)))

table$`Substantive Reliability`[13] <- (sum(OCBI$OCBI)^2)/((sum(OCBI$OCBI)^2)+(sum(OCBI$Mood)^2)+(sum(OCBI$NW)^2)+sum(OCBI$Error))
table$`Mood Reliability`[13] <- (sum(OCBI$Mood)^2)/((sum(OCBI$OCBI)^2)+(sum(OCBI$Mood)^2)+(sum(OCBI$NW)^2)+sum(OCBI$Error))
table$`Negative Item Wording Reliability`[13] <- (sum(OCBI$NW)^2)/((sum(OCBI$OCBI)^2)+(sum(OCBI$Mood)^2)+(sum(OCBI$NW)^2)+sum(OCBI$Error))
table$`Total Reliability`[13] <- (table$`Substantive Reliability`[13]+table$`Mood Reliability`[13]+table$`Negative Item Wording Reliability`[13])
table$`% Mood Reliability`[13] <- (table$`Mood Reliability`[13]/table$`Total Reliability`[13])
table$`% Negative Item Wording Reliability`[13] <- (table$`Negative Item Wording Reliability`[13]/table$`Total Reliability`[13])

OCBO <- as.data.frame(m2.factorloadings[["FillerScales"]][["lambda"]])  %>%
  abs() %>%
  rownames_to_column %>%
  slice(25:32) %>%
  mutate(Sub.Rel = OCBO^2) %>%
  mutate(Mood.Rel = Mood^2) %>%
  mutate(NW.Rel = NW^2) %>%
  as.data.frame() %>%
  mutate(Error = (1-(Sub.Rel+Mood.Rel+NW.Rel)))

table$`Substantive Reliability`[18] <- (sum(OCBO$OCBO)^2)/((sum(OCBO$OCBO)^2)+(sum(OCBO$Mood)^2)+(sum(OCBO$NW)^2)+sum(OCBO$Error))
table$`Mood Reliability`[18] <- (sum(OCBO$Mood)^2)/((sum(OCBO$OCBO)^2)+(sum(OCBO$Mood)^2)+(sum(OCBO$NW)^2)+sum(OCBO$Error))
table$`Negative Item Wording Reliability`[18] <- (sum(OCBO$NW)^2)/((sum(OCBO$OCBO)^2)+(sum(OCBO$Mood)^2)+(sum(OCBO$NW)^2)+sum(OCBO$Error))
table$`Total Reliability`[18] <- (table$`Substantive Reliability`[18]+table$`Mood Reliability`[18]+table$`Negative Item Wording Reliability`[18])
table$`% Mood Reliability`[18] <- (table$`Mood Reliability`[18]/table$`Total Reliability`[18])
table$`% Negative Item Wording Reliability`[18] <- (table$`Negative Item Wording Reliability`[18]/table$`Total Reliability`[18])

# Focus on the FillerScales
PP <- as.data.frame(m2.factorloadings[["ScaleRand"]][["lambda"]])  %>%
  abs() %>%
  rownames_to_column %>%
  slice(1:10) %>%
  mutate(Sub.Rel = PP^2)%>%
  mutate(Mood.Rel = Mood^2)%>%
  mutate(NW.Rel = NW^2) %>%
  as.data.frame() %>%
  mutate(Error = (1-(Sub.Rel+Mood.Rel+NW.Rel)))

table$`Substantive Reliability`[4] <- (sum(PP$PP)^2)/((sum(PP$PP)^2)+(sum(PP$Mood)^2)+sum(PP$Error))
table$`Mood Reliability`[4] <- (sum(PP$Mood)^2)/((sum(PP$PP)^2)+(sum(PP$Mood)^2)+sum(PP$Error))
table$`Negative Item Wording Reliability`[4] <- (sum(PP$NW)^2)/((sum(PP$PP)^2)+(sum(PP$Mood)^2)+(sum(PP$NW)^2)+sum(PP$Error))
table$`Total Reliability`[4] <- (table$`Substantive Reliability`[4]+table$`Mood Reliability`[4]+table$`Negative Item Wording Reliability`[4])
table$`% Mood Reliability`[4] <- (table$`Mood Reliability`[4]/table$`Total Reliability`[4])
table$`% Negative Item Wording Reliability`[4] <- (table$`Negative Item Wording Reliability`[4]/table$`Total Reliability`[4])

IRB <- as.data.frame(m2.factorloadings[["ScaleRand"]][["lambda"]])  %>%
  abs() %>%
  rownames_to_column %>%
  slice(11:17) %>%
  mutate(Sub.Rel = IRB^2) %>%
  mutate(Mood.Rel = Mood^2) %>%
  mutate(NW.Rel = NW^2) %>%
  as.data.frame() %>%
  mutate(Error = (1-(Sub.Rel+Mood.Rel+NW.Rel)))

table$`Substantive Reliability`[9] <- (sum(IRB$IRB)^2)/((sum(IRB$IRB)^2)+(sum(IRB$Mood)^2)+(sum(IRB$NW)^2)+sum(IRB$Error))
table$`Mood Reliability`[9] <- (sum(IRB$Mood)^2)/((sum(IRB$IRB)^2)+(sum(IRB$Mood)^2)+(sum(IRB$NW)^2)+sum(IRB$Error))
table$`Negative Item Wording Reliability`[9] <- (sum(IRB$NW)^2)/((sum(IRB$IRB)^2)+(sum(IRB$Mood)^2)+(sum(IRB$NW)^2)+sum(IRB$Error))
table$`Total Reliability`[9] <- (table$`Substantive Reliability`[9]+table$`Mood Reliability`[9]+table$`Negative Item Wording Reliability`[9])
table$`% Mood Reliability`[9] <- (table$`Mood Reliability`[9]/table$`Total Reliability`[9])
table$`% Negative Item Wording Reliability`[9] <- (table$`Negative Item Wording Reliability`[9]/table$`Total Reliability`[9])

OCBI <- as.data.frame(m2.factorloadings[["ScaleRand"]][["lambda"]])  %>%
  abs() %>%
  rownames_to_column %>%
  slice(18:24) %>%
  mutate(Sub.Rel = OCBI^2) %>%
  mutate(Mood.Rel = Mood^2) %>%
  mutate(NW.Rel = NW^2) %>%
  as.data.frame() %>%
  mutate(Error = (1-(Sub.Rel+Mood.Rel+NW.Rel)))

table$`Substantive Reliability`[14] <- (sum(OCBI$OCBI)^2)/((sum(OCBI$OCBI)^2)+(sum(OCBI$Mood)^2)+(sum(OCBI$NW)^2)+sum(OCBI$Error))
table$`Mood Reliability`[14] <- (sum(OCBI$Mood)^2)/((sum(OCBI$OCBI)^2)+(sum(OCBI$Mood)^2)+(sum(OCBI$NW)^2)+sum(OCBI$Error))
table$`Negative Item Wording Reliability`[14] <- (sum(OCBI$NW)^2)/((sum(OCBI$OCBI)^2)+(sum(OCBI$Mood)^2)+(sum(OCBI$NW)^2)+sum(OCBI$Error))
table$`Total Reliability`[14] <- (table$`Substantive Reliability`[14]+table$`Mood Reliability`[14]+table$`Negative Item Wording Reliability`[14])
table$`% Mood Reliability`[14] <- (table$`Mood Reliability`[14]/table$`Total Reliability`[14])
table$`% Negative Item Wording Reliability`[14] <- (table$`Negative Item Wording Reliability`[14]/table$`Total Reliability`[14])

OCBO <- as.data.frame(m2.factorloadings[["ScaleRand"]][["lambda"]])  %>%
  abs() %>%
  rownames_to_column %>%
  slice(25:32) %>%
  mutate(Sub.Rel = OCBO^2) %>%
  mutate(Mood.Rel = Mood^2) %>%
  mutate(NW.Rel = NW^2) %>%
  as.data.frame() %>%
  mutate(Error = (1-(Sub.Rel+Mood.Rel+NW.Rel)))

table$`Substantive Reliability`[19] <- (sum(OCBO$OCBO)^2)/((sum(OCBO$OCBO)^2)+(sum(OCBO$Mood)^2)+(sum(OCBO$NW)^2)+sum(OCBO$Error))
table$`Mood Reliability`[19] <- (sum(OCBO$Mood)^2)/((sum(OCBO$OCBO)^2)+(sum(OCBO$Mood)^2)+(sum(OCBO$NW)^2)+sum(OCBO$Error))
table$`Negative Item Wording Reliability`[19] <- (sum(OCBO$NW)^2)/((sum(OCBO$OCBO)^2)+(sum(OCBO$Mood)^2)+(sum(OCBO$NW)^2)+sum(OCBO$Error))
table$`Total Reliability`[19] <- (table$`Substantive Reliability`[19]+table$`Mood Reliability`[19]+table$`Negative Item Wording Reliability`[19])
table$`% Mood Reliability`[19] <- (table$`Mood Reliability`[19]/table$`Total Reliability`[19])
table$`% Negative Item Wording Reliability`[19] <- (table$`Negative Item Wording Reliability`[19]/table$`Total Reliability`[19])

# Focus on the ItemRand
PP <- as.data.frame(m2.factorloadings[["ItemRand"]][["lambda"]])  %>%
  abs() %>%
  rownames_to_column %>%
  slice(1:10) %>%
  mutate(Sub.Rel = PP^2)%>%
  mutate(Mood.Rel = Mood^2)%>%
  mutate(NW.Rel = NW^2) %>%
  as.data.frame() %>%
  mutate(Error = (1-(Sub.Rel+Mood.Rel+NW.Rel)))

table$`Substantive Reliability`[5] <- (sum(PP$PP)^2)/((sum(PP$PP)^2)+(sum(PP$Mood)^2)+sum(PP$Error))
table$`Mood Reliability`[5] <- (sum(PP$Mood)^2)/((sum(PP$PP)^2)+(sum(PP$Mood)^2)+sum(PP$Error))
table$`Negative Item Wording Reliability`[5] <- (sum(PP$NW)^2)/((sum(PP$PP)^2)+(sum(PP$Mood)^2)+(sum(PP$NW)^2)+sum(PP$Error))
table$`Total Reliability`[5] <- (table$`Substantive Reliability`[5]+table$`Mood Reliability`[5]+table$`Negative Item Wording Reliability`[5])
table$`% Mood Reliability`[5] <- (table$`Mood Reliability`[5]/table$`Total Reliability`[5])
table$`% Negative Item Wording Reliability`[5] <- (table$`Negative Item Wording Reliability`[5]/table$`Total Reliability`[5])

IRB <- as.data.frame(m2.factorloadings[["ItemRand"]][["lambda"]])  %>%
  abs() %>%
  rownames_to_column %>%
  slice(11:17) %>%
  mutate(Sub.Rel = IRB^2) %>%
  mutate(Mood.Rel = Mood^2) %>%
  mutate(NW.Rel = NW^2) %>%
  as.data.frame() %>%
  mutate(Error = (1-(Sub.Rel+Mood.Rel+NW.Rel)))

table$`Substantive Reliability`[10] <- (sum(IRB$IRB)^2)/((sum(IRB$IRB)^2)+(sum(IRB$Mood)^2)+(sum(IRB$NW)^2)+sum(IRB$Error))
table$`Mood Reliability`[10] <- (sum(IRB$Mood)^2)/((sum(IRB$IRB)^2)+(sum(IRB$Mood)^2)+(sum(IRB$NW)^2)+sum(IRB$Error))
table$`Negative Item Wording Reliability`[10] <- (sum(IRB$NW)^2)/((sum(IRB$IRB)^2)+(sum(IRB$Mood)^2)+(sum(IRB$NW)^2)+sum(IRB$Error))
table$`Total Reliability`[10] <- (table$`Substantive Reliability`[10]+table$`Mood Reliability`[10]+table$`Negative Item Wording Reliability`[10])
table$`% Mood Reliability`[10] <- (table$`Mood Reliability`[10]/table$`Total Reliability`[10])
table$`% Negative Item Wording Reliability`[10] <- (table$`Negative Item Wording Reliability`[10]/table$`Total Reliability`[10])

OCBI <- as.data.frame(m2.factorloadings[["ItemRand"]][["lambda"]])  %>%
  abs() %>%
  rownames_to_column %>%
  slice(18:24) %>%
  mutate(Sub.Rel = OCBI^2) %>%
  mutate(Mood.Rel = Mood^2) %>%
  mutate(NW.Rel = NW^2) %>%
  as.data.frame() %>%
  mutate(Error = (1-(Sub.Rel+Mood.Rel+NW.Rel)))

table$`Substantive Reliability`[15] <- (sum(OCBI$OCBI)^2)/((sum(OCBI$OCBI)^2)+(sum(OCBI$Mood)^2)+(sum(OCBI$NW)^2)+sum(OCBI$Error))
table$`Mood Reliability`[15] <- (sum(OCBI$Mood)^2)/((sum(OCBI$OCBI)^2)+(sum(OCBI$Mood)^2)+(sum(OCBI$NW)^2)+sum(OCBI$Error))
table$`Negative Item Wording Reliability`[15] <- (sum(OCBI$NW)^2)/((sum(OCBI$OCBI)^2)+(sum(OCBI$Mood)^2)+(sum(OCBI$NW)^2)+sum(OCBI$Error))
table$`Total Reliability`[15] <- (table$`Substantive Reliability`[15]+table$`Mood Reliability`[15]+table$`Negative Item Wording Reliability`[15])
table$`% Mood Reliability`[15] <- (table$`Mood Reliability`[15]/table$`Total Reliability`[15])
table$`% Negative Item Wording Reliability`[15] <- (table$`Negative Item Wording Reliability`[15]/table$`Total Reliability`[15])

OCBO <- as.data.frame(m2.factorloadings[["ItemRand"]][["lambda"]])  %>%
  abs() %>%
  rownames_to_column %>%
  slice(25:32) %>%
  mutate(Sub.Rel = OCBO^2) %>%
  mutate(Mood.Rel = Mood^2) %>%
  mutate(NW.Rel = NW^2) %>%
  as.data.frame() %>%
  mutate(Error = (1-(Sub.Rel+Mood.Rel+NW.Rel)))

table$`Substantive Reliability`[20] <- (sum(OCBO$OCBO)^2)/((sum(OCBO$OCBO)^2)+(sum(OCBO$Mood)^2)+(sum(OCBO$NW)^2)+sum(OCBO$Error))
table$`Mood Reliability`[20] <- (sum(OCBO$Mood)^2)/((sum(OCBO$OCBO)^2)+(sum(OCBO$Mood)^2)+(sum(OCBO$NW)^2)+sum(OCBO$Error))
table$`Negative Item Wording Reliability`[20] <- (sum(OCBO$NW)^2)/((sum(OCBO$OCBO)^2)+(sum(OCBO$Mood)^2)+(sum(OCBO$NW)^2)+sum(OCBO$Error))
table$`Total Reliability`[20] <- (table$`Substantive Reliability`[20]+table$`Mood Reliability`[20]+table$`Negative Item Wording Reliability`[20])
table$`% Mood Reliability`[20] <- (table$`Mood Reliability`[20]/table$`Total Reliability`[20])
table$`% Negative Item Wording Reliability`[20] <- (table$`Negative Item Wording Reliability`[20]/table$`Total Reliability`[20])

# Round columsn to nearest 100th
table <- table %>% 
  mutate_if(is.numeric, ~round(., 2))

# Call table 6
table6 <- table
```
```{r Table 3, results = 'asis', tab.cap = NULL, echo = FALSE, include = FALSE}
table5 %>%
  knitr::kable(format = "latex", booktabs=T ,escape = T) %>%
  kable_styling(latex_options = c("scale_down", "hold_position"),
                 repeat_header_method = c("replace")) %>%
  footnote(general = "* p < .05; ** p < .01; *** p < .001. ") %>%
  landscape() %>%
  kable_styling(full_width = T) %>%
  save_kable("Tables/Table3.pdf")
```
```{r Table 4, results = 'asis', tab.cap = NULL, echo = FALSE, include = FALSE}
table6 %>%
  knitr::kable(format = "latex", booktabs=T ,escape = T) %>%
  kable_styling(latex_options = c("scale_down", "hold_position"),
                 repeat_header_method = c("replace")) %>%
  footnote(general = "Total reliability is a sum of the substantive and method reliability components (see Williams et al., 2010). ") %>%
  landscape() %>%
  kable_styling(full_width = T) %>%
  save_kable("Tables/Table4.pdf")
```
# ---------------
# Insert Tables 3 and 4 about here
# ---------------

Our proposed model had a close fit ($\chi^2$(`r df.ms2`) = `r csq.ms2`, *p* = `r p.ms2`, $\chi^2$/df = `r cdfratio.ms2`, CFI = `r cfi.ms2`, RMSEA = `r remsea.ms2`, SRMR = `r srmr.ms2`) and so we moved to interpreting the results. Table 3 contains the estimates for the latent construct correlations. A consistent trend emerged regarding filler scales and randomizing items. Compared to the control condition, the correlations observed in these remedied conditions were consistently lower. Indeed, the correlations in the filler scale condition were statistically non-significant. The pattern of effects linked to the other remedies was less clear, suggesting mixed support for hypothesis 1.

Table 4 contains the reliability decomposition for our key study variables. Another clear trend emerged. Specifically, the substantive reliability estimates were all smaller in the remedied conditions compared to the non-remedied condition. This indicates that proximal causes of method variance affected the estimate of substantive variance in the control condition and that using remedies reduces such method variance, supporting hypothesis 2. We also can see that the proportion of reliable variance attributable to mood is generally larger in the remedied condition compared to the non-remedied condition. 

# Discussion Study 2

Study 2 provided further evidence for the efficacy of specific proximal remedies for common method variance. We found that in a condition where filler scales were used or scale-order was randomized, the estimated construct correlations were all weakened. [^7] Crucially, in the filler scale condition these reductions were dramatic and rendered these correlations statistically non-significant, suggesting these substantive linkages (as assessed by same-source designs) can be explained by serial scale-order context and mood effects. [^8] In other words, our data suggest that scale-order effects explain the correlation linking self-descriptions of proactive personality and workplace behavior (i.e., in-role behavior, OCBI, OCBO) for designs relying on single-sources. 

[^7]: While this may suggest that the filler scale and scale randomization remedies each have a unique influence, it should be noted that randomizing the presentation of scales likely has similar effects as using filler scales (e.g., OCBI can be presented first and proactive personality can be presented after a series of other measures). What this suggests is that increasing the separation among whole scales (via filler scales or randomizing the presentation of scales) can reduce the estimated inter-construct correlation among said scales.

[^8]: Though not reported in the tables, there was substantially more error involved in estimating these latent construct correlations, hence why they are not statistically significant.

Data from study 2 also mirrored the same unexpected trend regarding mood (see table 6). As the amount of the substantive variance captured was lower in the remedied condition, we also generally observed more variance attributable to momentary mood. This further supports the notion that failing to design for sources of method variance can mean that one underestimates the role played by methodological factors such as mood. 

## Study 3 - Efficacy of Introducing a 1-Week Separation Between Predictor and Outcome Measures

As observed in study 1 and study 2, mood played a contaminating role in our observations. This is to be expected as data were gathered using a single-source at a single point in time. With study 3, we examined how the contaminating role played by momentary mood might be reduced. Specifically, we examined the efficacy of introducing a one-week temporal separation of measurement between the administration of our predictor and outcome measures. Conventionally this involves gathering predictor data first followed by criterion data (i.e., a lagged scale-order effect might be at play). It should be kept in mind that the only remedy applied in study 3 is a one-week temporal separation. In other words, other method effects examined in study 1 and 2 (i.e., item context effects) likely play a role in this data, which has implications for hypothesis 2. What this means is that the substantive variance estimates, which are contaminated by measurement context, should be relatively equivalent across both conditions. However, in the non-remedied condition the total reliability (i.e., substantive variance + method variance) captured by the overall measurement model will be larger because it contains additional method variance attributable to mood, which has not been remedied. Overall, the latent construct correlations in the remedied condition should generally be lower (which would support hypothesis 1) and the amount of mood-related variance contaminating the measurement model should also be lower. This latter claim we articulate with hypothesis 3:

*Hypothesis 3: Inserting a 1-week temporal separation between the administration of predictor and outcome measures will reduce the contaminating role of mood in substantive measures of interest.*

# Method - Study 3

## Sample, procedure, and analytical approach
```{r Load Study 3 Data, include=FALSE}
data3 <- read_sav("Data/Study 3.sav")

#Address the misnamed OCB variables. Fix OCBO ordering to align with other datasets.
colnames(data3)[colnames(data3)=="OCBI7"] <- "OCBO1new"
colnames(data3)[colnames(data3)=="OCBO7"] <- "OCBI7new"
colnames(data3)[colnames(data3)=="OCBO6"] <- "OCBO7"
colnames(data3)[colnames(data3)=="OCBO5"] <- "OCBO6"
colnames(data3)[colnames(data3)=="OCBO4"] <- "OCBO5"
colnames(data3)[colnames(data3)=="OCBO3"] <- "OCBO4"
colnames(data3)[colnames(data3)=="OCBO2"] <- "OCBO3"
colnames(data3)[colnames(data3)=="OCBO1"] <- "OCBO2"
colnames(data3)[colnames(data3)=="OCBO1new"] <- "OCBO1"
colnames(data3)[colnames(data3)=="OCBI7new"] <- "OCBI7"

#Re-order variables.
data3 <- data3[c(2,3,87,88,89,4:13,30:35,43,37:42,36,44:50,60:79,86)]

#Analyses revealed that some cases of data had been imputed. Missing data appears to have been dealt with by simply imputing an average of sorts. This could be problematic (see Enders, C. K. (2003). Using the expectation maximization algorithm to estimate coefficient alpha for scales with item-level missing data. Psychological Methods, 8(3), 322-337.; Enders, C. K. (2010). Applied missing data analysis: New York, NY: The Guilford Press. Additionally, and more importantly, the required estimation methods are not maximum likelihood but are ordinal, further requiring the each response category be represented (i.e., averages do not fall into a particular response category).  Reversing this decision:
data3$PA2[data3$PA2==3.23] <- NA
data3$PA5[data3$PA5==3.58] <- NA
data3$PA6[data3$PA6==3.59] <- NA
data3$PA8[data3$PA8==3.77] <- NA
data3$PA9[data3$PA9==3.75] <- NA
data3$PA10[data3$PA10==3.57] <- NA
data3$PA3[data3$PA3==3.44] <- NA
data3$PA4[data3$PA4==3.56] <- NA
data3$PA7[data3$PA7==3.45] <- NA
data3$NA9[data3$NA9==1.87] <- NA
data3$NA2[data3$NA2==3.23] <- NA
data3$NA3[data3$NA3==1.72] <- NA
data3$NA6[data3$NA6==2.17] <- NA
data3$NA7[data3$NA7==1.57] <- NA
data3$NA8[data3$NA8==2.11] <- NA
data3$PP1[data3$PP1==3.49] <- NA
data3$PP3[data3$PP3==3.92] <- NA
data3$PP3[data3$PP3==0] <- NA
data3$PP8[data3$PP8==3.57] <- NA
data3$PP9[data3$PP9==3.53] <- NA
data3$PP10[data3$PP10==3.45] <- NA
data3$IRB1[data3$IRB1==4.47] <- NA
data3$IRB2[data3$IRB2==4.47] <- NA
data3$IRB4[data3$IRB4==4.49] <- NA
data3$IRB5[data3$IRB5==3.59] <- NA
data3$IRB5[data3$IRB5==3.96] <- NA
data3$IRB6[data3$IRB6==1.88] <- NA
data3$IRB7[data3$IRB7==1.65] <- NA
data3$IRB7[data3$IRB7==0] <- NA
data3$OCBI3[data3$OCBI3==3.7] <- NA
data3$OCBI3[data3$OCBI3==0] <- NA
data3$OCBI5[data3$OCBI5==3.99] <- NA
data3$OCBI7[data3$OCBI7==4.06] <- NA
data3$OCBO2[data3$OCBO2==4.24] <- NA
data3$OCBO4[data3$OCBO4==1.92] <- NA
data3$OCBO4[data3$OCBO4==0] <- NA
data3$OCBO5[data3$OCBO5==2.12] <- NA
data3$OCBO5[data3$OCBO5==0] <- NA
data3$OCBO6[data3$OCBO6==2.12] <- NA
data3$OCBO7[data3$OCBO7==3.89] <- NA

#Examine missingness pattern.
#MDAplot <-  aggr(data3, col=mdc(1:2), numbers=TRUE, sortVars=TRUE, labels=names(data3), cex.axis=.7, gap=3, ylab=c("Proportion of missingness","Missingness Pattern"))
#Delete cases who did not complete the didn't complete the demographic questionnaire.
data3 <- data3[-c(38,94,141,153,161,165,170,183,188,190,193,195,196,231,300),]
#MDAplot <-  aggr(data3, col=mdc(1:2), numbers=TRUE, sortVars=TRUE, labels=names(data3), cex.axis=.7, gap=3, ylab=c("Proportion of missingness","Missingness Pattern"))
#MDA <- LittleMCAR(data3[c(6:31,33:56)]) #Only 50 items can be analyzed, so one item with full data was dropped from the analysis. 

##Thirty six missing data patterns and a just significant LittleMCAR test suggest that data may not be MCAR. Though imputation is used to fill in gaps, this should be taken as a limitation.

#Count of individuals who agree to participate.
N1 <- as.numeric(freq_vect(data3$COND)[1,"Count"])  #Cross-sectional/Control
N2 <- as.numeric(freq_vect(data3$COND)[2,"Count"])  #Temporal Separation of Measurement

#Missing data analysis using 'sapply(data3, function(x) sum(is.na(x)))' which likert data were missing. 
init <- mice(data3, maxit = 0)
meth <- init$method
predM <- init$predictorMatrix
meth[c("PP1")]="norm"
meth[c("PP3")]="norm"
meth[c("PP8")]="norm"
meth[c("PP9")]="norm"
meth[c("PP10")]="norm"
meth[c("OCBI3")]="norm"
meth[c("OCBI5")]="norm"
meth[c("OCBO1")]="norm"
meth[c("OCBO3")]="norm"
meth[c("OCBO4")]="norm"
meth[c("OCBO5")]="norm"
meth[c("OCBO6")]="norm"
meth[c("OCBO7")]="norm"
meth[c("IRB2")]="norm"
meth[c("IRB4")]="norm"
meth[c("IRB5")]="norm"
meth[c("IRB6")]="norm"
meth[c("IRB7")]="norm"
meth[c("PA1")]="norm"
meth[c("PA2")]="norm"
meth[c("PA3")]="norm"
meth[c("PA4")]="norm"
meth[c("PA5")]="norm"
meth[c("PA6")]="norm"
meth[c("PA7")]="norm"
meth[c("PA8")]="norm"
meth[c("PA9")]="norm"
meth[c("PA10")]="norm"
meth[c("NA1")]="norm"
meth[c("NA2")]="norm"
meth[c("NA3")]="norm"
meth[c("NA4")]="norm"
meth[c("NA5")]="norm"
meth[c("NA6")]="norm"
meth[c("NA7")]="norm"
meth[c("NA8")]="norm"
meth[c("NA9")]="norm"
meth[c("NA10")]="norm"
imputed <- mice(data3, method=meth, predictorMatrix=predM, m=5)
data3 <- complete(imputed)
#Round imputed data to nearest whole number for estimation purposes. 
data3$PP1 <- ceiling(data3$PP1)
data3$PP3 <- ceiling(data3$PP3)
data3$PP8 <- ceiling(data3$PP8)
data3$PP9 <- ceiling(data3$PP9)
data3$PP10 <- ceiling(data3$PP10)
data3$OCBI3 <- ceiling(data3$OCBI3)
data3$OCBI5 <- ceiling(data3$OCBI5)
data3$OCBO1 <- ceiling(data3$OCBO1)
data3$OCBO3 <- ceiling(data3$OCBO3)
data3$OCBO4 <- ceiling(data3$OCBO4)
data3$OCBO5 <- ceiling(data3$OCBO5)
data3$OCBO6 <- ceiling(data3$OCBO6)
data3$OCBO7 <- ceiling(data3$OCBO7)
data3$IRB2 <- ceiling(data3$IRB2)
data3$IRB4 <- ceiling(data3$IRB4)
data3$IRB5 <- ceiling(data3$IRB5)
data3$IRB6 <- ceiling(data3$IRB6)
data3$IRB7 <- ceiling(data3$IRB7)
data3$PA1 <- ceiling(data3$PA1)
data3$PA2 <- ceiling(data3$PA2)
data3$PA3 <- ceiling(data3$PA3)
data3$PA4 <- ceiling(data3$PA4)
data3$PA5 <- ceiling(data3$PA5)
data3$PA6 <- ceiling(data3$PA6)
data3$PA7 <- ceiling(data3$PA7)
data3$PA8 <- ceiling(data3$PA8)
data3$PA9 <- ceiling(data3$PA9)
data3$PA10 <- ceiling(data3$PA10)
data3$NA1 <- ceiling(data3$NA1)
data3$NA2 <- ceiling(data3$NA2)
data3$NA3 <- ceiling(data3$NA3)
data3$NA4 <- ceiling(data3$NA4)
data3$NA5 <- ceiling(data3$NA5)
data3$NA6 <- ceiling(data3$NA6)
data3$NA7 <- ceiling(data3$NA7)
data3$NA8 <- ceiling(data3$NA8)
data3$NA9 <- ceiling(data3$NA9)
data3$NA10 <- ceiling(data3$NA10)

#Setup demographics.
#Rename variables.
colnames(data3)[colnames(data3)=="SEX"] <- "GENDER"

#Calculate descriptives
M <- mean(data3$AGE, na.rm = TRUE)
SD <- sd(data3$AGE, na.rm = TRUE)

#Calculate frequencies
##Recode single "f" to female. 
data3$GENDER[data3$GENDER=="f"] <- 1
female.n <- as.numeric(freq_vect(data3$GENDER)[2,"Count"])
female.p <- as.numeric(freq_vect(data3$GENDER)[2,"Percentage"])
white.n <- as.numeric(freq_vect(data3$RACE)[2,"Count"])
white.p <- as.numeric(freq_vect(data3$RACE)[2,"Percentage"])
fulltime.n <- as.numeric(freq_vect(data3$PartFullTime)[2,"Count"])
fulltime.p <- as.numeric(freq_vect(data3$PartFullTime)[2,"Percentage"])
```
```{r Study 3: Cronbach Alphas, include = FALSE}
#Cronbach Alphas
my.scales <- scoreItems(my.keys.list,data3)
PP.alpha <- round(my.scales[["alpha"]][1],2)
IRB.alpha <- round(my.scales[["alpha"]][2],2)
OCBI.alpha <- round(my.scales[["alpha"]][3],2)
OCBO.alpha <- round(my.scales[["alpha"]][4],2)
PA.alpha <- round(my.scales[["alpha"]][5],2)
```
```{r Study 3: Build for Descriptive Statistics, include=FALSE}
#Create Descriptives Table
scales <- my.scales[["scores"]]
scales <- cbind(data3[c(1,3,4)],scales,data3[c("MOOD_T1")])
table7 <- apa.cor.table(scales, table.number = 1, landscape = FALSE)
```
```{r Study 3: Table 7: Descriptives, results="asis"}
#table7
```
```{r Study 3: Construct Validity for Affect, include = FALSE}
mood <- cfa(mood.cfa, data = data3, estimator = "DWLS", parameterization = "theta", information = "expected",  std.lv=TRUE, test = "Satorra-Bentler")
summary(mood, standardized = TRUE, fit.measures =TRUE)
affect <- as.data.frame(predict(mood))

#Bind to data
data3 <- cbind(data3,affect[c(1)])
```

Participants were recruited using a Survey Monkey panel and randomly assigned to a condition where they received all measures at the same time (i.e., control) or a condition whereby a temporal separation of one week was used to divide the administration of predictor and criterion measures. All respondents received $5 to complete this survey. Fifteen respondents did not answer demographic questions and were deleted. For the temporal remedy condition (n = `r apa(N2,0,T)`), after one week, the in-role behavior and OCB measures were administered along with a measure of positive and negative affectivity and the demographics questionnaire. Additionally, anticipating a significant amount of attrition in temporal separation condition, we requested that individuals twice as many in the longitudinal condition to end up with roughly equal sample sizes. For the cross-sectional data (n = `r apa(N1,0,T)`), respondents completed all measures at the same point in time. Notwithstanding missing demographic data, the sample was female biased (n = `r apa(female.n,0,T)`), predominantly Caucasian, and  the average age was *M* = `r apa(M,2,T)` (*SD* = `r apa(SD,2,T)`). The majority (n = `r apa(fulltime.n,0,T)`) of respondents worked full-time. The same measures and analytical approach used in study 1 and 2 were used in study 3. Cronbach alphas ranged from `r apa(OCBO.alpha)` for OCBO to `r apa(PA.alpha)` for positive affectivity.

# Results - Study 3
```{r Study 3: Model Testing, include = FALSE}
m3.cfa <- ' 
# Substantive factors
PP =~ c(pp1a, pp1b)*PP1 + c(pp2a, pp2b)*PP2 + c(pp3a,pp3b)*PP3 + c(pp4a,pp4b)*PP4 + c(pp5a,pp5b)*PP5 + c(pp6a,pp6b)*PP6 + c(pp7a,pp7b)*PP7 + c(pp8a,pp8b)*PP8 + c(pp9a,pp9b)*PP9 + c(pp10a,pp10b)*PP10
IRB =~ c(irb1a,irb1b)*IRB1 + c(irb2a,irb2b)*IRB2 + c(irb3a,irb3b)*IRB3 + c(irb4a,irb4b)*IRB4 + c(irb5a,irb5b)*IRB5 + c(irb6a,irb6b)*IRB6 + c(irb7a,irb7b)*IRB7
OCBI =~ c(ocbi1a,ocbi1b)*OCBI1 + c(ocbi2a,ocbi2b)*OCBI2 + c(ocbi3a,ocbi3b)*OCBI3 + c(ocbi4a,ocbi4b)*OCBI4 + c(ocbi5a,ocbi5b)*OCBI5 + c(ocbi6a,ocbi6b)*OCBI6 + c(ocbi7a,ocbi7b)*OCBI7
OCBO =~ c(ocbo1a,ocbo1b)*OCBO1 + c(ocbo2a,ocbo2b)*OCBO2 + c(ocbo3a,ocbo3b)*OCBO3 + c(ocbo4a,ocbo4b)*OCBO4 + c(ocbo5a,ocbo5b)*OCBO5 + c(ocbo6a,ocbo6b)*OCBO6 + c(ocbo7a,ocbo7b)*OCBO7

# Negative Item Content Factor
NW =~ c(nw1a, nw1b)*IRB6 + c(nw2a, nw2b)*IRB7 + c(nw3a, nw3b)*OCBO3 + c(nw4a, nw4b)*OCBO4 + c(nw5a, nw5b)*OCBO5
NW ~~ c(1,1)*NW
NW ~ c(0,0)*1
NW ~~ c(0,0)*IRB
NW ~~ c(0,0)*OCBO
NW ~~ c(0,0)*PP
NW ~~ c(0,0)*OCBI

#Factor variances are fixed to 1 to allow estimation.
###Substantive factors
PP ~~ c(1,1)*PP
IRB ~~ c(1,1)*IRB
OCBI ~~ c(1,1)*OCBI
OCBO ~~ c(1,1)*OCBO

##Factor covariances labels 
PP ~~ c(PPIRB1,PPIRB2)*IRB
PP ~~ c(PPOCBI1,PPOCBI2)*OCBI
PP ~~ c(PPOCBO1,PPOCBO2)*OCBO
IRB ~~ c(IRBOCBI1,IRBOCBI2)*OCBI
IRB ~~ c(IRBOCBO1,IRBOCBO2)*OCBO
OCBI ~~ c(OCBIO1,OCBIO2)*OCBO

#Factor means of both groups are fixed at zero to allow identification.
##Substantive factors
PP ~ c(0,0)*1
IRB ~ c(0,0)*1
OCBI ~ c(0,0)*1
OCBO ~ c(0,0)*1

# Add in "Mood" as a single-item latent construct that is an uncorrelated with all factors.
Mood =~ .95*MOOD_T1 + c(mpp1a, mpp1b)*PP1 + c(mpp2a, mpp2b)*PP2 + c(mpp3a,mpp3b)*PP3 + c(mpp4a,mpp4b)*PP4 + c(mpp5a,mpp5b)*PP5 + c(mpp6a,mpp6b)*PP6 + c(mpp7a,mpp7b)*PP7 + c(mpp8a,mpp8b)*PP8 + c(mpp9a,mpp9b)*PP9 + c(mpp10a,mpp10b)*PP10 + c(mirb1a,mirb1b)*IRB1 + c(mirb2a,mirb2b)*IRB2 + c(mirb3a,mirb3b)*IRB3 + c(mirb4a,mirb4b)*IRB4 + c(mirb5a,mirb5b)*IRB5 + c(mirb6a,mirb6b)*IRB6 + c(mirb7a,mirb7b)*IRB7 + c(mocbi1a,mocbi1b)*OCBI1 + c(mocbi2a,mocbi2b)*OCBI2 + c(mocbi3a,mocbi3b)*OCBI3 + c(mocbi4a,mocbi4b)*OCBI4 + c(mocbi5a,mocbi5b)*OCBI5 + c(mocbi6a,mocbi6b)*OCBI6 + c(mocbi7a,mocbi7b)*OCBI7 + c(mocbo1a,mocbo1b)*OCBO1 + c(mocbo2a,mocbo2b)*OCBO2 + c(mocbo3a,mocbo3b)*OCBO3 + c(mocbo4a,mocbo4b)*OCBO4 + c(mocbo5a,mocbo5b)*OCBO5 + c(mocbo6a,mocbo6b)*OCBO6 + c(mocbo7a,mocbo7b)*OCBO7
 # (see Anderson & Gerbing, 1988; Sorbom & Joreskig, 1982; as cited by Petrescu, 2013)
MOOD_T1 ~~ (1.837648)*(1-.85)*MOOD_T1

# Add in PA as a single-item latent construct that is an uncorrelated with all factors.
PosAff =~ .95*PA
PA ~~ (0.9279188)*(1-.92)*PA

# Make uncorrelated with NW
NW ~~ 0*PosAff

# Add in causal sructure for PA and Mood
Mood ~ PosAff # PA causes mood

# Mood as a proximal cause of method variance
Mood ~~ 0*PP # Mood uncorrelated with proactive personality
Mood ~~ 0*IRB # Mood uncorrelated with IRB
Mood ~~ 0*OCBI # Mood uncorreated with OCBI
Mood ~~ 0*OCBO # Mood uncorrelated with OCBO
NW ~~ 0*Mood # Mood uncorrelated with NW
'

m3 <- cfa(m3.cfa, data = data3, group = "COND", estimator = "DWLS", parameterization = "theta", information = "expected",  std.lv=TRUE,test = "Satorra-Bentler")
summary(m3, standardized = TRUE, fit.measures = TRUE)
mod.m <- modindices(m3, minimum.value = 10, sort = TRUE)
fitm3.m <- fitmeasures(m3)
mod.m <- modindices(m3, minimum.value = 10, sort = TRUE)
fitm3.m <- fitmeasures(m3)

#Gather stats
csq.ms3 <- round(as.numeric(fitm3.m[c(6)]), digits = 2) ##Chi-Square
df.ms3 <- round(as.numeric(fitm3.m[c(7)]), digits = 2) ##df
cdfratio.ms3 <- round(csq.ms3/df.ms3,2) ## Chi-square / df ratio
p.ms3 <- pvalr(as.numeric(fitm3.m[c(8)]), digits = 2) ##Chi-Square p value
cssf.ms3 <- round(as.numeric(fitm3.m[c(9)]), digits = 2) ##Chisq.scaling factor
cfi.ms3 <- round(as.numeric(fitm3.m[c(27)]), digits = 2) #CFI robust
remsea.ms3 <- round(as.numeric(fitm3.m[c(44)]), digits = 3) ##RMSEA robust
remseal.ms3 <- round(as.numeric(fitm3.m[c(45)]), digits = 3) #RMSEA lower robust
remseau.ms3 <- round(as.numeric(fitm3.m[c(46)]), digits = 3) #RMSEA upper robust
srmr.ms3<- round(as.numeric(fitm3.m[c(50)]), digits = 3) #SRMR
```
```{r Study 3: Build Latent Correlation and Reliability Decomposition Tables, include = FALSE}
# Build table for latent covariances
factors <- c("Proactive Personality","")
table <- as.data.frame(cbind(factors,"Condition","In-Role Behavior","","OCBI","","OCBO",""))
colnames(table)[1] <- "Predictor"
colnames(table)[2] <- "Condition"
colnames(table)[3] <- "In-Role Behavior"
colnames(table)[4] <- "In-Role Behaviorp"
colnames(table)[5] <- "OCBI"
colnames(table)[6] <- "OCBIp"
colnames(table)[7] <- "OCBO"
colnames(table)[8] <- "OCBOp"
table$Condition <- as.character(table$Condition)
table$`In-Role Behavior` <- as.numeric(table$`In-Role Behavior`)
table$`In-Role Behaviorp` <- as.character(table$`In-Role Behavior`)
table$`OCBI` <- as.numeric(table$`OCBI`)
table$OCBIp <- as.character(table$OCBIp)
table$`OCBO` <- as.numeric(table$`OCBO`)
table$OCBOp <- as.character(table$OCBOp)
table$Condition[1] <- "No Remedies"
table$Condition[2] <- "One-Week Separation"

# Extract pvalues and significance levels for mapping purposes.
data <- parameterEstimates(m3, standardized=TRUE)%>% 
  dplyr::filter(op == "~~") %>%
  dplyr::filter(label == "PPIRB1" | label == "PPOCBI1" | label == "PPOCBO1" | label == "PPIRB2" | label == "PPOCBI2" | label == "PPOCBO2") %>%
  select(label, est, pvalue) %>%
  mutate_if(is.numeric, ~round(., 2)) 
data$p <- stars.pval(data$pvalue)

# Extract the latent covariances loadings, map on significance levels, and place into table.
m3.covariances<- inspect(m3,"std.all")
table$`In-Role Behavior`[1] <- m3.covariances[["0"]][["psi"]][1,2]  # PP-IRBnr
table$`In-Role Behaviorp`[1] <- data %>% dplyr::filter(label == "PPIRB1") %>% select(p) 
table$`In-Role Behavior`[2] <- m3.covariances[["1"]][["psi"]][1,2]  # PP-IRBr
table$`In-Role Behaviorp`[2] <- data %>% dplyr::filter(label == "PPIRB2") %>% select(p) 
table$OCBI[1] <- m3.covariances[["0"]][["psi"]][1,3]  # PP-OCBInr
table$OCBIp[1] <- data %>% dplyr::filter(label == "PPOCBI1") %>% select(p) 
table$OCBI[2] <- m3.covariances[["1"]][["psi"]][1,3]  # PP-OCBIr
table$OCBIp[2] <- data %>% dplyr::filter(label == "PPOCBI2") %>% select(p) 
table$OCBO[1] <- m3.covariances[["0"]][["psi"]][1,4]  # PP-OCBOnr
table$OCBOp[1] <- data %>% dplyr::filter(label == "PPOCBO1") %>% select(p) 
table$OCBO[2] <- m3.covariances[["1"]][["psi"]][1,4]  # PP-OCBOr
table$OCBOp[2] <- data %>% dplyr::filter(label == "PPOCBO2") %>% select(p) 

# round
table <- table %>% 
  mutate_if(is.numeric, ~round(., 2))

# Combine columns
table$`In-Role Behavior` <- paste(table$`In-Role Behavior`,table$`In-Role Behaviorp`)
table$OCBI <- paste(table$OCBI,table$OCBIp)
table$OCBO <- paste(table$OCBO,table$OCBOp)

# Name table 8
table8 <- table %>%
  select(-one_of("In-Role Behaviorp","OCBIp","OCBOp"))

# Build reliability decomposition table 
factors <- c("Proactive Personality","","In-Role Behavior", "","OCBI","", "OCBO", "")
table <- as.data.frame(cbind(factors,"Condition","Total Reliability","Substantive Reliability","Mood Reliability","% Mood Reliability","Negative Item Wording Reliability","% Negative Item Wording Reliability"))
colnames(table)[1] <- "Latent Variable"
colnames(table)[2] <- "Condition"
colnames(table)[3] <- "Total Reliability"
colnames(table)[4] <- "Substantive Reliability"
colnames(table)[5] <- "Mood Reliability"
colnames(table)[6] <- "% Mood Reliability"
colnames(table)[7] <- "Negative Item Wording Reliability"
colnames(table)[8] <- "% Negative Item Wording Reliability"
table$Condition <- as.character(table$Condition)
table$`Total Reliability` <- as.numeric(table$`Total Reliability`)
table$`Substantive Reliability` <- as.numeric(table$`Substantive Reliability`)
table$`Mood Reliability` <- as.numeric(table$`Mood Reliability`)
table$`% Mood Reliability` <- as.numeric(table$`% Mood Reliability`)
table$`Negative Item Wording Reliability` <- as.numeric(table$`Negative Item Wording Reliability`)
table$`% Negative Item Wording Reliability` <- as.numeric(table$`% Negative Item Wording Reliability`)
table$Condition[1] <- "No Remedies"
table$Condition[2] <- "One-Week Separation"
table$Condition[3] <- "No Remedies"
table$Condition[4] <- "One-Week Separation"
table$Condition[5] <- "No Remedies"
table$Condition[6] <- "One-Week Separation"
table$Condition[7] <- "No Remedies"
table$Condition[8] <- "One-Week Separation"

# Extract the factor loadings
m3.factorloadings <- inspect(m3,"est")

# Focus on the remedied group
PP <- as.data.frame(m3.factorloadings[["1"]][["lambda"]])  %>%
  abs() %>%
  rownames_to_column %>%
  slice(1:10) %>%
  mutate(Sub.Rel = PP^2)%>%
  mutate(Mood.Rel = Mood^2)%>%
  mutate(NW.Rel = NW^2) %>%
  as.data.frame() %>%
  mutate(Error = (1-(Sub.Rel+Mood.Rel+NW.Rel)))

table$`Substantive Reliability`[2] <- (sum(PP$PP)^2)/((sum(PP$PP)^2)+(sum(PP$Mood)^2)+sum(PP$Error))
table$`Mood Reliability`[2] <- (sum(PP$Mood)^2)/((sum(PP$PP)^2)+(sum(PP$Mood)^2)+sum(PP$Error))
table$`Negative Item Wording Reliability`[2] <- (sum(PP$NW)^2)/((sum(PP$PP)^2)+(sum(PP$Mood)^2)+(sum(PP$NW)^2)+sum(PP$Error))
table$`Total Reliability`[2] <- (table$`Substantive Reliability`[2]+table$`Mood Reliability`[2]+table$`Negative Item Wording Reliability`[2])
table$`% Mood Reliability`[2] <- (table$`Mood Reliability`[2]/table$`Total Reliability`[2])
table$`% Negative Item Wording Reliability`[2] <- (table$`Negative Item Wording Reliability`[2]/table$`Total Reliability`[2])

IRB <- as.data.frame(m3.factorloadings[["1"]][["lambda"]])  %>%
  abs() %>%
  rownames_to_column %>%
  slice(11:17) %>%
  mutate(Sub.Rel = IRB^2) %>%
  mutate(Mood.Rel = Mood^2) %>%
  mutate(NW.Rel = NW^2) %>%
  as.data.frame() %>%
  mutate(Error = (1-(Sub.Rel+Mood.Rel+NW.Rel)))

table$`Substantive Reliability`[4] <- (sum(IRB$IRB)^2)/((sum(IRB$IRB)^2)+(sum(IRB$Mood)^2)+(sum(IRB$NW)^2)+sum(IRB$Error))
table$`Mood Reliability`[4] <- (sum(IRB$Mood)^2)/((sum(IRB$IRB)^2)+(sum(IRB$Mood)^2)+(sum(IRB$NW)^2)+sum(IRB$Error))
table$`Negative Item Wording Reliability`[4] <- (sum(IRB$NW)^2)/((sum(IRB$IRB)^2)+(sum(IRB$Mood)^2)+(sum(IRB$NW)^2)+sum(IRB$Error))
table$`Total Reliability`[4] <- (table$`Substantive Reliability`[4]+table$`Mood Reliability`[4]+table$`Negative Item Wording Reliability`[4])
table$`% Mood Reliability`[4] <- (table$`Mood Reliability`[4]/table$`Total Reliability`[4])
table$`% Negative Item Wording Reliability`[4] <- (table$`Negative Item Wording Reliability`[4]/table$`Total Reliability`[4])

OCBI <- as.data.frame(m3.factorloadings[["1"]][["lambda"]])  %>%
  abs() %>%
  rownames_to_column %>%
  slice(18:24) %>%
  mutate(Sub.Rel = OCBI^2) %>%
  mutate(Mood.Rel = Mood^2) %>%
  mutate(NW.Rel = NW^2) %>%
  as.data.frame() %>%
  mutate(Error = (1-(Sub.Rel+Mood.Rel+NW.Rel)))

table$`Substantive Reliability`[6] <- (sum(OCBI$OCBI)^2)/((sum(OCBI$OCBI)^2)+(sum(OCBI$Mood)^2)+(sum(OCBI$NW)^2)+sum(OCBI$Error))
table$`Mood Reliability`[6] <- (sum(OCBI$Mood)^2)/((sum(OCBI$OCBI)^2)+(sum(OCBI$Mood)^2)+(sum(OCBI$NW)^2)+sum(OCBI$Error))
table$`Negative Item Wording Reliability`[6] <- (sum(OCBI$NW)^2)/((sum(OCBI$OCBI)^2)+(sum(OCBI$Mood)^2)+(sum(OCBI$NW)^2)+sum(OCBI$Error))
table$`Total Reliability`[6] <- (table$`Substantive Reliability`[6]+table$`Mood Reliability`[6]+table$`Negative Item Wording Reliability`[6])
table$`% Mood Reliability`[6] <- (table$`Mood Reliability`[6]/table$`Total Reliability`[6])
table$`% Negative Item Wording Reliability`[6] <- (table$`Negative Item Wording Reliability`[6]/table$`Total Reliability`[6])

OCBO <- as.data.frame(m3.factorloadings[["1"]][["lambda"]])  %>%
  abs() %>%
  rownames_to_column %>%
  slice(25:32) %>%
  mutate(Sub.Rel = OCBO^2) %>%
  mutate(Mood.Rel = Mood^2) %>%
  mutate(NW.Rel = NW^2) %>%
  as.data.frame() %>%
  mutate(Error = (1-(Sub.Rel+Mood.Rel+NW.Rel)))

table$`Substantive Reliability`[8] <- (sum(OCBO$OCBO)^2)/((sum(OCBO$OCBO)^2)+(sum(OCBO$Mood)^2)+(sum(OCBO$NW)^2)+sum(OCBO$Error))
table$`Mood Reliability`[8] <- (sum(OCBO$Mood)^2)/((sum(OCBO$OCBO)^2)+(sum(OCBO$Mood)^2)+(sum(OCBO$NW)^2)+sum(OCBO$Error))
table$`Negative Item Wording Reliability`[8] <- (sum(OCBO$NW)^2)/((sum(OCBO$OCBO)^2)+(sum(OCBO$Mood)^2)+(sum(OCBO$NW)^2)+sum(OCBO$Error))
table$`Total Reliability`[8] <- (table$`Substantive Reliability`[8]+table$`Mood Reliability`[8]+table$`Negative Item Wording Reliability`[8])
table$`% Mood Reliability`[8] <- (table$`Mood Reliability`[8]/table$`Total Reliability`[8])
table$`% Negative Item Wording Reliability`[8] <- (table$`Negative Item Wording Reliability`[8]/table$`Total Reliability`[8])

# Focus on non-remedied
PP <- as.data.frame(m3.factorloadings[["0"]][["lambda"]])  %>%
  abs() %>%
  rownames_to_column %>%
  slice(1:10) %>%
  mutate(Sub.Rel = PP^2)%>%
  mutate(Mood.Rel = Mood^2)%>%
  mutate(NW.Rel = NW^2) %>%
  as.data.frame() %>%
  mutate(Error = (1-(Sub.Rel+Mood.Rel+NW.Rel)))

table$`Substantive Reliability`[1] <- (sum(PP$PP)^2)/((sum(PP$PP)^2)+(sum(PP$Mood)^2)+sum(PP$Error))
table$`Mood Reliability`[1] <- (sum(PP$Mood)^2)/((sum(PP$PP)^2)+(sum(PP$Mood)^2)+sum(PP$Error))
table$`Negative Item Wording Reliability`[1] <- (sum(PP$NW)^2)/((sum(PP$PP)^2)+(sum(PP$Mood)^2)+(sum(PP$NW)^2)+sum(PP$Error))
table$`Total Reliability`[1] <- (table$`Substantive Reliability`[1]+table$`Mood Reliability`[1]+table$`Negative Item Wording Reliability`[1])
table$`% Mood Reliability`[1] <- (table$`Mood Reliability`[1]/table$`Total Reliability`[1])
table$`% Negative Item Wording Reliability`[1] <- (table$`Negative Item Wording Reliability`[1]/table$`Total Reliability`[1])

IRB <- as.data.frame(m3.factorloadings[["0"]][["lambda"]])  %>%
  abs() %>%
  rownames_to_column %>%
  slice(11:17) %>%
  mutate(Sub.Rel = IRB^2) %>%
  mutate(Mood.Rel = Mood^2) %>%
  mutate(NW.Rel = NW^2) %>%
  as.data.frame() %>%
  mutate(Error = (1-(Sub.Rel+Mood.Rel+NW.Rel)))

table$`Substantive Reliability`[3] <- (sum(IRB$IRB)^2)/((sum(IRB$IRB)^2)+(sum(IRB$Mood)^2)+(sum(IRB$NW)^2)+sum(IRB$Error))
table$`Mood Reliability`[3] <- (sum(IRB$Mood)^2)/((sum(IRB$IRB)^2)+(sum(IRB$Mood)^2)+(sum(IRB$NW)^2)+sum(IRB$Error))
table$`Negative Item Wording Reliability`[3] <- (sum(IRB$NW)^2)/((sum(IRB$IRB)^2)+(sum(IRB$Mood)^2)+(sum(IRB$NW)^2)+sum(IRB$Error))
table$`Total Reliability`[3] <- (table$`Substantive Reliability`[3]+table$`Mood Reliability`[3]+table$`Negative Item Wording Reliability`[3])
table$`% Mood Reliability`[3] <- (table$`Mood Reliability`[3]/table$`Total Reliability`[3])
table$`% Negative Item Wording Reliability`[3] <- (table$`Negative Item Wording Reliability`[3]/table$`Total Reliability`[3])

OCBI <- as.data.frame(m3.factorloadings[["0"]][["lambda"]])  %>%
  abs() %>%
  rownames_to_column %>%
  slice(18:24) %>%
  mutate(Sub.Rel = OCBI^2) %>%
  mutate(Mood.Rel = Mood^2) %>%
  mutate(NW.Rel = NW^2) %>%
  as.data.frame() %>%
  mutate(Error = (1-(Sub.Rel+Mood.Rel+NW.Rel)))

table$`Substantive Reliability`[5] <- (sum(OCBI$OCBI)^2)/((sum(OCBI$OCBI)^2)+(sum(OCBI$Mood)^2)+(sum(OCBI$NW)^2)+sum(OCBI$Error))
table$`Mood Reliability`[5] <- (sum(OCBI$Mood)^2)/((sum(OCBI$OCBI)^2)+(sum(OCBI$Mood)^2)+(sum(OCBI$NW)^2)+sum(OCBI$Error))
table$`Negative Item Wording Reliability`[5] <- (sum(OCBI$NW)^2)/((sum(OCBI$OCBI)^2)+(sum(OCBI$Mood)^2)+(sum(OCBI$NW)^2)+sum(OCBI$Error))
table$`Total Reliability`[5] <- (table$`Substantive Reliability`[5]+table$`Mood Reliability`[5]+table$`Negative Item Wording Reliability`[5])
table$`% Mood Reliability`[5] <- (table$`Mood Reliability`[5]/table$`Total Reliability`[5])
table$`% Negative Item Wording Reliability`[5] <- (table$`Negative Item Wording Reliability`[5]/table$`Total Reliability`[5])

OCBO <- as.data.frame(m3.factorloadings[["0"]][["lambda"]])  %>%
  abs() %>%
  rownames_to_column %>%
  slice(25:32) %>%
  mutate(Sub.Rel = OCBO^2) %>%
  mutate(Mood.Rel = Mood^2) %>%
  mutate(NW.Rel = NW^2) %>%
  as.data.frame() %>%
  mutate(Error = (1-(Sub.Rel+Mood.Rel+NW.Rel)))

table$`Substantive Reliability`[7] <- (sum(OCBO$OCBO)^2)/((sum(OCBO$OCBO)^2)+(sum(OCBO$Mood)^2)+(sum(OCBO$NW)^2)+sum(OCBO$Error))
table$`Mood Reliability`[7] <- (sum(OCBO$Mood)^2)/((sum(OCBO$OCBO)^2)+(sum(OCBO$Mood)^2)+(sum(OCBO$NW)^2)+sum(OCBO$Error))
table$`Negative Item Wording Reliability`[7] <- (sum(OCBO$NW)^2)/((sum(OCBO$OCBO)^2)+(sum(OCBO$Mood)^2)+(sum(OCBO$NW)^2)+sum(OCBO$Error))
table$`Total Reliability`[7] <- (table$`Substantive Reliability`[7]+table$`Mood Reliability`[7]+table$`Negative Item Wording Reliability`[7])
table$`% Mood Reliability`[7] <- (table$`Mood Reliability`[7]/table$`Total Reliability`[7])
table$`% Negative Item Wording Reliability`[7] <- (table$`Negative Item Wording Reliability`[7]/table$`Total Reliability`[7])

# Round columsn to nearest 100th
table <- table %>% 
  mutate_if(is.numeric, ~round(., 2))

# Call table 9
table9 <- table
```
```{r Table 5}
table8 %>%
  knitr::kable(format = "latex", booktabs=T ,escape = T) %>%
  kable_styling(latex_options = c("scale_down", "hold_position"),
                 repeat_header_method = c("replace")) %>%
  footnote(general = "* p < .05; ** p < .01; *** p < .001. ") %>%
  landscape() %>%
  kable_styling(full_width = T) %>%
  save_kable("Tables/Table5.pdf")
```
```{r Table 6}
table9 %>%
  knitr::kable(format = "latex", booktabs=T ,escape = T) %>%
  kable_styling(latex_options = c("scale_down", "hold_position"),
                 repeat_header_method = c("replace")) %>%
  footnote(general = "Total reliability is a sum of the substantive and method reliability components (see Williams et al., 2010). ") %>%
  landscape() %>%
  kable_styling(full_width = T) %>%
  save_kable("Tables/Table6.pdf")
```
# ---------------
# Insert Tables 5 and 6 about here
# ---------------

Our proposed model had a close fit ($\chi^2$(`r df.ms3`) = `r csq.ms3`, *p* = `r p.ms3`, $\chi^2$/df = `r cdfratio.ms3`, CFI = `r cfi.ms3`, RMSEA = `r remsea.ms3`, SRMR = `r srmr.ms3`) and so we moved to interpreting the results. Table 5 contains the estimates for the latent construct correlations. A consistent trend emerged. Specifically, all of the latent construct correlations were larger under the non-remedied condition relative to the remedied condition, supporting hypothesis 1. Furthermore, the proactive personality-OCBO relationship was non-significant in the remedied condition, suggesting that self-reported data indicating this relationship can be explained by methodological factors that are addressed via a one-week temporal separation (e.g., mood).

Table 6 contains the reliability decomposition for our key study variables. Mood clearly contaminated the OCBO data across both conditions, but was weaker in the remedied condition. However, mood effects were virtually identical for in-role behavior and OCBI, providing mixed support for hypothesis 3. 

# Discussion - Study 3

In our third study, we found that predictor and outcome data gathered using a one-week temporal separation of measurement resulted in lower substantive predictor-outcome relations. Indeed, in the case of proactive personality-OCBO, this relationship was rendered non-significant, suggesting that proximal causes of method variance addressed by a one-week temporal separation explained this relationship. Study 3 also provided some evidence that a one-week separation can reduce the contaminating role played by momentary mood, particularly regarding assessments of OCBO. We did not observe the same effects for in-role behavior or OCBI. This finding might suggest that a one-week temporal separation works better for some behavioral measures rather than others. However, given the findings from study 1 and 2, it could also be the case that other proximal causes of method variance that were not controlled for in this study (i.e., serial-item-order effects) suppressed the mood effects. In other words, had we included other proximal remedies into the study of a one-week temporal separation, then we might have observed stronger mood effects in the temporal separation condition.

# General Discussion

The results of our investigations revealed that proximal remedies do, indeed, reduce the contaminating role of method variance. Thus, we contribute to the literature by demonstrating that proximal remedies for method variance are, indeed, effective insofar as they produce results that differ from designs where such remedies are not utilized. Scholars wishing to address proximal causes of method variance may thus find a great deal of value in using the bundled remedies we examined here. [^9] 

[^9]: Though not tested here, including a temporal separation of measurement of one week could prove useful. However, our data suggest that this intervention is about as effective as using the bundled remedies. Researchers planning their research should keep this in mind when designing their studies.

Study 1 provided unambiguous evidence that bundling remedies can render one's substantive correlations of interest statistically non-significant, whereas study 2 isolated this effect to increasing the distance between predictor and outcome measures via filler scales and scale randomization. Specifically, study 1 demonstrated that proximal causes of method variance can account for the correlation between proactive personality and both in-role behavior and OCB. Additionally, study 2 how these remedies for method variance effect these relations for distinct- and multi-source designs deserves future research. Study 2 suggested that introducing a separation in between predictor and outcome measurements (e.g., using filler scales and randomizing scale presentation) reduces the inter-construct correlation. While it could be argued that such separation did not render the substantive correlations statistically insignificant in study 3, this could be due to other method effects (e.g., demand effects and consistency motifs) that went uncontrolled  Study 3 provided some evidence that mood effects can be nullified by separating measurements of predictors and outcome variables by one week. All studies suggested that the examined remedies appear affect the amount of substantive variance captured by a measurement model, indicating that methodological causes inflate substantive variance estimates when remedies go unused.

Echoing views discussed in the method variance literature [@PodsakoffCommonmethodbiases2003; @spectorNewPerspectiveMethod2017; @williamsIdealNonidealNomarker2015; @WilliamsMethodVarianceMarker2010], our findings suggest that scholars make use of both procedural and statistical remedies to address method bias. However, findings from study 1 suggest that when procedural remedies are ignored, scholars guided chiefly by statistical remedies may falsely conclude that methodological factors do not contaminate their data (i.e., a Type II error). We observed mood contamination that was more prominent in our remedied condition than in the non-remedied condition. Given random assignment, mood would be expected to play the same role across conditions. This suggests that we methodological factors can act to suppress method effects. As this finding was unexpected, we call for more research into the possibility that methodological factors can play a suppressing role for other method effects.

# Limitations and Future Research 

Given that proximal causes of method variance in many instances explained proactive personality-workplace behavior relations in our studies, we make the following recommendations. First, studies of similar phenomena should include proximal remedies to address proximal causes of method variance. Our study provides evidence regarding the utility of these proximal remedies and we encourage future scholars to consider this in designing their studies. Second, as we rather exhaustively studied single-source designs, studies utilizing distinct- and multi-source remedies to study phenomena like those examined here are needed. Such studies would shed light on how method factors that are common and unique affect substantive conclusions of interest [see @spectorNewPerspectiveMethod2017]. This is particularly relevant for scholars who study the role of proactive personality at work, as our findings call into question conclusions that are based on single-source data that do no involve the use of proximal remedies for method variance.

Mixed methods studies seeking to unpack the black box that is the item response process, and more specifically how method factors might contaminate this process, are sorely needed. This can technically be achieved using a similar analytical approach to the one that we have used here (i.e., DWLS estimation that also involves estimating threshold parameters) or others (e.g., item response theory-based approaches). Albeit, much larger sample sizes (n > 1000) than what we used here will be required. [^10] For instance, experiments similar to ours could be conducted but with the inclusion that participants are asked to explain what is on their mind when they select a particular item response. Such research might bring more clarity as to how and when method variance plays a contaminating role and when this method variance is uncommon or common [@spectorNewPerspectiveMethod2017].

[^10]: Indeed, we had to treat our ordinal level measures as continuous in our analysis, when clearly they are not. However, estimating parameters for ordinal data was too cost prohibitive given our resources.

Lastly, as we did not manipulate momentary mood directly to control method variance attributable to mood, future research should examine such effects and whether or not they are truly contaminants. As mood might facilitate recall, inducing individuals to consider "how they typically feel" during work on average might help individuals to more accurately describe their behaviors. 
\newpage

# References
```{r create_r-references}
r_refs(file = "r-references.bib")
```
\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}
