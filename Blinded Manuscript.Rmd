---
title         : "You've Gotta Keep 'em Separated: Examining the Efficacy of Proximal Remedies for Method Variance"
shorttitle    : "PROXIMAL METHOD VARIANCE REMEDIES"

author:
- address: XXXXXXXXXXXXXXXX
  affiliation: '1'
  corresponding: yes
  email: XXXXXXXXXXXXXXXX
  name: XXXXXXXXXXXXXXXX
- affiliation: '2'
  name: XXXXXXXXXXXXXXXX
- affilitation: '3'
  name: XXXXXXXXXXXXXXXX
affiliation:
- id: '1'
  institution: XXXXXXXXXXXXXXXX
- id: '2'
  institution: XXXXXXXXXXXXXXXX
- id: '3'
  institution: XXXXXXXXXXXXXXXX

output            : papaja::apa6_word

class: "man"
figsintext: no
figurelist: no
footnotelist: no
tablelist: no

lang: "en-US"
lineno: no

author_note: "An earlier version of this manuscript received both a Research Methods Division Best Paper Award and Best Conference Paper Award from the 2017 annual conference for the Southern Management Association. We'd like to thank several individuals for providing friendly reviewer feedback on the development of this manuscript, including Hettie Richardson, Larry Williams, Alyssa McGonagle, and Chris Rosen. Additionally, we'd like to thank members of the lavaan user group, specifically Terrence Jorgenson and Ed Rigdon for providing helpful feedback as we were troubleshooting the analysis. Any mistakes are ours. All data and code have been made publicly available for re-analysis at https://osf.io/wdskv/."

abstract: "Scholars have argued that proximal causes of method variance (e.g., mood state, demand characteristics) bias estimates of covariation derived from same-source single time-point observations. To address such method bias, scholars have proposed procedural remedies such as (1) presenting participants with a cover story to disguise the purpose of the survey, (2) randomizing item and (3) scale presentation around filler scales to eliminate the systematic influence of any item and scale context effects, and (4) introducing a brief temporal separation of one-week between assessments of the predictor and outcome variables. There are no experiments demonstrating whether data gathered using these remedies produce results that differ from those where remedies are not utilized. Here, we present the findings from three studies wherein we estimate the correlations between proactive personality and job performance behavior across conditions where these remedies were and were not utilized. We examine the effectiveness of bundling remedies (study 1), using single remedies (study 2), or inserting a one-week separation between predictor and outcome data collection (study 3). Our results demonstrate that estimates of correlation do vary across the remedies. Studies 1 and 3 suggest that bundling proximal remedies or using a one-week temporal separation weakens predictor-outcome correlations, whereas study 2 revealed that using single remedies can produce stronger or weaker correlations. We close with a call for more research, particularly theory development, that explains how design features (e.g., procedural remedies) relate to method bias."    

header-includes:
  - \raggedbottom
  - \usepackage{caption}
  
keywords: "method variance, procedural remedies, statistical remedies"

wordcount: 8256

bibliography: r-references.bib
---
```{r Load Packages, include=FALSE}
packages <- c("papaja", "apaTables","citr","tidyverse","haven","readr","MOTE",
              "frequencies",
              "mice","psych","car","boot","lavaan","semTools","semPlot","searcher","fastDummies","gtools",
              #"rmdrive",
              "flextable",
              "packrat","scales","kableExtra","huxtable")
lapply(packages, library, character.only = TRUE)
set.seed(19)
```
# Introduction

Few methodological problems related to using survey data have been discussed more frequently than the presence and impact of common method variance [CMV; @PodsakoffCommonmethodbiases2003; @PodsakoffSourcesMethodBias2012; @spectorNewPerspectiveMethod2017]. Though long debated over the past several decades [e.g., @CampbellConvergentDiscriminantValidation1959; @LanceUseindependentmeasures2015; @RichardsonTaleThreePerspectives2009; @SpectorMethodVarianceOrganizational2006], the problem of CMV is often viewed as a serious one in the organizational sciences when researchers rely upon observations made using sources believed to share common method variance [i.e., variation in observations that is attributable to a common cause, such as mood held by respondents; see @PodsakoffCommonmethodbiases2003]. To guard against CMV, procedural remedies, such as separating the collection of predictor and outcome measures in some fashion [@PodsakoffCommonmethodbiases2003; @PodsakoffSourcesMethodBias2012], have been proposed. The proposed solutions--use a cover story to disguise the purpose of a study, counterbalance item presentation, and separate measurement by an interval of time--are routinely encouraged by journal editors [e.g., @AshkanasySubmittingyourmanuscript2008].

However, while procedural remedies are often proposed as solutions that allow researchers to control for CMV [@PodsakoffCommonmethodbiases2003; @PodsakoffSourcesMethodBias2012], prior researchers have not experimentally demonstrated evidence to support this claim. Knowing whether procedural remedies control for CMV is important for scholars who are planning their research and hope to produce findings that are unbiased by method variance. Experiments (compared to quasi-experiments or observational studies) also provide a less controversial means of studying method effects. Moreover, the extent to which specific remedies mitigate concerns of common method variance is important, yet remains unknown. Therefore, we set out to test the effectiveness of proximal remedies for CMV. Furthermore, by testing procedural remedies for method variance, we have the opportunity to speak to the larger debate regarding the nature of method variance, namely, the role of method variance, whether or not it should be a concern of researchers, and which remedies to consider using.

In three studies, we contribute to the existing literature by experimentally examining the effects of using proximal remedies for method variance in single-source designs. We examine several proximal remedies for method variance for single-source designs because these designs are likely to be the most prevalent in the organizational sciences due to lower cost and reduced administrative burden. Also, single-source designs should be ripe for method variance [see @PodsakoffCommonmethodbiases2003; @PodsakoffSourcesMethodBias2012]. In study 1, we bundle a series of proximal remedies for method variance (i.e., cover story, item and scale randomization, using a filler scale) to increase the separation (and therefore the biasing effects of CMV) between a focal predictor measure (i.e., proactive personality) and a set of outcomes (i.e., in-role behavior, organizational citizenship). In study 2, we examine the unique effects of each of these procedural remedies *individually* in a follow-up experiment. In study 3, we examine the efficacy of a one-week temporal separation of measurement between our predictor and outcomes. 

# Theoretical Overview of Method Variance

Differing views exist regarding the terms 'method,' 'method variance,' and 'method bias. @PodsakoffSourcesMethodBias2012 argued that method encompasses several abstract elements (e.g., taking a paper-and-pencil instrument, responding using Likert scales, characteristics of the examiner, mood, social desirability) and that when these elements are shared across methods or measures in the same investigation there will be a convergence resulting in bias. According to Podsakoff and colleagues' view, method variance is synonymous with method bias such that if a methodological aspect is shared across studies (e.g., self-reported data are obtained on both predictors and outcomes) then the study is biased. In other words, studies that contain overlapping methods (e.g., self-reports of predictor and outcomes) are biased, whereas studies that contain procedural remedies for method variance, or include design features intended to reduce the number of overlapping elements (e.g., self-reports of predictors and other-reports on outcomes) are superior and assumed to be either less biased or unbiased. As many social science journal editors and reviewers have expectations of how researchers treat conclusions drawn from same source data, it seems this perspective is shared by many of them. 

In contrast to Podsakoff and colleagues' view, @LanceMethodEffectsMeasurement2010 defined 'method' as alternative means of enumerating observations (e.g., self- vs. other-report, explicit vs. implicit assessment) to indicate standings on latent traits. This view focuses on the impact of a methodological aspect of a study on particular observation(s) (e.g., self-reports resulting in overly positive reports of workplace behavior). Lance et al.s view implies that the impact of these aspects may (or may not) be so strong as to cause parameter estimates to differ systematically. While this view has been criticized as omitting common rater sources of method variance (e.g., mood), such sources have been included in theoretical elaborations by other scholars. Specifically, @spectorNewPerspectiveMethod2017 suggested that method variance can be common (e.g., rater biases contaminate self-appraisals of performance) or uncommon (e.g., supervisor- and subordinate-rated performance are affected by independent biases). While the former (CMV) can inflate estimated correlations, particularly if method constructs are more strongly related than the substantive constructs [@williamsMethodVarianceOrganizational1994], the latter (UMV) can attenuate estimated correlations. Further, method variance (common or uncommon) may or may not rise to such levels as to cause differences in estimated correlations (i.e., the substantive correlations of interest may or may not be biased by methodological elements) [@fullerCommonMethodsVariance2016; @spectorNewPerspectiveMethod2017; @williamsMethodVarianceOrganizational1994]. @spectorNewPerspectiveMethod2017 encourage researchers to adopt a measure-centric view of method variance and clearly articulate the cause of method variance likely to be at play with regard to one's measures.

In our manuscript, we examine whether substantive estimates regarding measures of proactive personality, in-role behavior, and organizational citizenship behavior, vary across conditions where proximal remedies have and have not been applied. Our remedies were aimed at reducing method variance believed to bias correlations linking proactive personality (predictor) to in-role and organizational citizenship behavior (outcomes). Single-source designs should be ripe for method variance because the cognitive processes of retrieval (i.e., remembering responses to previously answered items rather than the whole of one's experience), judgment (i.e., falsely judging that one has effectively retrieved relevant memories), and in some instances reporting (i.e., modifying one's responses to be consistent) may not be addressed unless a remedy is applied [see @PodsakoffCommonmethodbiases2003; @PodsakoffSourcesMethodBias2012]. We chose to study proactive personality-workplace behavior relationships for two reasons that are important for both organizational scholars and method variance testing in general. First, these relationships are often described in the literature [e.g.,@FullerJr.Changedrivennature2009; @GrantReversingExtravertedLeadership2011], suggesting that they are important to a broad audience of organizational scholars. Second, prior research suggests that method variance might influence these relationships [e.g., @FullerJr.Changedrivennature2009]. A meta-analysis by @FullerJr.Changedrivennature2009 demonstrated that these relationships were stronger when measured using same source data, rather than distinct sources. Specifically, they observed that the relationships between proactive personality and workplace behaviors were between 129% and 308% stronger when study designs used a common rather than distinct sources. Though these percentages likely misstate the impact of method variance by failing to account for uncommon method variance [e.g., @LanceUseindependentmeasures2015; @spectorNewPerspectiveMethod2017], which even further obscures the role of method variance, they nevertheless suggest that methodological factors could play a role. To resolve these disparities, we tested the same measurement model in our three studies, examining the effects of various hypothesized common method factors (e.g., momentary mood) and uncommon method factors (e.g., negative item wording). Figure 1 illustrates the substantive and methodological factors of interest.

# - - - - - - - - - - - 
# Insert Figure 1 about here
# - - - - - - - - - - - 

## Common Rater Effects

Common rater effects are any nuisance covariation introduced by gathering observations on or from the same rater [@PodsakoffCommonmethodbiases2003]. Of the many common rater effects, there are four that are particularly relevant for same-source single time-point investigations: consistency motifs, implicit theories, mood states, and trait affectivity factors. Consistency motifs refer to the propensity for respondents to maintain consistency in their responses to questions, in which the desire to appear rational should affect response reporting [@PodsakoffCommonmethodbiases2003]. Research indicates that consistency motifs contaminate measures and may even bias results [@HarrisonContextCognitionCommon1996]. Similarly, implicit theories refer to respondents’ beliefs about the covariation among particular traits, behaviors, and or outcomes that may not accurately reflect reality [@SternbergPeopleconceptionsintelligence1981]. These biases are believed to affect judgments of response appropriateness [@PodsakoffCommonmethodbiases2003]. In research settings, if participants are aware of the study purpose, it could equip them with an understanding of the hypotheses, inclining them to respond in a manner that is consistent with these hypotheses. To address these two common rater effects (consistency motifs and implicit theories), researchers have used cover stories [@PodsakoffCommonmethodbiases2003], which are designed to both disguise the hypotheses under investigation and dispel implicit theories held by the participants [@PodsakoffCommonmethodbiases2003]. 

Different from consistency motifs and implicit theories are the interrelated notions of mood states and trait affectivity, both of which may be a source of method variance. Mood states refer to respondents’ momentary or brief mood state, which can influence the contents of what is recalled from memory by priming affectively similar material stored in memory [cf. @BlaneyAffectmemoryreview1986; @ParrottMoodmemorynatural1990]. Sometimes this has a desirable effect, such as when mood-congruence facilitates recall [@BowerMoodmemory1981; @IsenPositiveaffectfactor1991]. Mood state should also be related to general affectivity, which is a respondent's propensity to view themselves or their environments in positive or negative ways [@PodsakoffCommonmethodbiases2003]. Similar to mood, trait affectivity should exert comparable effects [@spectorNewPerspectiveMethod2017]. @BriefShouldnegativeaffectivity1988 observed that negative affectivity contaminates observations of stress, job and life satisfaction, and depression, resulting in biased estimates of covariation [see also @ChenNegativeaffectivityunderlying1991; @Jeximpactnegativeaffectivity1996; @Williamsalternativeapproachmethod1994]. To address the role of mood states, researchers recommend separating measures temporally to increase the amount of time between when observations of predictor and outcome variables are made [@PodsakoffCommonmethodbiases2003; @PodsakoffSourcesMethodBias2012]. Though it involves separation by time, this remedy addresses a proximal cause of method variance (i.e., momentary mood state). Despite wide use of these procedural remedies, there is no experimental evidence that they are effective.

## Relevant Context Effects

Context effects refer to any nuisance covariation among observations caused by an item or scale’s location or relationship to other items or scales in a survey (e.g., item/scale priming and embeddedness effects). @PodsakoffCommonmethodbiases2003 proposed that item context can affect retrieval and judgments of response appropriateness. @TourangeauAttitudestructurebelief1991 demonstrated that participants respond more quickly to similar items placed closer together rather than further apart. Tourangeau et al.’s (1991) findings suggest that item inter-correlations may be a function of the consistent ordering of items within a particular instrument. Essentially, if responses made to previous items or scales are maintained in short-term memory, these responses may influence responses to future items or scales. @SteinbergContextserialordereffects1994 found that one item in a 20-item scale became slightly more discriminating when it was presented later in the scale rather than as the first item, which suggests that a systematic item context effect can artificially increase the systematic variance explained by a latent trait. Furthermore, when negative scales were presented first, estimated inter-scale correlations were much stronger, suggesting the presence of scale embeddedness effects. 

To address item priming and item embeddedness effects, researchers have proposed counterbalancing or randomizing the order of items or scales within a survey or including one or more filler scales in-between the administration of a predictor and outcome scale [@PodsakoffCommonmethodbiases2003]. Randomizing items within a scale essentially eliminates the influence of a serial item-order effect that would apply to a study that presents all items within a scale in the same order. Randomizing scale presentation within a survey should have the same influence but at the level of the scale, thus eliminating any potential scale-serial-order effects. Lastly, introducing filler scales makes it more difficult to recall responses to prior scales, reducing the influence of recall effects. Similarly to the remedies mentioned above (i.e., using cover stories or temporal separation), no experimental research has examined the efficacy of these remedies.

# General Purpose

Across three studies, we examine the effectiveness of proximal remedies for method variance. Procedural remedies for method variance are tactics designed to reduce any nuisance covariation introduced by gathering observations on (or from) a common source [@PodsakoffCommonmethodbiases2003]. Of the many remedies proposed by @PodsakoffCommonmethodbiases2003, those that are relevant for same-source single time-point are our focus. These remedies include such methodological aspects as (i) using a cover story to introduce a psychological separation of measurement to eliminate hypothesis guessing effects, (ii) randomizing items and scales to eliminate any method effects related to item and scale locations within a survey such as item priming effects and item/scale embeddedness effects [i.e., measurement context effects; see @HarrisonContextCognitionCommon1996; @SteinbergContextserialordereffects1994; @WeinbergerItemsContextEffects2006], (iii) introducing a filler scale in-between the predictor and outcome measures within a survey to eliminate recall and consistency motif effects [see @SalancikExaminationNeedSatisfactionModels1977], and (iv) introducing a temporal separation of measurement, such as a one-week separation of measurement, between the collection of predictor and outcome measures. Collectively, we refer to these remedies as proximal remedies for method variance. 

Given that proximal causes of method variance can inflate the correlation among same-source single-time-point observations [@PodsakoffCommonmethodbiases2003], applying a proximal remedy to target these proximal causes of method variance should result in weaker correlations between the substantive factors of interest. Therefore, each study tests the following hypothesis:

*Hypothesis 1: Data gathered using proximal remedies (i.e., using of a cover story, randomizing item and scale order, using a filler scale, separating collection of predictor data from outcome data by one week) will result in weaker correlations between the substantive predictor and outcome variables.*

As suggested by @spectorNewPerspectiveMethod2017, when method variance is shared by items belonging to a measure, latent factor approaches will mistake method variance for trait variance [see also @PodsakoffCommonmethodbiases2003]. However, when a remedy has been applied, such method variance should not be correlated with the latent trait in question. For instance, maintaining a consistent item-order in one's survey introduces a serial-item order effect that could cause factor loadings for a measurement model to be biased. Conversely, randomizing the presentation of items such that participants receive unique orderings of items introduces unsystematic variation into the group-level measurement (i.e., item-order effects are largely confined to the item residuals). As the reliability of observed data can be decomposed into modeled sources of variance, both method and non-method [@WilliamsMethodVarianceMarker2010], we can examine how methodological conditions influence estimates of substantive variance across conditions. As proximal causes of method variance operate within non-remedied data, studies that do not use proximal remedies should result in a biased impression of the amount of substantive variance that is captured by a measurement model [@PodsakoffCommonmethodbiases2003]. In other words, in comparing the amount of substantive and method variance captured by a measurement model across studies that do and do not apply remedies, non-remedied data should reflect more (and remedied data should reflect less) substantive variance. Therefore, each study tests the following hypothesis:

*Hypothesis 2: Data gathered using proximal remedies (e.g., using of a cover story, randomizing item- and scale-order, using a filler scale) will result in weaker estimates of substantive (i.e., non-method) variance for the substantive variables.*

Affective traits should (and appear to) relate to general descriptions of workplace behavior, such as in-role behavior and OCB [see @kaplanRolePositiveNegative2009]. Therefore, viewing affective tendencies as methodological contaminants per se, and seeking to control them statistically, would be inadvisable [@spectorWhyNegativeAffectivity2000]. However, momentary mood, though a possible cause of workplace behavior within a specific performance episode [@bealEpisodicProcessModel2005], should be independent from general descriptions of workplace behavior. Any relationships linking momentary mood to substantive item indicators could, rather, be interpreted as a methodological contaminant [@spectorNewPerspectiveMethod2017; @WilliamsMethodVarianceMarker2010]. Therefore, we examined the contaminating role of mood across all study conditions but in study 3 examined the efficacy of a procedural remedy (i.e., gathering predictor and outcome data with one-week separating these measurements) to address these effects.

# Study 1 - Bundling Proximal Remedies 

In study 1, we examined the efficacy of bundling proximal remedies for method variance. More specifically, we examined how bundling a cover story, randomizing items within scales, and randomizing the presentation of scales around a filler scale affects the parameter estimates for our substantive measures. Participants were randomly assigned to a condition where observations were obtained with or without these remedies. For our non-remedied (i.e., control) condition, an online self-reported survey was designed such that all items and scales appeared in the same order but were separated by presenting each scale on its own page. This was a conventional survey design. For our remedied condition, we used a cover story that was designed to both blind participants to the purpose of our study and dispel implicit theories regarding the substantive intention guiding a study such as ours: test for positive relationships linking proactive personality to workplace behaviors. Also, the item and scale order were randomized around a series of filler scales. These filler scales measured other sources of method variance that were not addressed by our procedural remedies (i.e., positive and negative affectivity, and momentary mood). To ensure that a large enough separation of measurement between predictor and outcome measures, a variety of other scales that are commonly used to examined method effects were included (i.e., attitudes toward the color blue, preference for brand label clothing, seeing a survey as valuable, and seeing a survey as enjoyable). However, as these scales were not of substantive interest to us, it should be kept in mind that their inclusion was solely based on ensuring that the filler scales provide a strong proximal separation of measurement. Researchers who are interested in these data for substantive reasons can find them by visiting our study details on the Open Science Framework (https://osf.io/wdskv/?view_only=a50e339d4cd2497db02574d3861639da). 

# Methods - Study 1

## Sample and Procedure
```{r Load Study 1 Data, include=FALSE}
#Load data
data <- read_sav("Data/Study 1.sav")

#Count of individuals who agree to participate (Q42=1 OR Q44=1). Note: 1 is control/non-remedied and 2 is treatment/experimental/remedied.
N1 <- as.numeric(freq_vect(data$Q42)[1,"Count"])
N2 <- as.numeric(freq_vect(data$Q44)[1,"Count"])
Nall <- sum(N1,N2)

#Subset in those who agreed to participate and passed the manipulation check question (Q42=1 OR Q44=1 & correctly answered attention checks).
Agree <- subset(data, Q42 == 1 & Q47 == 1 | Q44 == 1 & Q79 == 2)

#Get success rates (SR) for responding correctly across both conditions.
n1 <- as.numeric(freq_vect(Agree$Q42)[1,"Count"])
n2 <- as.numeric(freq_vect(Agree$Q42)[2,"Count"])
n <- n1+n2
sr1 <- percent(n1/N1)
sr2 <- percent(n2/N2)

#Filter in attentive responders.
Inattent <- subset(Agree, Q26_11 == 1 | Q105_11 ==  1)

#Calculate sum of inattentive responders (iar) that have been screened out of the Inattent dataset for either incorrectly or failing to answer "strongly disagree" to an attention check question. 
iar1 <- freq_vect(Agree$Q26_11)
iar1[3,1] <- "2" 
iar1[4,1] <- "2" 
iar1[5,1] <- "2" 
iar1 <- freq_vect(iar1$data)
iar1 <- as.numeric(iar1[2,"Count"])
iar2 <- freq_vect(Agree$Q105_11)
iar2[2,1] <- "2" 
iar2[3,1] <- "2" 
iar2 <- freq_vect(iar2$data)
iar2 <- as.numeric(iar2[2,"Count"])
iar <- iar1+iar2+(nrow(Agree) - (sum(iar1,iar2)) - nrow(Inattent))

#Delete cases who did not complete the demographic questionnaire and barely completed any tests.
Inattent <- Inattent[-c(1, 275, 276, 277),]

#Setup demographics.
#Rename variables.
Inattent$COND <- Inattent$Q42 
Inattent$AGE <- Inattent$Q33
Inattent$RACE <- Inattent$Q34
Inattent$GENDER <- Inattent$Q35
Inattent$EDUCAT <- Inattent$Q36.0
Inattent$EMPLOYED <- Inattent$Q37
Inattent$JOBTENURE <- Inattent$Q38
Inattent$USREGION <- Inattent$Q40
Inattent$PROFESSION <- Inattent$Q39

#Then, combine data from separate surveys in order to form the final cleaned and well-structured dataset. Note: there were two surveys setup on Qualtrics, hence why data needs to be combined. You'll see redundant items moving forward.
##Convert NAs to 0 to allow merging. Later, you'll need to reasign NAs to conduct a missing data analysis. 
Inattent[is.na(Inattent)] <- 0

#Proactive personality
Inattent$PP1	<-	Inattent$Q26_1	+Inattent$Q105_1 #Wherever I have been, I have been a powerful force for change.
Inattent$PP2	<-	Inattent$Q26_2	+Inattent$Q105_2 #I am constantly on the lookout for new ways to improve my life.
Inattent$PP3	<-	Inattent$Q26_3	+Inattent$Q105_3 #If I see something I don't like, I fix it.
Inattent$PP4	<-	Inattent$Q26_4	+Inattent$Q105_4 #I am always looking for better ways to do things.
Inattent$PP5	<-	Inattent$Q26_5	+Inattent$Q105_5 #No matter what the odds, if I believe in something, I will make it happen.
Inattent$PP6	<-	Inattent$Q26_6	+Inattent$Q105_6 #Nothing is more exciting than seeing my ideas turn into reality.
Inattent$PP7	<-	Inattent$Q26_7	+Inattent$Q105_7 #I love being a champion for my ideas, even against others' opposition.
Inattent$PP8	<-	Inattent$Q26_8	+Inattent$Q105_8 #I excel at identifying opportunities.
Inattent$PP9	<-	Inattent$Q26_9	+Inattent$Q105_9 #If I believe in an idea, no obstacle will prevent me.
Inattent$PP10	<-	Inattent$Q26_10	+Inattent$Q105_10 #I can spot a good opportunity long before others can.

#Voice
Inattent$VC1	<-	Inattent$Q27_1	+	Inattent$Q106_1 #I develop and make recommendations concerning issues that affect my work group.
Inattent$VC2	<-	Inattent$Q27_2	+	Inattent$Q106_2 #I speak up and encourage others in my group to get involved in issues that affect the group.
Inattent$VC3	<-	Inattent$Q27_3	+	Inattent$Q106_3 #I communicate my opinions about work issues to others in my group even if my opinion is different and others in the group disagree with me.
Inattent$VC4	<-	Inattent$Q27_4	+	Inattent$Q106_4 #I keep well informed about issues where my opinion might be useful to my work group.
Inattent$VC5	<-	Inattent$Q27_5	+	Inattent$Q106_5 #I get involved in issues that affect the quality of work life here in my group.
Inattent$VC6	<-	Inattent$Q27_6	+	Inattent$Q106_6 #I speak up in my group with ideas for new projects or changes in procedures.

#Taking Charge
Inattent$TC1	<-	Inattent$Q28_1	+	Inattent$Q106_8  #I often try to adopt improved procedures for doing my job.
Inattent$TC2	<-	Inattent$Q28_2	+	Inattent$Q106_9  #I often try to change how my job is executed in order to be more effective.
Inattent$TC3	<-	Inattent$Q28_3	+	Inattent$Q106_10 #I often try to bring about improved procedures for the work unit or department.
Inattent$TC4	<-	Inattent$Q28_4	+	Inattent$Q106_11 #I often try to institute new work methods that are more effective for this company.
Inattent$TC5	<-	Inattent$Q28_5	+	Inattent$Q106_12 #I often try to change organizational rules or policies that are nonproductive or counterproductive.
Inattent$TC6	<-	Inattent$Q28_6	+	Inattent$Q106_13 #I often make constructive suggestions for imprving how things operate within the organization.
Inattent$TC7	<-	Inattent$Q28_7	+	Inattent$Q106_14 #I often try to correct a faulty procedure or practice.
Inattent$TC8	<-	Inattent$Q28_8	+	Inattent$Q106_15 #I often try to eliminate redundant or unnecessary procedures.
Inattent$TC9	<-	Inattent$Q28_9	+	Inattent$Q106_16 #I often try to implement solutions to pressing organizational problems.
Inattent$TC10	<-	Inattent$Q28_10	+	Inattent$Q106_17 #I often try to introduce new structures, technologies, or approaches to improve efficiency

#OCBI
Inattent$OCBI1	<-	Inattent$Q29_1	+	Inattent$Q106_19 #I help others who have been absent.
Inattent$OCBI2	<-	Inattent$Q29_2	+	Inattent$Q106_20 #I help others who have heavy work loads.
Inattent$OCBI3  <-	Inattent$Q29_3	+	Inattent$Q106_21 #I assist my supervisor with his/her work load (when not asked).
Inattent$OCBI4	<-	Inattent$Q29_4	+	Inattent$Q106_22 #I take time to listen to co-workers' problems and worries.
Inattent$OCBI5	<-	Inattent$Q29_5	+	Inattent$Q106_23 #I go out of my way to help new employees.
Inattent$OCBI6	<-	Inattent$Q29_6	+	Inattent$Q106_24 #I take a personal interest in other employees.
Inattent$OCBI7	<-	Inattent$Q29_7	+	Inattent$Q106_25 #I pass along information to co-workers.

#OCBO
Inattent$OCBO1	<-	Inattent$Q30_1	+	Inattent$Q106_27 #My attendance at work is above the norm.
Inattent$OCBO2	<-	Inattent$Q30_2	+	Inattent$Q106_28 #I give advance notice when I'm unable to come to work.
Inattent$OCBO3	<-	Inattent$Q30_3	+	Inattent$Q106_29 #I take undeserved work breaks. (r) 
Inattent$OCBO4	<-	Inattent$Q30_4	+	Inattent$Q106_30 #I spend a great deal of time with personal phone conversations. (r)
Inattent$OCBO5	<-	Inattent$Q30_5	+	Inattent$Q106_31 #I complain about insignificant things at work. (r)
Inattent$OCBO6	<-	Inattent$Q30_6	+	Inattent$Q106_32 #I conserve and protect organizational property.
Inattent$OCBO7	<-	Inattent$Q30_7	+	Inattent$Q106_33 #I adhere to informal rules devised to maintain order. 

#IRB
Inattent$IRB1	<-	Inattent$Q31_1	+	Inattent$Q106_35 #I adepquately complete assigned duties.
Inattent$IRB2	<-	Inattent$Q31_2	+	Inattent$Q106_36 #I fulfill responsibilities specific in my job description.
Inattent$IRB3	<-	Inattent$Q31_3	+	Inattent$Q106_37 #I perform tasks that are expected of me.
Inattent$IRB4	<-	Inattent$Q31_4	+	Inattent$Q106_38 #I meet formal performance requirements of the job.
Inattent$IRB5	<-	Inattent$Q31_5	+	Inattent$Q106_39 #I engage in activities that will directly affect my performance.
Inattent$IRB6	<-	Inattent$Q31_6	+	Inattent$Q106_40 #I nelgect aspects of my job that I'm obligated to perform. (r)
Inattent$IRB7	<-	Inattent$Q31_7	+	Inattent$Q106_41 #I fail to perform essential job duties. (r)

#Consistency Motif
Inattent$CM1 <- Inattent$Q27_7 + Inattent$Q106_7   #"I am a brave person"
Inattent$CM2 <- Inattent$Q28_11 + Inattent$Q106_18 #"I am a courageous person"  
Inattent$CM3 <- Inattent$Q29_8 + Inattent$Q106_26  #"I am a talkative person"
Inattent$CM4 <- Inattent$Q30_8 + Inattent$Q106_34  #"I am a silent person"
Inattent$CM5 <- Inattent$Q33_6 + Inattent$Q112_6   #"I am an optimistic person"
Inattent$CM6 <- Inattent$Q41_7 + Inattent$Q116_7   #"I am a pessimistic person"
Inattent$CM7 <- Inattent$Q31_8 + Inattent$Q106_42  #"I seldom feel blue."
Inattent$CM8 <- Inattent$Q32_5 + Inattent$Q111_5   #"I often feel blue."

###Create PANAS items.
##PA
Inattent$PA1 <- Inattent$Q34_1 + Inattent$Q113_1   #Interested
Inattent$PA2 <- Inattent$Q34_3 + Inattent$Q113_3   #Excited
Inattent$PA3 <- Inattent$Q34_5 + Inattent$Q113_5   #Strong
Inattent$PA4 <- Inattent$Q34_9 + Inattent$Q113_9   #Enthusiastic
Inattent$PA5 <- Inattent$Q34_10 + Inattent$Q113_10 #Proud
Inattent$PA6 <- Inattent$Q35_2 + Inattent$Q114_2   #Alert
Inattent$PA7 <- Inattent$Q35_4 + Inattent$Q114_4   #Inspired
Inattent$PA8 <- Inattent$Q35_6 + Inattent$Q114_6   #Determined
Inattent$PA9 <- Inattent$Q35_7 + Inattent$Q114_7   #Attentive
Inattent$PA10 <- Inattent$Q35_9 + Inattent$Q114_9  #Active

##NA
Inattent$NA1 <- Inattent$Q34_2 + Inattent$Q113_2    #Distressed
Inattent$NA2 <- Inattent$Q34_4 + Inattent$Q113_4    #Upset
Inattent$NA3 <- Inattent$Q34_6 + Inattent$Q113_6    #Guilty
Inattent$NA4 <- Inattent$Q34_7 + Inattent$Q113_7    #Scared
Inattent$NA5 <- Inattent$Q34_8 + Inattent$Q113_8    #Hostile
Inattent$NA6 <- Inattent$Q35_1 + Inattent$Q114_1    #Irritable
Inattent$NA7 <- Inattent$Q35_3 + Inattent$Q114_3    #Ashamed
Inattent$NA8 <- Inattent$Q35_5 + Inattent$Q114_5    #Nervous
Inattent$NA9 <- Inattent$Q35_8 + Inattent$Q114_8    #Jittery
Inattent$NA10 <- Inattent$Q35_10 + Inattent$Q114_10 #Afraid

##Mood
Inattent$MOOD <- Inattent$Q36 + Inattent$Q115 # “My mood today can best be described as...”

#Recode 0 values to missing.
l <- Inattent[c(224:299)]
l <- as.data.frame(apply(l, 2, function(x) {x <- recode(x,"0=NA"); x}))

#Retain only variables used for testing purposes.
data1 <- cbind(Inattent[c(215:223)],l)
N <- as.numeric(nrow(data1))
n1f <- as.numeric(freq_vect(data1$COND)[2,"Count"])
n2f <- as.numeric(freq_vect(data1$COND)[1,"Count"])

#Missing data analysis using 'sapply(data1, function(x) sum(is.na(x)))' revealed some missing likert data. 
init <- mice(data1, maxit = 0)
meth <- init$method
predM <- init$predictorMatrix
meth[c("PP5")]="norm"
meth[c("OCBI5")]="norm"
meth[c("IRB4")]="norm"
imputed <- mice(data1, method=meth, predictorMatrix=predM, m=5)
data1 <- complete(imputed)

#Round imputed data to nearest whole number for estimation purposes. 
#data1$PP5 <- ceiling(data1$PP5)
#data1$OCBI5 <- ceiling(data1$OCBI5)
#data1$IRB4 <- ceiling(data1$IRB4)

#Calculate descriptives
data1$AGE <- as.numeric(data1$AGE)
M <- mean(data1$AGE, na.rm = TRUE)
SD <- sd(data1$AGE, na.rm = TRUE)

#Calculate frequencies
##Recode single "f" to female. 
female.n <- as.numeric(freq_vect(data1$GENDER)[1,"Count"])
female.p <- as.numeric(freq_vect(data1$GENDER)[1,"Percentage"])
white.n <- as.numeric(freq_vect(data1$RACE)[2,"Count"])
white.p <- as.numeric(freq_vect(data1$RACE)[2,"Percentage"])
fulltime.n <- as.numeric(freq_vect(data1$EMPLOYED)[4,"Count"])
fulltime.p <- as.numeric(freq_vect(data1$EMPLOYED)[4,"Percentage"])

# Create response style indicators from observed data (Falk & Cai, 2016).
# Recode all Likert measures (l)
l <- data1[c(10:56)]
## Extreme Response Style (ERS)
ERS <- as.data.frame(apply(l, 2, function(x) {x <- recode(x,"1=1; 2=0; 3=0; 4=0; 5=1"); x}))
ERS <- as.data.frame(round(rowSums(ERS)/ncol(ERS),2))
## Midpoint Response Style(MRS)
MRS <- as.data.frame(apply(l, 2, function(x) {x <- recode(x,"1=0; 2=0; 3=1; 4=0; 5=0"); x}))
MRS <- as.data.frame(round(rowSums(MRS)/ncol(MRS),2))
## Acquiescence as a tendency to respond above the midpoint
ARS <- as.data.frame(apply(l, 2, function(x) {x <- recode(x,"1=0; 2=0; 3=0; 4=1; 5=1"); x}))
ARS <- as.data.frame(round(rowSums(ARS)/ncol(ARS),2))
## Dis-acquiescence as a tendency to respond above the midpoint
DRS <- as.data.frame(apply(l, 2, function(x) {x <- recode(x,"1=1; 2=1; 3=0; 4=0; 5=0"); x}))
DRS <- as.data.frame(round(rowSums(DRS)/ncol(DRS),2))
## Socially desirable responding. Requires separating datasets into positive and negatively worded items and keying for socially desirable resonding.
lp <- data1[c(10:44,48:54)]
SDR1 <- as.data.frame(apply(l, 2, function(x) {x <- recode(x,"1=0; 2=0; 3=0; 4=1; 5=0"); x}))
ln <- data1[c(45:47,55,56)]
SDR2 <- as.data.frame(apply(l, 2, function(x) {x <- recode(x,"1=0; 2=1; 3=0; 4=0; 5=0"); x}))
SDR <- cbind(SDR1,SDR2)
SDR<- as.data.frame(round(rowSums(SDR)/ncol(SDR),2))

## Merge all response style variables.
RS <- cbind(ERS,MRS,ARS,DRS,SDR)

## Merge response style measures with main dataset.
data1 <- cbind(data1,RS)

# Rename response style variables.
colnames(data1)[colnames(data1)== "round(rowSums(ERS)/ncol(ERS), 2)"] <- "ERS"
colnames(data1)[colnames(data1)== "round(rowSums(MRS)/ncol(MRS), 2)"] <- "MRS"
colnames(data1)[colnames(data1)== "round(rowSums(ARS)/ncol(ARS), 2)"] <- "ARS"
colnames(data1)[colnames(data1)== "round(rowSums(DRS)/ncol(DRS), 2)"] <- "DRS"
colnames(data1)[colnames(data1)== "round(rowSums(SDR)/ncol(SDR), 2)"] <- "SDR"
```
  
Six hundred and twenty-one workers from Amazon's Mechanical Turk were paid $1.30 for completing a survey, of which `r Nall` agreed to participate in our study after reading our informed consent form. These `r Nall` participants were randomly assigned to one of two conditions: non-remedied (n = `r N1`) or remedied (n = `r N2`). Approximately `r sr1` (n = `r n1`) in the non-remedied and approximately `r sr2` (n = `r n2`) in the remedied condition correctly responded to the manipulation check of proper interpretation of the study purpose and were allowed to continue. Respondents' data were eliminated due to incorrectly or not responding to an attention check item asking individuals to "Click on the first circle indicating 'Strongly Disagree?'" (n = `r iar`) [@MeadeIdentifyingcarelessresponses2012], or not reporting demographics and abandoning the survey (n = 4). This filtering process resulted in a final sample of `r N` individuals (`r n1f` in the no remedies and `r n2f` in the bundled remedies condition). Notwithstanding missing demographic data, the sample was predominantly Caucasian, and the average age was *M* = `r apa(M,2,T)` (*SD* = `r apa(SD,2,T)`). The majority (n = `r fulltime.n`) of respondents worked full-time. Three cases of missing item-level data were handled using the normal model approach, which treats ordinal missing data as normally distributed [for more details, see @WuComparisonImputationStrategies2015].
  
Following guidance by @PodsakoffCommonmethodbiases2003, we sought to create a psychological separation of measurement using a cover story. In the bundled remedies condition where the proximal separation of measurement remedy was used, participants were given a cover story designed to disguise the purpose of the study: "In this study, you will be asked to respond to statements about yourself and how you behave at work. Separate researchers built the content of this survey for separate purposes, and so the questions may or may not relate to one another. As such, there are no right or wrong answers, so please provide honest responses." By comparison, in the no remedies condition participants were given a message to make the purpose of the study transparent: "The purpose of this study is to test for relationships between proactive personality and workplace behaviors (including taking charge at work, having a voice in the workplace, organizational citizenship behavior at work, and job performance)." These participants were also given a survey with all items and scales presented in the same order (demographics were presented last). Following the randomly assigned cover story, we asked our participants to respond to the following item indicating whether they understood the purpose of our study: "Before you take our survey, please tell us which of the following correctly describes the purpose of this study." Three response options were given: (a) "The purpose of this study is to test for relationships between personality and workplace behaviors. As such, there is a clear purpose to the study." (b) "Separate researchers built the content of this survey for separate purposes, and so the questions may or may not relate to one another. As such, there is no single clear purpose to this study." and (c) "The purpose of this study is to measure emotional intelligence and workplace behaviors." In addition to the use of a cover story, we followed guidance by @PodsakoffCommonmethodbiases2003 and included other procedural remedies for method variance within the bundled remedies condition. We placed filler scales in between the administration of the proactive personality and outcomes scales. Notably, all outcome measures included in the same survey block. Several filler scales were included, which contained the positive and negative affect schedule [@WatsonDevelopmentValidationBrief1988] and psychometric synonyms and antonyms [@goldbergPredictionSemanticConsistency1985]. Additionally, to address scale-order effects, we randomized the placement of proactive personality and outcomes scales. To address item-order effects, we randomized items within all scales.

## Measures
```{r Study 1: Cronbach Alphas, include = FALSE}
#Create scale scores
my.keys.list <- list(PP=c("PP1","PP2","PP3","PP4","PP5","PP6","PP7","PP8","PP9","PP10"),
                     IRB=c("IRB1","IRB2","IRB3","IRB4","IRB5","-IRB6","-IRB7"), 
                     OCBI=c("OCBI1","OCBI2","OCBI3","OCBI4","OCBI5","OCBI6","OCBI7"),
                     OCBO=c("OCBO1","OCBO2", "-OCBO3","-OCBO4","-OCBO5","OCBO6","OCBO7"),
                     PA=c("PA1","PA2","PA3","PA4","PA5","PA6","PA7","PA8","PA9","PA10"))
my.scales <- scoreItems(my.keys.list,data1)
PP.alpha <- round(my.scales[["alpha"]][1],2)
IRB.alpha <- round(my.scales[["alpha"]][2],2)
OCBI.alpha <- round(my.scales[["alpha"]][3],2)
OCBO.alpha <- round(my.scales[["alpha"]][4],2)
PA.alpha <- round(my.scales[["alpha"]][5],2)
```

### Proactive personality

We used the 10-item proactive personality scale to capture proactive personality [@SeibertProactivepersonalitycareer1999]. Example items include: "I am constantly on the lookout for new ways to improve," "If I see something I don't like, I fix it," and "I excel at identifying opportunities." This and all scales included in this study utilized a five-point Likert rating scale (1 = strongly disagree, 5 = strongly agree) ($\alpha$ = `r apa(PP.alpha,2,T)`).

### In-role and organizational citizenship behavior 

Williams and Anderson’s (1994) In-Role Performance Behavior (IRB) and Organizational Citizenship Behavior (OCB) scales, the latter of which includes OCBI (OCB directed at the individual) and OCBO (OCB directed at the organization), were used as outcome measures. The items were initially written to reflect a supervisor's perspective and so were adjusted here to be self-referent. Example items (and Cronbach alphas) for each respective scale: "I perform tasks that are expected of me" ($\alpha$ = `r apa(IRB.alpha,2,T)`), "I take a personal interest in other employees" ($\alpha$ = `r apa(OCBI.alpha,2,T)`), and "I conserve and protect organizational property" ($\alpha$ = `r apa(OCBO.alpha,2,T)`).

### Positive affectivity

We administered the positive and negative affect schedule [PANAS; see @WatsonDevelopmentValidationBrief1988] and asked individuals to describe their how they generally feel. Data were modeled using a bi-factor approach [see @LeuePANASstructurerevisited2011] and only positive affectivity trait scores were retained for our purposes ($\alpha$ = `r apa(PA.alpha,2,T)`) to reduce the number of parameters we estimated. We modeled positive affectivity as a single-item latent construct following guidance by @PetrescuMarketingresearchusing2013.

### Momentary mood state 

Momentary mood state was measured with a single item (“My mood today can best be described as...”) with a seven-point scale (1 = unpleasant; 7 = pleasant), which was used to model a latent momentary mood construct. Following guidance on modeling single-item latent constructs [@PetrescuMarketingresearchusing2013], we set $\lambda$ conservatively as .95 * variance. We also calculated the error variance as the sample variance of this item * (1 - scale reliability) and the scale reliability was set at .85. 

```{r Study 1: Construct Validity for Affect, include = FALSE}
mood.cfa <- '
#Affectivity
PA =~ PA1 + PA2 + PA3 + PA4 + PA5 + PA6 + PA7 + PA8 + PA9 + PA10 
Na =~ NA1 + NA2 + NA3 + NA4 + NA5 + NA6 + NA7 + NA8 + NA9 + NA10 
AP =~ PA1 + PA2 + PA3 + PA4 + PA5 + PA6 + PA7 + PA8 + PA9 + PA10 + NA1 + NA2 + NA3 + NA4 + NA5 + NA6 + NA7 + NA8 + NA9 + NA10 

#Constrain bifactor covariances
PA ~~ 0*Na
PA ~~ 0*AP
Na ~~ 0*AP
'
mood <- cfa(mood.cfa, data = data1, estimator = "DWLS", parameterization = "theta", information = "expected",  std.lv=TRUE, test = "Satorra-Bentler")
summary(mood, standardized = TRUE, fit.measures =TRUE)
affect <- as.data.frame(predict(mood))

#Bind to data
data1 <- cbind(data1,affect[c(1)])
```

## Analytical Approach

We conducted a multiple group analysis with 'lavaan' in R. Specifically, a model was tested whereby all latent construct covariances were freely estimated with certain exceptions. Given the presence of negatively-keyed items in these measures, we included a negative item wording content factor that explained variance in the negatively-keyed items only [see @dalalNegativelyWordedItems2015; @ZhangImprovingFactorStructure2016]. This negative method factor was theoretically independent from IRB and OCBO [@ConwayWhatReviewersShould2010]. Positive affectivity was modeled as a cause of the momentary mood state [@spectorNewPerspectiveMethod2017; @watsonDevelopmentValidationBrief1988]. Importantly, mood was modeled as a contaminant of substantive item responses whereas positive affectivity was modeled as a correlate of the substantive factors of interest [@kaplanRolePositiveNegative2009]. This approach allowed us to capture the contaminating influences of mood independent of the substantive influences of positive affectivity [for a similar approach, see @spectorNewPerspectiveMethod2017]. As our scales are Likert-type (i.e., only five response categories, cannot be normally distributed), we used the diagonally weighted least squares (DWLS) estimator and scale-shifted test statistics [see @beauducelPerformanceMaximumLikelihood2006; @katsikatsouPairwiseLikelihoodEstimation2012]. Model fit was assessed by means of the Satorra-Bentler scaled chi-square test statistics and robust variants of the root-mean-square error of approximation (RMSEA), the standardized root-mean-square residual (SRMR), and the comparative fit index (CFI) [see @katsikatsouPairwiseLikelihoodEstimation2012]. According to @huCutoffCriteriaFit1999, RMSEA of ~.06 indicates a close fit, whereas an RMSEA of ~.08 indicates acceptable fit [see also @beauducelPerformanceMaximumLikelihood2006]. For the SRMR, a cutoff close to .08 or below has been recommended [@huCutoffCriteriaFit1999]. The CFI should be greater than .95.

Following an assessment of model data fit, we tested our hypotheses by computing z-scores, which described the difference between key parameters of interest across conditions (*p* < .05). For hypothesis 1, these difference scores were in regard to the latent factor correlations. For hypothesis 2, we decomposed the reliability of our measures into substantive and method factors following guidance from the literature @WilliamsMethodVarianceMarker2010. Briefly, this involves estimating the proportion of a measure's reliability represented by trait, measured method (e.g., mood and negative item wording) represented and error. The reliability estimates are ratios describing the estimated proportion of substantive and method variance relative to all other sources of modeled variance (trait, method, and error) [see @WilliamsMethodVarianceMarker2010]. This clarifies how the procedural remedies affect substantive and method factors [@coteMeasurementErrorTheory1988; @williamsMethodVarianceOrganizational1994]. Crucially, these estimates only account for measured factors. As some method factors could not be measured (e.g., order effects), we strive to interpret our estimates in light of such confounds. Furthermore, as we are not aware of any accepted statistical test comparing decomposed reliability coefficients, we first interpreted the reliability decomposition tables descriptively. Still, to address concerns regarding statistical significance, we have also reported z-scores describing the average difference between the factor loadings for a given measurement model. For a given measure, this z-score was calculated by first computing a z-score that described the difference between a pair of factor loadings and then taking the average of these z-scores to see whether the remedies affected estimates of the substantive variance for each scale. 

# Results - Study 1
```{r Study 1: Build for Descriptives, include = FALSE}
#Create Descriptives Table
scales <- my.scales[["scores"]]
scales <- cbind(data1[c(1,2,4)],scales,data1[c(85)])
table1 <- apa.cor.table(scales, table.number = 1, landscape = TRUE, show.conf.interval = TRUE)
```
```{r results="asis"}
#table1 
```
```{r Study 1: Model Testing, include = FALSE}
# Create pvalue function to conver values < .001 to "<.001".
pvalr <- function(pvals, sig.limit = .001, digits = 3, html = FALSE) {

  roundr <- function(x, digits = 1) {
    res <- sprintf(paste0('%.', digits, 'f'), x)
    zzz <- paste0('0.', paste(rep('0', digits), collapse = ''))
    res[res == paste0('-', zzz)] <- zzz
    res
  }

  sapply(pvals, function(x, sig.limit) {
    if (x < sig.limit)
      if (html)
        return(sprintf('&lt; %s', format(sig.limit))) else
          return(sprintf('< %s', format(sig.limit)))
    if (x > .1)
      return(roundr(x, digits = 2)) else
        return(roundr(x, digits = digits))
  }, sig.limit = sig.limit)
}

#Model 1 freely estimates these differences. 
m1.cfa <- ' 
# Substantive factors
PP =~ c(pp1a, pp1b)*PP1 + c(pp2a, pp2b)*PP2 + c(pp3a,pp3b)*PP3 + c(pp4a,pp4b)*PP4 + c(pp5a,pp5b)*PP5 + c(pp6a,pp6b)*PP6 + c(pp7a,pp7b)*PP7 + c(pp8a,pp8b)*PP8 + c(pp9a,pp9b)*PP9 + c(pp10a,pp10b)*PP10
IRB =~ c(irb1a,irb1b)*IRB1 + c(irb2a,irb2b)*IRB2 + c(irb3a,irb3b)*IRB3 + c(irb4a,irb4b)*IRB4 + c(irb5a,irb5b)*IRB5 + c(irb6a,irb6b)*IRB6 + c(irb7a,irb7b)*IRB7
OCBI =~ c(ocbi1a,ocbi1b)*OCBI1 + c(ocbi2a,ocbi2b)*OCBI2 + c(ocbi3a,ocbi3b)*OCBI3 + c(ocbi4a,ocbi4b)*OCBI4 + c(ocbi5a,ocbi5b)*OCBI5 + c(ocbi6a,ocbi6b)*OCBI6 + c(ocbi7a,ocbi7b)*OCBI7
OCBO =~ c(ocbo1a,ocbo1b)*OCBO1 + c(ocbo2a,ocbo2b)*OCBO2 + c(ocbo3a,ocbo3b)*OCBO3 + c(ocbo4a,ocbo4b)*OCBO4 + c(ocbo5a,ocbo5b)*OCBO5 + c(ocbo6a,ocbo6b)*OCBO6 + c(ocbo7a,ocbo7b)*OCBO7

# Negative Item Content Factor
NW =~ c(nw1a, nw1b)*IRB6 + c(nw2a, nw2b)*IRB7 + c(nw3a, nw3b)*OCBO3 + c(nw4a, nw4b)*OCBO4 + c(nw5a, nw5b)*OCBO5
NW ~~ c(1,1)*NW
NW ~ c(0,0)*1
NW ~~ c(0,0)*IRB
NW ~~ c(0,0)*OCBO
NW ~~ c(0,0)*PP
NW ~~ c(0,0)*OCBI

#Factor variances are fixed to 1 to allow estimation.
###Substantive factors
PP ~~ c(1,1)*PP
IRB ~~ c(1,1)*IRB
OCBI ~~ c(1,1)*OCBI
OCBO ~~ c(1,1)*OCBO

##Factor covariances labels 
PP ~~ c(PPIRB1,PPIRB2)*IRB
PP ~~ c(PPOCBI1,PPOCBI2)*OCBI
PP ~~ c(PPOCBO1,PPOCBO2)*OCBO
IRB ~~ c(IRBOCBI1,IRBOCBI2)*OCBI
IRB ~~ c(IRBOCBO1,IRBOCBO2)*OCBO
OCBI ~~ c(OCBIO1,OCBIO2)*OCBO

#Factor means of both groups are fixed at zero to allow identification.
##Substantive factors
PP ~ c(0,0)*1
IRB ~ c(0,0)*1
OCBI ~ c(0,0)*1
OCBO ~ c(0,0)*1

# Add in "Mood" as a single-item latent construct that is an uncorrelated with all factors.
Mood =~ .95*MOOD + c(mpp1a, mpp1b)*PP1 + c(mpp2a, mpp2b)*PP2 + c(mpp3a,mpp3b)*PP3 + c(mpp4a,mpp4b)*PP4 + c(mpp5a,mpp5b)*PP5 + c(mpp6a,mpp6b)*PP6 + c(mpp7a,mpp7b)*PP7 + c(mpp8a,mpp8b)*PP8 + c(mpp9a,mpp9b)*PP9 + c(mpp10a,mpp10b)*PP10 + c(mirb1a,mirb1b)*IRB1 + c(mirb2a,mirb2b)*IRB2 + c(mirb3a,mirb3b)*IRB3 + c(mirb4a,mirb4b)*IRB4 + c(mirb5a,mirb5b)*IRB5 + c(mirb6a,mirb6b)*IRB6 + c(mirb7a,mirb7b)*IRB7 + c(mocbi1a,mocbi1b)*OCBI1 + c(mocbi2a,mocbi2b)*OCBI2 + c(mocbi3a,mocbi3b)*OCBI3 + c(mocbi4a,mocbi4b)*OCBI4 + c(mocbi5a,mocbi5b)*OCBI5 + c(mocbi6a,mocbi6b)*OCBI6 + c(mocbi7a,mocbi7b)*OCBI7 + c(mocbo1a,mocbo1b)*OCBO1 + c(mocbo2a,mocbo2b)*OCBO2 + c(mocbo3a,mocbo3b)*OCBO3 + c(mocbo4a,mocbo4b)*OCBO4 + c(mocbo5a,mocbo5b)*OCBO5 + c(mocbo6a,mocbo6b)*OCBO6 + c(mocbo7a,mocbo7b)*OCBO7
 # (see Anderson & Gerbing, 1988; Sorbom & Joreskig, 1982; as cited by Petrescu, 2013)
MOOD ~~ (1.593178)*(1-.85)*MOOD

# Add in PA as a single-item latent construct that is an uncorrelated with all factors.
PosAff =~ .95*PA
PA ~~ (0.9241972)*(1-.92)*PA

# Make uncorrelated with NW
NW ~~ 0*PosAff

# Add in causal sructure for PA and Mood
Mood ~ PosAff # PA causes mood

# Mood as a proximal cause of method variance
Mood ~~ 0*PP # Mood uncorrelated with proactive personality
Mood ~~ 0*IRB # Mood uncorrelated with IRB
Mood ~~ 0*OCBI # Mood uncorreated with OCBI
Mood ~~ 0*OCBO # Mood uncorrelated with OCBO
NW ~~ 0*Mood

# Estimate differences in key parameters (factor loadings and covariances). 
# Compute differences in factor loadings
pp1c	:=	pp1a	-	pp1b
pp2c	:=	pp2a	-	pp2b
pp3c	:=	pp3a	-	pp3b
pp4c	:=	pp4a	-	pp4b
pp5c	:=	pp5a	-	pp5b
pp6c	:=	pp6a	-	pp6b
pp7c	:=	pp7a	-	pp7b
pp8c	:=	pp8a	-	pp8b
pp9c	:=	pp9a	-	pp9b
pp10c	:=	pp10a	-	pp10b
irb1c	:=	irb1a	-	irb1b
irb2c	:=	irb2a	-	irb2b
irb3c	:=	irb3a	-	irb3b
irb4c	:=	irb4a	-	irb4b
irb5c	:=	irb5a	-	irb5b
irb6c	:=	irb6a	-	irb6b
irb7c	:=	irb7a	-	irb7b
ocbi1c	:=	ocbi1a	-	ocbi1b
ocbi2c	:=	ocbi2a	-	ocbi2b
ocbi3c	:=	ocbi3a	-	ocbi3b
ocbi4c	:=	ocbi4a	-	ocbi4b
ocbi5c	:=	ocbi5a	-	ocbi5b
ocbi6c	:=	ocbi6a	-	ocbi6b
ocbi7c	:=	ocbi7a	-	ocbi7b
ocbo1c	:=	ocbo1a	-	ocbo1b
ocbo2c	:=	ocbo2a	-	ocbo2b
ocbo3c	:=	ocbo3a	-	ocbo3b
ocbo4c	:=	ocbo4a	-	ocbo4b
ocbo5c	:=	ocbo5a	-	ocbo5b
ocbo6c	:=	ocbo6a	-	ocbo6b
ocbo7c	:=	ocbo7a	-	ocbo7b
nw1c	:=	nw1a	-	nw1b
nw2c	:=	nw2a	-	nw2b
nw3c	:=	nw3a	-	nw3b
nw4c	:=	nw4a	-	nw4b
nw5c	:=	nw5a	-	nw5b

# Compute covariance differences (tests H1)
PPIRB3 := PPIRB1-PPIRB2
PPOCBI3 := PPOCBI1-PPOCBI2
PPOCBO3 := PPOCBO1-PPOCBO2

# Compute average factor loading differences (tests H2)
ppfl.z := (pp1c + pp2c + pp3c + pp4c + pp5c + pp6c + pp7c + pp8c + pp9c + pp10c)/10
irbfl.z := (irb1c + irb2c + irb3c + irb4c + irb5c + irb6c + irb7c)/7
ocbifl.z := (ocbi1c + ocbi2c + ocbi3c + ocbi4c + ocbi5c + ocbi6c + ocbi7c)/7
ocbofl.z := (ocbo1c + ocbo2c + ocbo3c + ocbo4c + ocbo5c + ocbo6c + ocbo7c)/7
nwfl.z := (nw1c + nw2c + nw3c + nw4c + nw5c)/5
'

m1 <- cfa(m1.cfa, data = data1, group = "COND", estimator = "DWLS", parameterization = "theta", information = "expected", std.lv=TRUE,test = "Satorra-Bentler")
summary(m1, standardized = TRUE, fit.measures = TRUE)
mod.m <- modindices(m1, minimum.value = 10, sort = TRUE)
fitm1.m <- fitmeasures(m1)
mod.m <- modindices(m1, minimum.value = 10, sort = TRUE)
fitm1.m <- fitmeasures(m1)

#Gather stats
csq.ms1 <- round(as.numeric(fitm1.m[c(6)]), digits = 2) ##Chi-Square
df.ms1 <- round(as.numeric(fitm1.m[c(7)]), digits = 2) ##df
cdfratio.ms1 <- round(csq.ms1/df.ms1,2) ## Chi-square / df ratio
p.ms1 <- pvalr(as.numeric(fitm1.m[c(8)]), digits = 2) ##Chi-Square p value
cssf.ms1 <- round(as.numeric(fitm1.m[c(9)]), digits = 2) ##Chisq.scaling factor
cfi.ms1 <- round(as.numeric(fitm1.m[c(27)]), digits = 2) #CFI robust
remsea.ms1 <- round(as.numeric(fitm1.m[c(44)]), digits = 3) ##RMSEA robust
remseal.ms1 <- round(as.numeric(fitm1.m[c(45)]), digits = 3) #RMSEA lower robust
remseau.ms1 <- round(as.numeric(fitm1.m[c(46)]), digits = 3) #RMSEA upper robust
srmr.ms1<- round(as.numeric(fitm1.m[c(50)]), digits = 3) #SRMR
```
```{r Study 1: Build Latent Correlation and Reliability Decomposition Tables, include = FALSE}
# Build table for latent covariances
factors <- c("Proactive Personality","")
table <- as.data.frame(cbind(factors,"Condition","In-Role Behavior","","","","OCBI","","","","OCBO","","",""))
colnames(table)[1] <- "Predictor"
colnames(table)[2] <- "Condition"
colnames(table)[3] <- "In-Role Behavior"
colnames(table)[4] <- "In-Role Behaviorp"
colnames(table)[5] <- "In-Role Behaviorz"
colnames(table)[6] <- "In-Role Behaviorzp"
colnames(table)[7] <- "OCBI"
colnames(table)[8] <- "OCBIp"
colnames(table)[9] <- "OCBIz"
colnames(table)[10] <- "OCBIzp"
colnames(table)[11] <- "OCBO"
colnames(table)[12] <- "OCBOp"
colnames(table)[13] <- "OCBOz"
colnames(table)[14] <- "OCBOzp"
table$Condition <- as.character(table$Condition)
table$`In-Role Behavior` <- as.numeric(table$`In-Role Behavior`)
table$`In-Role Behaviorp` <- as.character(table$`In-Role Behavior`)
table$`OCBI` <- as.numeric(table$`OCBI`)
table$OCBIp <- as.character(table$OCBIp)
table$`OCBO` <- as.numeric(table$`OCBO`)
table$OCBOp <- as.character(table$OCBOp)
table$Condition[1] <- "No Remedies"
table$Condition[2] <- "Remedies Bundled"

# Extract pvalues and significance levels for mapping purposes.
data <- parameterEstimates(m1, standardized=TRUE)%>% 
  dplyr::filter(op == "~~") %>%
  dplyr::filter(label == "PPIRB1" | label == "PPOCBI1" | label == "PPOCBO1" | label == "PPIRB2" | label == "PPOCBI2" | label == "PPOCBO2") %>%
  select(label, est, pvalue) %>%
  mutate_if(is.numeric, ~round(., 2)) 
data$p <- stars.pval(data$pvalue)

# Extract the latent covariances loadings, map on significance levels, and place into table.
m1.covariances<- inspect(m1,"std.all")
table$`In-Role Behavior`[1] <- m1.covariances[["1"]][["psi"]][1,2]  # PP-IRBnr
PPIRB1a <- round(table$`In-Role Behavior`[1],2)
table$`In-Role Behaviorp`[1] <- data %>% dplyr::filter(label == "PPIRB2") %>% select(p) 
table$`In-Role Behavior`[2] <- m1.covariances[["0"]][["psi"]][1,2]  # PP-IRBr
PPIRB1b <- round(table$`In-Role Behavior`[2],2)
table$`In-Role Behaviorp`[2] <- data %>% dplyr::filter(label == "PPIRB1") %>% select(p) 
table$OCBI[1] <- m1.covariances[["1"]][["psi"]][1,3]  # PP-OCBInr
PPOCBI1a <- round(table$OCBI[1],2)
table$OCBIp[1] <- data %>% dplyr::filter(label == "PPOCBI2") %>% select(p) 
table$OCBI[2] <- m1.covariances[["0"]][["psi"]][1,3]  # PP-OCBIr
PPOCBI1b <- round(table$OCBI[2], 2)
table$OCBIp[2] <- data %>% dplyr::filter(label == "PPOCBI1") %>% select(p) 
table$OCBO[1] <- m1.covariances[["1"]][["psi"]][1,4]  # PP-OCBOnr
PPOCBO1a <- round(table$OCBO[1],2)
table$OCBOp[1] <- data %>% dplyr::filter(label == "PPOCBO2") %>% select(p) 
table$OCBO[2] <- m1.covariances[["0"]][["psi"]][1,4]  # PP-OCBOr
PPOCBO1b <- round(table$OCBO[2],2)
table$OCBOp[2] <- data %>% dplyr::filter(label == "PPOCBO1") %>% select(p) 

#Extract z-scores and significance levels for mapping purposes.
z <- parameterestimates(m1, standardized = T) %>%
  dplyr::filter(op == ":=") %>%
  dplyr::filter(label == "PPIRB3" | label == "PPOCBI3" | label == "PPOCBO3") %>%
  select(label, z, pvalue) %>%
  mutate_if(is.numeric, ~round(., 3))
PPIRB1z <- z$z[1]
PPIRB1zp <- z$pvalue[1]
PPOCBI1z <- z$z[2]
PPOCBI1zp <- z$pvalue[2] %>% pvalr()
PPOCBO1z <- z$z[3]
PPOCBO1zp <- z$pvalue[3]
z$p <- stars.pval((z$pvalue))
z$z <- paste0("(", format(unlist(z$z)),")")

# Map z scores and pvalue onto table
table$`In-Role Behaviorz` <-  as.character(table$`In-Role Behaviorz` )
table$`In-Role Behaviorz`[2] <-  z$z[1]
table$`In-Role Behaviorzp` <-  as.character(table$`In-Role Behaviorzp` )
table$`In-Role Behaviorzp`[2] <-  z$p[1]
table$`OCBIz` <-  as.character(table$`OCBIz` )
table$`OCBIz`[2] <-  z$z[2]
table$`OCBIzp` <-  as.character(table$`OCBIzp` )
table$`OCBIzp`[2] <-  z$p[2]
table$`OCBOz` <-  as.character(table$`OCBOz` )
table$`OCBOz`[2] <-  z$z[3]
table$`OCBOzp` <-  as.character(table$`OCBOzp` )
table$`OCBOzp`[2] <-  z$p[3]

# round
table <- table %>% 
  mutate_if(is.numeric, ~round(., 2))

# Combine columns
table$`In-Role Behavior` <- paste(table$`In-Role Behavior`,table$`In-Role Behaviorp`,table$`In-Role Behaviorz`,table$`In-Role Behaviorzp`)
table$OCBI <- paste(table$OCBI,table$OCBIp,table$OCBIz,table$OCBIzp)
table$OCBO <- paste(table$OCBO,table$OCBOp,table$OCBOz,table$OCBOzp)

# Name table 2
table2 <- table %>%
  select(-one_of("In-Role Behaviorp","In-Role Behaviorz","In-Role Behaviorzp","OCBIp","OCBIz","OCBIzp","OCBOp","OCBOz","OCBOzp"))

# Build reliability decomposition table 
factors <- c("Proactive Personality","","In-Role Behavior", "","OCBI","", "OCBO", "")
table <- as.data.frame(cbind(factors,"Condition","Total Reliability","Substantive Reliability","Mood Reliability","% Mood Reliability","Negative Item Wording Reliability","% Negative Item Wording Reliability"))
colnames(table)[1] <- "Latent Variable"
colnames(table)[2] <- "Condition"
colnames(table)[3] <- "Total Reliability"
colnames(table)[4] <- "Substantive Reliability"
colnames(table)[5] <- "Mood Reliability"
colnames(table)[6] <- "% Mood Reliability"
colnames(table)[7] <- "Negative Item Wording Reliability"
colnames(table)[8] <- "% Negative Item Wording Reliability"
table$Condition <- as.character(table$Condition)
table$`Total Reliability` <- as.numeric(table$`Total Reliability`)
table$`Substantive Reliability` <- as.numeric(table$`Substantive Reliability`)
table$`Mood Reliability` <- as.numeric(table$`Mood Reliability`)
table$`% Mood Reliability` <- as.numeric(table$`% Mood Reliability`)
table$`Negative Item Wording Reliability` <- as.numeric(table$`Negative Item Wording Reliability`)
table$`% Negative Item Wording Reliability` <- as.numeric(table$`% Negative Item Wording Reliability`)
table$Condition[1] <- "No Remedies"
table$Condition[2] <- "Remedies Bundled"
table$Condition[3] <- "No Remedies"
table$Condition[4] <- "Remedies Bundled"
table$Condition[5] <- "No Remedies"
table$Condition[6] <- "Remedies Bundled"
table$Condition[7] <- "No Remedies"
table$Condition[8] <- "Remedies Bundled"

# Extract the factor loadings
m1.factorloadings <- inspect(m1,"est")

# Focus on the remedied group
PP <- as.data.frame(m1.factorloadings[["0"]][["lambda"]])  %>%
  abs() %>%
  rownames_to_column %>%
  slice(1:10) %>%
  mutate(Sub.Rel = PP^2)%>%
  mutate(Mood.Rel = Mood^2)%>%
  mutate(NW.Rel = NW^2) %>%
  as.data.frame() %>%
  mutate(Error = (1-(Sub.Rel+Mood.Rel+NW.Rel)))

table$`Substantive Reliability`[2] <- (sum(PP$PP)^2)/((sum(PP$PP)^2)+(sum(PP$Mood)^2)+sum(PP$Error))
table$`Mood Reliability`[2] <- (sum(PP$Mood)^2)/((sum(PP$PP)^2)+(sum(PP$Mood)^2)+sum(PP$Error))
table$`Negative Item Wording Reliability`[2] <- (sum(PP$NW)^2)/((sum(PP$PP)^2)+(sum(PP$Mood)^2)+(sum(PP$NW)^2)+sum(PP$Error))
table$`Total Reliability`[2] <- (table$`Substantive Reliability`[2]+table$`Mood Reliability`[2]+table$`Negative Item Wording Reliability`[2] )
table$`% Mood Reliability`[2] <- (table$`Mood Reliability`[2]/table$`Total Reliability`[2])
table$`% Negative Item Wording Reliability`[2] <- (table$`Negative Item Wording Reliability`[2]/table$`Total Reliability`[2])

IRB <- as.data.frame(m1.factorloadings[["0"]][["lambda"]])  %>%
  abs() %>%
  rownames_to_column %>%
  slice(11:17) %>%
  mutate(Sub.Rel = IRB^2) %>%
  mutate(Mood.Rel = Mood^2) %>%
  mutate(NW.Rel = NW^2) %>%
  as.data.frame() %>%
  mutate(Error = (1-(Sub.Rel+Mood.Rel+NW.Rel)))

table$`Substantive Reliability`[4] <- (sum(IRB$IRB)^2)/((sum(IRB$IRB)^2)+(sum(IRB$Mood)^2)+(sum(IRB$NW)^2)+sum(IRB$Error))
table$`Mood Reliability`[4] <- (sum(IRB$Mood)^2)/((sum(IRB$IRB)^2)+(sum(IRB$Mood)^2)+(sum(IRB$NW)^2)+sum(IRB$Error))
table$`Negative Item Wording Reliability`[4] <- (sum(IRB$NW)^2)/((sum(IRB$IRB)^2)+(sum(IRB$Mood)^2)+(sum(IRB$NW)^2)+sum(IRB$Error))
table$`Total Reliability`[4] <- (table$`Substantive Reliability`[4]+table$`Mood Reliability`[4]+table$`Negative Item Wording Reliability`[4] )
table$`% Mood Reliability`[4] <- (table$`Mood Reliability`[4]/table$`Total Reliability`[4])
table$`% Negative Item Wording Reliability`[4] <- (table$`Negative Item Wording Reliability`[4]/table$`Total Reliability`[4])

OCBI <- as.data.frame(m1.factorloadings[["0"]][["lambda"]])  %>%
  abs() %>%
  rownames_to_column %>%
  slice(18:24) %>%
  mutate(Sub.Rel = OCBI^2) %>%
  mutate(Mood.Rel = Mood^2) %>%
  mutate(NW.Rel = NW^2) %>%
  as.data.frame() %>%
  mutate(Error = (1-(Sub.Rel+Mood.Rel+NW.Rel)))

table$`Substantive Reliability`[6] <- (sum(OCBI$OCBI)^2)/((sum(OCBI$OCBI)^2)+(sum(OCBI$Mood)^2)+(sum(OCBI$NW)^2)+sum(OCBI$Error))
table$`Mood Reliability`[6] <- (sum(OCBI$Mood)^2)/((sum(OCBI$OCBI)^2)+(sum(OCBI$Mood)^2)+(sum(OCBI$NW)^2)+sum(OCBI$Error))
table$`Negative Item Wording Reliability`[6] <- (sum(OCBI$NW)^2)/((sum(OCBI$OCBI)^2)+(sum(OCBI$Mood)^2)+(sum(OCBI$NW)^2)+sum(OCBI$Error))
table$`Total Reliability`[6] <- (table$`Substantive Reliability`[6]+table$`Mood Reliability`[6]+table$`Negative Item Wording Reliability`[6])
table$`% Mood Reliability`[6] <- (table$`Mood Reliability`[6]/table$`Total Reliability`[6])
table$`% Negative Item Wording Reliability`[6] <- (table$`Negative Item Wording Reliability`[6]/table$`Total Reliability`[6])

OCBO <- as.data.frame(m1.factorloadings[["0"]][["lambda"]])  %>%
  abs() %>%
  rownames_to_column %>%
  slice(25:32) %>%
  mutate(Sub.Rel = OCBO^2) %>%
  mutate(Mood.Rel = Mood^2) %>%
  mutate(NW.Rel = NW^2) %>%
  as.data.frame() %>%
  mutate(Error = (1-(Sub.Rel+Mood.Rel+NW.Rel)))

table$`Substantive Reliability`[8] <- (sum(OCBO$OCBO)^2)/((sum(OCBO$OCBO)^2)+(sum(OCBO$Mood)^2)+(sum(OCBO$NW)^2)+sum(OCBO$Error))
table$`Mood Reliability`[8] <- (sum(OCBO$Mood)^2)/((sum(OCBO$OCBO)^2)+(sum(OCBO$Mood)^2)+(sum(OCBO$NW)^2)+sum(OCBO$Error))
table$`Negative Item Wording Reliability`[8] <- (sum(OCBO$NW)^2)/((sum(OCBO$OCBO)^2)+(sum(OCBO$Mood)^2)+(sum(OCBO$NW)^2)+sum(OCBO$Error))
table$`Total Reliability`[8] <- (table$`Substantive Reliability`[8]+table$`Mood Reliability`[8]+table$`Negative Item Wording Reliability`[8])
table$`% Mood Reliability`[8] <- (table$`Mood Reliability`[8]/table$`Total Reliability`[8])
table$`% Negative Item Wording Reliability`[8] <- (table$`Negative Item Wording Reliability`[8]/table$`Total Reliability`[8])

# Focus on non-remedied
PP <- as.data.frame(m1.factorloadings[["1"]][["lambda"]])  %>%
  abs() %>%
  rownames_to_column %>%
  slice(1:10) %>%
  mutate(Sub.Rel = PP^2)%>%
  mutate(Mood.Rel = Mood^2)%>%
  mutate(NW.Rel = NW^2) %>%
  as.data.frame() %>%
  mutate(Error = (1-(Sub.Rel+Mood.Rel+NW.Rel)))

table$`Substantive Reliability`[1] <- (sum(PP$PP)^2)/((sum(PP$PP)^2)+(sum(PP$Mood)^2)+sum(PP$Error))
table$`Mood Reliability`[1] <- (sum(PP$Mood)^2)/((sum(PP$PP)^2)+(sum(PP$Mood)^2)+sum(PP$Error))
table$`Negative Item Wording Reliability`[1] <- (sum(PP$NW)^2)/((sum(PP$PP)^2)+(sum(PP$Mood)^2)+(sum(PP$NW)^2)+sum(PP$Error))
table$`Total Reliability`[1] <- (table$`Substantive Reliability`[1]+table$`Mood Reliability`[1]+table$`Negative Item Wording Reliability`[1])
table$`% Mood Reliability`[1] <- (table$`Mood Reliability`[1]/table$`Total Reliability`[1])
table$`% Negative Item Wording Reliability`[1] <- (table$`Negative Item Wording Reliability`[1]/table$`Total Reliability`[1])

IRB <- as.data.frame(m1.factorloadings[["1"]][["lambda"]])  %>%
  abs() %>%
  rownames_to_column %>%
  slice(11:17) %>%
  mutate(Sub.Rel = IRB^2) %>%
  mutate(Mood.Rel = Mood^2) %>%
  mutate(NW.Rel = NW^2) %>%
  as.data.frame() %>%
  mutate(Error = (1-(Sub.Rel+Mood.Rel+NW.Rel)))

table$`Substantive Reliability`[3] <- (sum(IRB$IRB)^2)/((sum(IRB$IRB)^2)+(sum(IRB$Mood)^2)+(sum(IRB$NW)^2)+sum(IRB$Error))
table$`Mood Reliability`[3] <- (sum(IRB$Mood)^2)/((sum(IRB$IRB)^2)+(sum(IRB$Mood)^2)+(sum(IRB$NW)^2)+sum(IRB$Error))
table$`Negative Item Wording Reliability`[3] <- (sum(IRB$NW)^2)/((sum(IRB$IRB)^2)+(sum(IRB$Mood)^2)+(sum(IRB$NW)^2)+sum(IRB$Error))
table$`Total Reliability`[3] <- (table$`Substantive Reliability`[3]+table$`Mood Reliability`[3]+table$`Negative Item Wording Reliability`[3])
table$`% Mood Reliability`[3] <- (table$`Mood Reliability`[3]/table$`Total Reliability`[3])
table$`% Negative Item Wording Reliability`[3] <- (table$`Negative Item Wording Reliability`[3]/table$`Total Reliability`[3])

OCBI <- as.data.frame(m1.factorloadings[["1"]][["lambda"]])  %>%
  abs() %>%
  rownames_to_column %>%
  slice(18:24) %>%
  mutate(Sub.Rel = OCBI^2) %>%
  mutate(Mood.Rel = Mood^2) %>%
  mutate(NW.Rel = NW^2) %>%
  as.data.frame() %>%
  mutate(Error = (1-(Sub.Rel+Mood.Rel+NW.Rel)))

table$`Substantive Reliability`[5] <- (sum(OCBI$OCBI)^2)/((sum(OCBI$OCBI)^2)+(sum(OCBI$Mood)^2)+(sum(OCBI$NW)^2)+sum(OCBI$Error))
table$`Mood Reliability`[5] <- (sum(OCBI$Mood)^2)/((sum(OCBI$OCBI)^2)+(sum(OCBI$Mood)^2)+(sum(OCBI$NW)^2)+sum(OCBI$Error))
table$`Negative Item Wording Reliability`[5] <- (sum(OCBI$NW)^2)/((sum(OCBI$OCBI)^2)+(sum(OCBI$Mood)^2)+(sum(OCBI$NW)^2)+sum(OCBI$Error))
table$`Total Reliability`[5] <- (table$`Substantive Reliability`[5]+table$`Mood Reliability`[5]+table$`Negative Item Wording Reliability`[5])
table$`% Mood Reliability`[5] <- (table$`Mood Reliability`[5]/table$`Total Reliability`[5])
table$`% Negative Item Wording Reliability`[5] <- (table$`Negative Item Wording Reliability`[5]/table$`Total Reliability`[5])

OCBO <- as.data.frame(m1.factorloadings[["1"]][["lambda"]])  %>%
  abs() %>%
  rownames_to_column %>%
  slice(25:32) %>%
  mutate(Sub.Rel = OCBO^2) %>%
  mutate(Mood.Rel = Mood^2) %>%
  mutate(NW.Rel = NW^2) %>%
  as.data.frame() %>%
  mutate(Error = (1-(Sub.Rel+Mood.Rel+NW.Rel)))

table$`Substantive Reliability`[7] <- (sum(OCBO$OCBO)^2)/((sum(OCBO$OCBO)^2)+(sum(OCBO$Mood)^2)+(sum(OCBO$NW)^2)+sum(OCBO$Error))
table$`Mood Reliability`[7] <- (sum(OCBO$Mood)^2)/((sum(OCBO$OCBO)^2)+(sum(OCBO$Mood)^2)+(sum(OCBO$NW)^2)+sum(OCBO$Error))
table$`Negative Item Wording Reliability`[7] <- (sum(OCBO$NW)^2)/((sum(OCBO$OCBO)^2)+(sum(OCBO$Mood)^2)+(sum(OCBO$NW)^2)+sum(OCBO$Error))
table$`Total Reliability`[7] <- (table$`Substantive Reliability`[7]+table$`Mood Reliability`[7]+table$`Negative Item Wording Reliability`[7])
table$`% Mood Reliability`[7] <- (table$`Mood Reliability`[7]/table$`Total Reliability`[7])
table$`% Negative Item Wording Reliability`[7] <- (table$`Negative Item Wording Reliability`[7]/table$`Total Reliability`[7])

# Round columsn to nearest 100th
table <- table %>% 
  mutate_if(is.numeric, ~round(., 2))

# Call table 3
table3 <- table

# Extract parameter estimates
pe <- parameterestimates(m1)
## PP
pp1z <- pe %>% 
  dplyr::filter(label == "ppfl.z") %>%
  select(z)
pp1z <- round(pp1z,3)
pp1z <- as.numeric(pp1z[1])
pp1p <- pe %>% 
  dplyr::filter(label == "ppfl.z") %>%
  select(pvalue)
pp1p <- round(pp1p,3)
pp1p <- as.numeric(pp1p[1])

## IRB
irb1z <- pe %>% 
  dplyr::filter(label == "irbfl.z") %>%
  select(z)
irb1z <- round(irb1z,3)
irb1z <- as.numeric(irb1z[1])
irb1p <- pe %>% 
  dplyr::filter(label == "irbfl.z") %>%
  select(pvalue)
irb1p <- round(irb1p,3)
irb1p <- as.numeric(irb1p[1])

## OCBI
ocbi1z <- pe %>% 
  dplyr::filter(label == "ocbifl.z") %>%
  select(z)
ocbi1z <- round(ocbi1z,3)
ocbi1z <- as.numeric(ocbi1z[1])
ocbi1p <- pe %>% 
  dplyr::filter(label == "ocbifl.z") %>%
  select(pvalue)
ocbi1p <- round(ocbi1p,3)
ocbi1p <- as.numeric(ocbi1p[1])

## OCBO
ocbo1z <- pe %>% 
  dplyr::filter(label == "ocbofl.z") %>%
  select(z)
ocbo1z <- round(ocbo1z,3)
ocbo1z <- as.numeric(ocbo1z[1])
ocbo1p <- pe %>% 
  dplyr::filter(label == "ocbofl.z") %>%
  select(pvalue)
ocbo1p <- round(ocbo1p,3)
ocbo1p <- as.numeric(ocbo1p[1])
```
```{r Table 1, results = 'asis', tab.cap = NULL, echo = FALSE, include = FALSE}
table2 %>%
  knitr::kable(format = 'html',booktabs=T ,escape = T) %>%
  kable_styling(latex_options = c(
   # "scale_down",
    "threeparttable",
    "hold_position"
    ),
                 repeat_header_method = c("replace")) %>%
  kableExtra::footnote(general = "* p < .05; ** p < .01; *** p < .001. ",
  "Values in parentheses are z-scores that describe the difference between the estimated correlations.") %>%
  landscape() %>%
  kable_styling(full_width = F) %>%
  save_kable("table1.html")
```
```{r Table 2, results = 'asis', tab.cap = NULL, echo = TRUE, include = FALSE}
table3 %>%
  knitr::kable(format = 'html', booktabs=T ,escape = T) %>%
  kable_styling(latex_options = c("scale_down", "hold_position"),
                 repeat_header_method = c("replace")) %>%
  kableExtra::footnote(general = "Total reliability is a sum of the substantive and method reliability components (see Williams et al., 2010). ") %>%
  landscape() %>%
  kable_styling(full_width = T) %>%
  save_kable("Table2.html")
```

Our proposed model had a close fit ($\chi^2$~(`r df.ms1`)~ = `r csq.ms1`, *p* `r p.ms1`, $\chi^2$/df = `r cdfratio.ms1`, CFI = `r cfi.ms1`, RMSEA = `r remsea.ms1`, SRMR = `r srmr.ms1`) and so we moved to interpreting the results. Table 1 contains the estimates for the latent construct correlations. A clear trend emerged. Specifically, all of the latent construct correlations were larger under the non-remedied condition relative to the remedied condition. The PP–IRB correlation was smaller in the remedied condition compared to the non-remedied condition (Φs: `r PPIRB1b` vs. `r PPIRB1a`, z = `r PPIRB1z`, *p* = `r PPIRB1zp`). The same was also observed for the PP–OCBI correlation (Φs: `r PPOCBI1b` vs. `r PPOCBI1a`,  z = `r PPOCBI1z`, *p* `r PPOCBI1zp`). However, the PP–OCBO correlation, though descriptively smaller and in the direction of our expectations, was not statistically significantly different (Φs: `r PPOCBO1b` vs. `r PPOCBO1a`,  z = `r PPOCBO1z`, *p* `r PPOCBO1zp`). Overall, these findings provide some support for hypothesis 1.

# - - - - - - - - - - - 
# Insert Tables 1 and 2 about here
# - - - - - - - - - - - 

Table 2 contains the reliability decomposition for our key study variables. The substantive reliability estimates were all smaller in the remedied condition compared to the non-remedied condition (indicating that proximal causes of method variance inflated the estimate of substantive variance in the non-remedied condition), which is consistent with hypothesis 2. However, we only observed statistically significant differences between the factor loadings for the IRB (z = `r irb1z`, *p* = `r irb1p`) and OCBO (z = `r ocbo1z`, *p* = `r ocbo1p`) measures (i.e., the substantive standardized factor loadings were weaker for these models by ~.11), providing some support for hypothesis 2. Coincidentally, these effects emerged for measures that were intermixed and contained both positive and negatively worded items. The average difference in substantive factor loadings across conditions was not statistically significant for PP (z = `r pp1z`, *p* = `r pp1p`) and OCBI (z = `r ocbi1z`, *p* = `r round(ocbi1p,2)`). Overall, this provides mixed evidence for hypothesis 2.

# Discussion Study 1

Study 1 provided evidence for the efficacy of bundling proximal remedies to address proximal causes of method variance. We found that when proximal remedies were bundled, the estimated inter-construct correlations were all weakened. Crucially, these reductions were dramatic in that all correlations were descriptively weakened (two reductions were statistically significant). Also, the correlations linking PP to IRB and OCBO rendered statistically non-significant. Specific to our study variables, our findings suggest that the correlations linking proactive personality to in-role behavior and OCB (as estimated by same-source designs that do not contain bundled proximal remedies) can be partly explained by proximal causes of method variance (e.g., measurement context and mood effects) when a  single-source self-report design is used. In other words, for single-source self-report designs that do not rely on bundled proximal remedies, proximal causes of method variance could explain the correlation linking proactive personality and performance outcomes (i.e., in-role behavior, OCBI, OCBO). 

In regard to measure reliability, we find that substantive reliability was reduced for all measures in our bundled remedy condition, which was consistent with our expectations. This suggests that the estimates from our non-remedied condition were inflated by method effects. However, looking for differences in the factor loadings reveals a slightly different picture. We found that when remedies were bundled, only inventories that contained reverse- or negatively-keyed items had slightly weaker substantive factor loadings. To help make sense of this pattern, it should be kept in mind that the substantive reliability estimate are a ratio of the trait variance (as estimated by substantive factor loadings) to all sources of modeled variance (trait, measured method, and error) reflected by the corresponding measurement models [@WilliamsMethodVarianceMarker2010]. For measures that are modeled as reflecting only two sources of variance (substantive and error), the interpretation is relatively straightforward: remedies that address method variance that is shared among an item set (e.g., a serial-item order effect) should result in weaker factor loadings and weaker estimates of substantive reliability. Correspondingly, higher levels of error variance should be observed. Unfortunately, for measures that are contaminated by multiple method effects (e.g., negative item wording), the interpretation thereof is more complicated. In Study 1, all measures were contaminated by mood to varying degrees across both conditions. Additionally, IRB and OCBO measures were contaminated by negative item wording. Furthermore, in the non-remedied condition, other method effects were at play (e.g., measurement context effects), which complicates interpretation. Though we consistently see weaker substantive reliability estimates for all measures in our bundled remedy condition, we only see evidence that bundling remedies *weakens* substantive factor loadings for inventories containing negatively worded items (i.e., IRB and OCBO), suggesting that method effects may allow inventories with a mix of positive and negatively worded items to hang together. Conversely, bundling remedies may reveal substantive reliability issues for a measure containing a mix of positive and negative worded items.

Overall, Study 1 provided evidence that bundling proximal remedies for method variance results in correlations that are weaker than when remedies are not bundled together. Additionally, the reliability of the measures may be unaffected unless inventories contain negatively-worded items and are intermixed, in which case substantive reliability may be somewhat weakened.

# Study 2 - Efficacy of Individual Proximal Remedies

With study 2, we sought to isolate the effects of the specific remedies used in Study 1 by running an experiment where participants were randomly assigned to a single proximal remedy for method variance, which allowed us to examine the effects of specific remedies isolated from the effects of other remedies. In other words, individuals were assigned to one of five conditions: (i) a condition where a cover story was used to disguise the purpose of the study, (ii) a condition where the item-order within scales was randomized, (ii) a condition where the scale-order was randomized, (iv) a condition where filler scales where included in-between measures of the predictor and outcomes, or (v) a condition whereby no remedies were applied.

As stated earlier, each remedy should address a specific cause of common method variance [@PodsakoffCommonmethodbiases2003; @PodsakoffSourcesMethodBias2012] and, therefore, produce estimates of correlation that are weaker relative to a condition where no remedy has been applied (i.e., our control or no-remedy condition). Cover stories should affect beliefs regarding the purpose of a study. Akin to a diffusion of treatment effect, cover stories should blind individuals from the purpose of the study, thus preventing them from responding in ways that are desired by the researchers (i.e., a demand effect). Assigning participants to a condition where they receive a unique order of items within a scale should reduce the prevalence of any serial item-order effects across the sample. A similar logic applies to scale-order which, when randomized within a sample, should reduce the prevalence of any serial scale-order effects. Using filler scales, by increasing the distance between assessments of predictor and outcomes, should make it more difficult for individuals to recall responses to previous measures and thereby allow for a less contaminated assessment. 

# Methods - Study 2

## Sample, Procedure, and Analytical Approach
```{r Load Study 2 Data, message = FALSE, warning = FALSE, include = FALSE}
data <- read_sav("Data/Study 2.sav")
N <- as.numeric(nrow(data))

#Count of individuals who agree to participate.
N1 <- as.numeric(freq_vect(data$IC1)[1,"Count"])  #Control
N2 <- as.numeric(freq_vect(data$IC2)[1,"Count"])  #Coverstory
N3 <- as.numeric(freq_vect(data$IC3)[1,"Count"])  #Randomized Scales
N4 <- as.numeric(freq_vect(data$FOR4)[1,"Count"]) #Randomized Items Within Scales
N5 <- as.numeric(freq_vect(data$FOR5)[1,"Count"]) #Filler Scales Used
Nall <- sum(N1,N2,N3,N4,N5)

#Recode consent and manipulation check so that only those who passed have their data examined. 
data$IC1[data$IC1 =="I Agree"] <- 1
data$IC2[data$IC2 =="I Agree"] <- 1
data$IC3[data$IC3 =="I Agree"] <- 1
data$FOR4[data$FOR4 =="I Agree"] <- 1
data$FOR5[data$FOR5 =="I Agree"] <- 1

#Subset in those individuals who paid attention and understood the purpose of the survey as it was presented to them.
Likert <- subset(data, IC1 == "1" & Dupe1 == "1" | IC2 == "1" & Dupe1 == "2" | IC3 == "1" & Dupe1 == "1" | FOR4 == "1" & Dupe1 == "1" |  FOR5 == "1" & Dupe1 == "1")

#Delete cases who did not complete the demographic questionnaire.
Likert <- Likert[complete.cases(Likert[ , 376:380]),]

#Get success rates (SR) for responding correctly across both conditions.
n1 <- as.numeric(freq_vect(Likert$IC1)[1,"Count"])  #Control
n2 <- as.numeric(freq_vect(Likert$IC2)[1,"Count"])  #Coverstory
n3 <- as.numeric(freq_vect(Likert$IC3)[1,"Count"])  #Randomized Scales
n4 <- as.numeric(freq_vect(Likert$FOR4)[1,"Count"]) #Randomized Items Within Scales
n5 <- as.numeric(freq_vect(Likert$FOR5)[1,"Count"]) #Filler Scales Used
n <- n1+n2+n2+n4+n5
sr1 <- round(n1/N1,2)
sr2 <- round(n2/N2,2)
sr3 <- round(n3/N3,2)
sr4 <- round(n4/N4,2)
sr5 <- round(n5/N5,2)
srall <- round(n/(N1+N2+N3+N4+N5),2)

##Collapse like data across columns.
###Conditions; build independent data sets and then rename the condition variable.
###1 Control
Likert$IC1[is.na(Likert$IC1)] <- 0
Likert$IC1 <- as.numeric(Likert$IC1)
###2 Cover Story Manipulation
Likert$IC2 <- ifelse(Likert$IC2 == 1, c("2"))
Likert$IC2[is.na(Likert$IC2)] <- 0
Likert$IC2 <- as.numeric(Likert$IC2)
###3 Randomized Scales
Likert$IC3 <- ifelse(Likert$IC3 == 1, c("3"))
Likert$IC3[is.na(Likert$IC3)] <- 0
Likert$IC3 <- as.numeric(Likert$IC3)
###4 Randomized Items
Likert$FOR4 <- ifelse(Likert$FOR4 == 1, c("4"))
Likert$FOR4[is.na(Likert$FOR4)] <- 0
Likert$FOR4 <- as.numeric(Likert$FOR4)
###5 Filler Scales
Likert$FOR5 <- ifelse(Likert$FOR5 == 1, c("5"))
Likert$FOR5[is.na(Likert$FOR5)] <- 0
Likert$FOR5 <- as.numeric(Likert$FOR5)
#Combine condition Likert into single variable column
Likert$COND <- Likert$IC1 + Likert$IC2+ Likert$IC3 + Likert$FOR4 + Likert$FOR5
#Apply value lables. 
Likert$COND <- factor(Likert$COND, levels = c(1,2,3,4,5), labels = c("Control", "CSManipulation","ScaleRand","ItemRand","FillerScales"))

#Consolidate item response measures
##Form Likert datasets, re-lable values, convert to numeric data, consolidate measures, and delete data.
agree <- Likert[c(21:60,89:138,157:196,225:236,262:289,293:332)]
#agree <- ifelse(agree == "Strongly Disagree", 1, ifelse(agree == "Disagree", 2, ifelse(agree == "Neither Agree nor Disagree", 3, ifelse(agree == "Agree", 4, ifelse(agree == "Strongly Agree", 5,0)))))
agree <- as.data.frame(agree)
agree[is.na(agree)] <- 0

#####Proactive Personality 
agree$PP1	<-	agree$PP1_1	+	agree$PP2_1	+	agree$PP5_1	+	agree$PP6_1	+	agree$PPx_1
agree$PP2	<-	agree$PP1_2	+	agree$PP2_2	+	agree$PP5_2	+	agree$PP6_2	+	agree$PPx_2
agree$PP3	<-	agree$PP1_3	+	agree$PP2_3	+	agree$PP5_3	+	agree$PP6_3	+	agree$PPx_3
agree$PP4	<-	agree$PP1_4	+	agree$PP2_4	+	agree$PP5_4	+	agree$PP6_4	+	agree$PPx_4
agree$PP5	<-	agree$PP1_5	+	agree$PP2_5	+	agree$PP5_5	+	agree$PP6_5	+	agree$PPx_5
agree$PP6	<-	agree$PP1_6	+	agree$PP2_6	+	agree$PP5_6	+	agree$PP6_6	+	agree$PPx_6
agree$PP7	<-	agree$PP1_7	+	agree$PP2_7	+	agree$PP5_7	+	agree$PP6_7	+	agree$PPx_7
agree$PP8	<-	agree$PP1_8	+	agree$PP2_8	+	agree$PP5_8	+	agree$PP6_8	+	agree$PPx_8
agree$PP9	<-	agree$PP1_9	+	agree$PP2_9	+	agree$PP5_9	+	agree$PP6_9	+	agree$PPx_9
agree$PP10	<-	agree$PP1_10	+	agree$PP2_10	+	agree$PP5_10	+	agree$PP6_10	+	agree$PPx_10

#####OCBI
agree$OCBI1	<-	agree$OCBI1_1	+	agree$OCBI2_1	+	agree$OCBI5_1	+	agree$OCBI6_1	+	agree$OCBIx_1
agree$OCBI2	<-	agree$OCBI1_3	+	agree$OCBI2_3	+	agree$OCBI5_3	+	agree$OCBI6_3	+	agree$OCBIx_3
agree$OCBI3	<-	agree$OCBI1_4	+	agree$OCBI2_4	+	agree$OCBI5_4	+	agree$OCBI6_4	+	agree$OCBIx_4
agree$OCBI4	<-	agree$OCBI1_5	+	agree$OCBI2_5	+	agree$OCBI5_5	+	agree$OCBI6_5	+	agree$OCBIx_5
agree$OCBI5	<-	agree$OCBI1_6	+	agree$OCBI2_6	+	agree$OCBI5_6	+	agree$OCBI6_6	+	agree$OCBIx_6
agree$OCBI6	<-	agree$OCBI1_7	+	agree$OCBI2_7	+	agree$OCBI5_7	+	agree$OCBI6_7	+	agree$OCBIx_7
agree$OCBI7	<-	agree$OCBI1_8	+	agree$OCBI2_8	+	agree$OCBI5_8	+	agree$OCBI6_8	+	agree$OCBIx_8

#####OCBO
agree$OCBO1	<-	agree$OCBO1_1	+	agree$OCBO2_1	+	agree$OCBO5_1	+	agree$OCBO6_1	+	agree$OCBOx_1
agree$OCBO2	<-	agree$OCBO1_2	+	agree$OCBO2_2	+	agree$OCBO5_2	+	agree$OCBO6_2	+	agree$OCBOx_2
agree$OCBO3	<-	agree$OCBO1_3	+	agree$OCBO2_3	+	agree$OCBO5_3	+	agree$OCBO6_3	+	agree$OCBOx_3
agree$OCBO4	<-	agree$OCBO1_5	+	agree$OCBO2_5	+	agree$OCBO5_5	+	agree$OCBO6_5	+	agree$OCBOx_5
agree$OCBO5	<-	agree$OCBO1_6	+	agree$OCBO2_6	+	agree$OCBO5_6	+	agree$OCBO6_6	+	agree$OCBOx_6
agree$OCBO6	<-	agree$OCBO1_7	+	agree$OCBO2_7	+	agree$OCBO5_7	+	agree$OCBO6_7	+	agree$OCBOx_7
agree$OCBO7	<-	agree$OCBO1_8	+	agree$OCBO2_8	+	agree$OCBO5_8	+	agree$OCBO6_8	+	agree$OCBOx_8

#####IRB
agree$IRB1	<-	agree$IRB1_1	+	agree$IRB2_1	+	agree$IRB5_1	+	agree$IRB6_1	+	agree$IRBx_1
agree$IRB2	<-	agree$IRB1_3	+	agree$IRB2_3	+	agree$IRB5_3	+	agree$IRB6_3	+	agree$IRBx_3
agree$IRB3	<-	agree$IRB1_4	+	agree$IRB2_4	+	agree$IRB5_4	+	agree$IRB6_4	+	agree$IRBx_4
agree$IRB4	<-	agree$IRB1_5	+	agree$IRB2_5	+	agree$IRB5_5	+	agree$IRB6_5	+	agree$IRBx_5
agree$IRB5	<-	agree$IRB1_6	+	agree$IRB2_6	+	agree$IRB5_6	+	agree$IRB6_6	+	agree$IRBx_6
agree$IRB6	<-	agree$IRB1_8	+	agree$IRB2_8	+	agree$IRB5_8	+	agree$IRB6_8	+	agree$IRBx_8
agree$IRB7	<-	agree$IRB1_9	+	agree$IRB2_9	+	agree$IRB5_9	+	agree$IRB6_9	+	agree$IRBx_9

#####Inattentive Responding
agree$IR	<-	agree$PP1_11	+	agree$PP2_11	+	agree$PP5_11	+	agree$PP6_11	+	agree$PPx_11

#####Consistency Motif
agree$CM1	<-	agree$PP1_12	+	agree$PP2_12	+	agree$PP5_12	+	agree$PP6_12	+	agree$PPx_12 #Brave
agree$CM2	<-	agree$IRB1_2	+	agree$IRB2_2	+	agree$IRB5_2	+	agree$IRB6_2	+	agree$IRBx_2 #Courageous
agree$CM3	<-	agree$OCBI1_9	+	agree$OCBI2_9	+	agree$OCBI5_9	+	agree$OCBI6_9	+	agree$OCBIx_9 #Talkative
agree$CM4	<-	agree$OCBO1_9	+	agree$OCBO2_9	+	agree$OCBO5_9	+	agree$OCBO6_9	+	agree$OCBOx_9 #Silent Person
agree$CM5	<-	agree$IRB1_7	+	agree$IRB2_7	+	agree$IRB5_7	+	agree$IRB6_7	+	agree$IRBx_7 #Optimistic
agree$CM6	<-	agree$OCBO1_4	+	agree$OCBO2_4	+	agree$OCBO5_4	+	agree$OCBO6_4	+	agree$OCBOx_4 #Pessimistic Person
agree$CM7	<-	agree$IRB1_10	+	agree$IRB2_10	+	agree$IRB5_10	+	agree$IRB6_10	+	agree$IRBx_10 #Seldom feel blue.
agree$CM8	<-	agree$OCBI1_2	+	agree$OCBI2_2	+	agree$OCBI5_2	+	agree$OCBI6_2	+	agree$OCBIx_2 #Often feel blue.

#Consolidate agree data
agree <- agree[c(211:250)]

#PANAS
PANAS <- Likert[c(65:84,133:152,201:220,241:260,337:356)]
#PANAS <- ifelse(PANAS == "Very slightly or not at all", 1, ifelse(PANAS == "A little", 2, ifelse(PANAS == "Moderately", 3, ifelse(PANAS == "Quite a bit", 4, ifelse(PANAS == "Extremely", 5, 0)))))
PANAS <- as.data.frame(PANAS)
PANAS[is.na(PANAS)] <- 0
PANAS$PA1	<-	PANAS$PAff1_1	+	PANAS$PAff2_1	+	PANAS$PAff5_1	+	PANAS$PAff6_1	+	PANAS$PAffx_1
PANAS$PA2	<-	PANAS$PAff1_2	+	PANAS$PAff2_2	+	PANAS$PAff5_2	+	PANAS$PAff6_2	+	PANAS$PAffx_2
PANAS$PA3	<-	PANAS$PAff1_3	+	PANAS$PAff2_3	+	PANAS$PAff5_3	+	PANAS$PAff6_3	+	PANAS$PAffx_3
PANAS$PA4	<-	PANAS$PAff1_4	+	PANAS$PAff2_4	+	PANAS$PAff5_4	+	PANAS$PAff6_4	+	PANAS$PAffx_4
PANAS$PA5	<-	PANAS$PAff1_5	+	PANAS$PAff2_5	+	PANAS$PAff5_5	+	PANAS$PAff6_5	+	PANAS$PAffx_5
PANAS$PA6	<-	PANAS$PAff1_6	+	PANAS$PAff2_6	+	PANAS$PAff5_6	+	PANAS$PAff6_6	+	PANAS$PAffx_6
PANAS$PA7	<-	PANAS$PAff1_7	+	PANAS$PAff2_7	+	PANAS$PAff5_7	+	PANAS$PAff6_7	+	PANAS$PAffx_7
PANAS$PA8	<-	PANAS$PAff1_8	+	PANAS$PAff2_8	+	PANAS$PAff5_8	+	PANAS$PAff6_8	+	PANAS$PAffx_8
PANAS$PA9	<-	PANAS$PAff1_9	+	PANAS$PAff2_9	+	PANAS$PAff5_9	+	PANAS$PAff6_9	+	PANAS$PAffx_9
PANAS$PA10	<-	PANAS$PAff1_10	+	PANAS$PAff2_10	+	PANAS$PAff5_10	+	PANAS$PAff6_10	+	PANAS$PAffx_10
PANAS$NA1	<-	PANAS$NAff1_1	+	PANAS$NAff2_1	+	PANAS$NAff5_1	+	PANAS$NAff6_1	+	PANAS$NAffx_1
PANAS$NA2	<-	PANAS$NAff1_2	+	PANAS$NAff2_2	+	PANAS$NAff5_2	+	PANAS$NAff6_2	+	PANAS$NAffx_2
PANAS$NA3	<-	PANAS$NAff1_3	+	PANAS$NAff2_3	+	PANAS$NAff5_3	+	PANAS$NAff6_3	+	PANAS$NAffx_3
PANAS$NA4	<-	PANAS$NAff1_4	+	PANAS$NAff2_4	+	PANAS$NAff5_4	+	PANAS$NAff6_4	+	PANAS$NAffx_4
PANAS$NA5	<-	PANAS$NAff1_5	+	PANAS$NAff2_5	+	PANAS$NAff5_5	+	PANAS$NAff6_5	+	PANAS$NAffx_5
PANAS$NA6	<-	PANAS$NAff1_6	+	PANAS$NAff2_6	+	PANAS$NAff5_6	+	PANAS$NAff6_6	+	PANAS$NAffx_6
PANAS$NA7	<-	PANAS$NAff1_7	+	PANAS$NAff2_7	+	PANAS$NAff5_7	+	PANAS$NAff6_7	+	PANAS$NAffx_7
PANAS$NA8	<-	PANAS$NAff1_8	+	PANAS$NAff2_8	+	PANAS$NAff5_8	+	PANAS$NAff6_8	+	PANAS$NAffx_8
PANAS$NA9	<-	PANAS$NAff1_9	+	PANAS$NAff2_9	+	PANAS$NAff5_9	+	PANAS$NAff6_9	+	PANAS$NAffx_9
PANAS$NA10	<-	PANAS$NAff1_10	+	PANAS$NAff2_10	+	PANAS$NAff5_10	+	PANAS$NAff6_10	+	PANAS$NAffx_10
#Consolidate "PANAS" dataset.
PANAS <- PANAS[c(101:120)]

#Mood
Mood <- as.data.frame(Likert[c(85,153,221,261,357)])
Mood[is.na(Mood)] <- 0
Mood$MOOD	<-	Mood$Mood1	+	Mood$MOOD2	+	Mood$MOOD5	+	Mood$Mood6	+	Mood$MOODx
#Consolidate Mood
MOOD <- Mood[c(6)]

#####HALO
Halo <- as.data.frame(Likert[c(61:64,129:132,197:200,237:240,333:336)])
Halo[is.na(Halo)] <- 0
Halo$HALO1	<-	Halo$HALO1_1	+	Halo$HALO2_1	+	Halo$HALO5_1	+	Halo$HALO6_1	+	Halo$HALOx_1 # Facial attractiveness
Halo$HALO2	<-	Halo$HALO1_2	+	Halo$HALO2_2	+	Halo$HALO5_2	+	Halo$HALO6_2	+	Halo$HALOx_2 # Intelligence
Halo$HALO3	<-	Halo$HALO1_3	+	Halo$HALO2_3	+	Halo$HALO5_3	+	Halo$HALO6_3	+	Halo$HALOx_3 # Atheletic ability
Halo$HALO4	<-	Halo$HALO1_4	+	Halo$HALO2_4	+	Halo$HALO5_4	+	Halo$HALO6_4	+	Halo$HALOx_4 # Trivia knowledge
#Consolidate Halo
HALO <- Halo[c(21:24)]

#Recode 0 values to missing.
l <- cbind(agree,PANAS,MOOD,HALO,Likert[c(358:365)])
l <- as.data.frame(apply(l, 2, function(x) {x <- recode(x,"0=NA"); x}))

#Missing data analysis using 'sapply(data2, function(x) sum(is.na(x)))' revealed some missing likert data. 
init <- mice(l[c(-8)], maxit = 0)
meth <- init$method
predM <- init$predictorMatrix
meth[c("IRB3")]="norm"
meth[c("NA3")]="norm"
imputed <- mice(l, method=meth, predictorMatrix=predM, m=5)
l <- complete(imputed)
#Round imputed data to nearest whole number for estimation purposes. 
#l$IRB3 <- ceiling(l$IRB3)
#l$NA3 <- ceiling(l$NA3)

#Retain only variables used for testing purposes.
data2 <- cbind(Likert[c(376:386)],l,Likert[c(365:375)])
n <- as.numeric(nrow(data2))

#Setup demographics.
#Rename variables.
colnames(data2)[colnames(data2)=="Age"] <- "AGE"
data2$AGE <- as.numeric(data2$AGE)
colnames(data2)[colnames(data2)=="Race"] <- "RACE"
colnames(data2)[colnames(data2)=="Gender"] <- "GENDER"
colnames(data2)[colnames(data2)=="Education"] <- "EDUCAT"
colnames(data2)[colnames(data2)=="EmployStat"] <- "EMPLOYED"
colnames(data2)[colnames(data2)=="EmployStatYrs"] <- "JOBTENURE"
colnames(data2)[colnames(data2)=="JobTitle"] <- "PROF_SPECIFY"

#Calculate descriptives
M <- mean(data2$AGE, na.rm = TRUE)
SD <- sd(data2$AGE, na.rm = TRUE)

#Calculate frequencies
##Recode single "f" to female. 
#data2$GENDER[data2$GENDER=="f"] <- 1
female.n <- as.numeric(freq_vect(data2$GENDER)[1,"Count"])
female.p <- as.numeric(freq_vect(data2$GENDER)[1,"Percentage"])
white.n <- as.numeric(freq_vect(data2$RACE)[7,"Count"])
white.p <- as.numeric(freq_vect(data2$RACE)[7,"Percentage"])
fulltime.n <- as.numeric(freq_vect(data2$EMPLOYED)[3,"Count"])
fulltime.p <- as.numeric(freq_vect(data2$EMPLOYED)[3,"Percentage"])

# Dummy code the conodition variable for lavaan.
data2 <- dummy_cols(data2, select_columns = "COND")
```
```{r Study 2: Construct Validity for Affect, include = FALSE}
mood.cfa <- '
#Affectivity
PA =~ PA1 + PA2 + PA3 + PA4 + PA5 + PA6 + PA7 + PA8 + PA9 + PA10 
Na =~ NA1 + NA2 + NA3 + NA4 + NA5 + NA6 + NA7 + NA8 + NA9 + NA10 
AP =~ PA1 + PA2 + PA3 + PA4 + PA5 + PA6 + PA7 + PA8 + PA9 + PA10 + NA1 + NA2 + NA3 + NA4 + NA5 + NA6 + NA7 + NA8 + NA9 + NA10 

#Constrain bifactor covariances
PA ~~ 0*Na
PA ~~ 0*AP
Na ~~ 0*AP
'
mood <- cfa(mood.cfa, data = data2, estimator = "DWLS", parameterization = "theta", information = "expected",  std.lv=TRUE, test = "Satorra-Bentler")
summary(mood, standardized = TRUE, fit.measures =TRUE)
affect <- as.data.frame(predict(mood))

#Bind to data
data2 <- cbind(data2,affect[c(1)])
```
```{r Study 2: Cronbach Alphas, include = FALSE}
#Create scale scores
my.scales <- scoreItems(my.keys.list,data2)
PP.alpha <- round(my.scales[["alpha"]][1],2)
IRB.alpha <- round(my.scales[["alpha"]][2],2)
OCBI.alpha <- round(my.scales[["alpha"]][3],2)
OCBO.alpha <- round(my.scales[["alpha"]][4],2)
PA.alpha <- round(my.scales[["alpha"]][5],2)
```

Nine hundred and one workers from Amazon's Mechanical Turk were paid $1.45 for completing a survey, of which `r apa(Nall,0,T)` consented to participating in our study. These `r apa(Nall,0,T)` participants were randomly assigned to one of five conditions: a non-remedied condition (n = `r apa(N1,0,T)`), a condition where a cover story was used to blind the purpose of the study (n = `r apa(N2,0,T)`), a condition where scale order was randomized (n = `r apa(N3,0,T)`), a condition where items within scales were randomized (n = `r apa(N4,0,T)`), or a condition wherein filler scales were employed (n = `r apa(N5,0,T)`). Respondents who incorrectly responded to an attention check item were excluded from the survey during administration and so their data were not collected. Five more cases were removed for failing to report demographics and abandoning the survey. The rates for successfully passing our manipulation check for the study purpose were between `r percent(sr5)` and `r apa(sr4)` with an overall success rate of `r percent(srall)` and were much stronger than Study 1. Our relatively higher success rates, compared to Study 1, might be attributed tentatively to our using both a video to communicate the purpose of the survey and/or requiring participants to restate the purpose of our study in their own terms in writing prior to participating in our study.

Our filtering process resulted in a final sample of `r apa(n1,0,T)` individuals in the control condition, `r apa(n2,0,T)` in the cover story condition, `r apa(n3,0,T)` in the randomized scales condition, `r apa(n4,0,T)` in the randomized items within scales condition, and `r apa(n5,0,T)` in the filler scales condition. The sample was predominantly Caucasian and the average age was *M* = `r apa(M,2,T)` (*SD* = `r apa(SD,2,T)`). The majority (n = `r apa(fulltime.n,0,T)`) of respondents worked full-time. 

The same measures and modeling approach used in study 1 were used in study 2. Cronbach alphas ranged from `r apa(OCBO.alpha)` for OCBO to `r apa(PA.alpha)` for positive affectivity. Again, cases of missing item-level data were handled using the normal model approach [see @WuComparisonImputationStrategies2015]. 

# Results - Study 2
```{r Study 2: Build for Descriptive Statistics, include=FALSE}
#Create Descriptives Table
scales <- my.scales[["scores"]]
scales <- cbind(data2[c("COND_Control","COND_CSManipulation","COND_FillerScales","COND_ScaleRand","COND_ItemRand","AGE","GENDER")],scales,data2[c("MOOD")])
table4 <- apa.cor.table(scales, table.number = 1, landscape = FALSE)
```
```{r Study 2: Table 4: Descriptives, results="asis"}
#table4
```
```{r Study 2: Model Testing, include = FALSE}
m2.cfa <- ' 
##Substantive factors
PP =~ c(pp1a, pp1b, pp1c, pp1d, pp1e)*PP1 + c(pp2a, pp2b, pp2c, pp2d, pp2e)*PP2 + c(pp3a,pp3b, pp3c, pp3d, pp3e)*PP3 + c(pp4a,pp4b, pp4c, pp4d, pp4e)*PP4 + c(pp5a,pp5b, pp5c, pp5d, pp5e)*PP5 + c(pp6a,pp6b, pp6c, pp6d, pp6e)*PP6 + c(pp7a,pp7b, pp7c, pp7d, pp7e)*PP7 + c(pp8a,pp8b, pp8c, pp8d, pp8e)*PP8 + c(pp9a,pp9b, pp9c, pp9d, pp9e)*PP9 + c(pp10a,pp10b, pp10c, pp10d, pp10e)*PP10
IRB =~ c(irb1a,irb1b,irb1c,irb1d,irb1e)*IRB1 + c(irb2a,irb2b,irb2c,irb2d,irb2e)*IRB2 + c(irb3a,irb3b,irb3c,irb3d,irb3e)*IRB3 + c(irb4a,irb4b,irb4c,irb4d,irb4e)*IRB4 + c(irb5a,irb5b,irb5c,irb5d,irb5e)*IRB5 + c(irb6a,irb6b,irb6c,irb6d,irb6e)*IRB6 + c(irb7a,irb7b,irb7c,irb7d,irb7e)*IRB7
OCBI =~ c(ocbi1a,ocbi1b,ocbi1c,ocbi1d,ocbi1e)*OCBI1 + c(ocbi2a,ocbi2b,ocbi2c,ocbi2d,ocbi2e)*OCBI2 + c(ocbi3a,ocbi3b,ocbi3c,ocbi3d,ocbi3e)*OCBI3 + c(ocbi4a,ocbi4b,ocbi4c,ocbi4d,ocbi4e)*OCBI4 + c(ocbi5a,ocbi5b,ocbi5c,ocbi5d,ocbi5e)*OCBI5 + c(ocbi6a,ocbi6b,ocbi6c,ocbi6d,ocbi6e)*OCBI6 + c(ocbi7a,ocbi7b,ocbi7c,ocbi7d,ocbi7e)*OCBI7
OCBO =~ c(ocbo1a,ocbo1b,ocbo1c,ocbo1d,ocbo1e)*OCBO1 + c(ocbo2a,ocbo2b,ocbo2c,ocbo2d,ocbo2e)*OCBO2 + c(ocbo3a,ocbo3b,ocbo3c,ocbo3d,ocbo3e)*OCBO3 + c(ocbo4a,ocbo4b,ocbo4c,ocbo4d,ocbo4e)*OCBO4 + c(ocbo5a,ocbo5b,ocbo5c,ocbo5d,ocbo5e)*OCBO5 + c(ocbo6a,ocbo6b,ocbo6c,ocbo6d,ocbo6e)*OCBO6 + c(ocbo7a,ocbo7b,ocbo7c,ocbo7d,ocbo7e)*OCBO7

# Negative Item Content Factor
NW =~ c(nw1a, nw1b, nw1c, nw1d, nw1e)*IRB6 + c(nw2a, nw2b, nw2c, nw2d, nw2e)*IRB7 + c(nw3a, nw3b, nw3c, nw3d, nw3e)*OCBO3 + c(nw4a, nw4b, nw4c, nw4d, nw4e)*OCBO4 + c(nw5a, nw5b, nw5c, nw5d, nw5e)*OCBO5
NW ~~ c(1,1,1,1,1)*NW
NW ~ c(0,0,0,0,0)*1
NW ~~ c(0,0,0,0,0)*IRB
NW ~~ c(0,0,0,0,0)*OCBO
NW ~~ c(0,0,0,0,0)*PP
NW ~~ c(0,0,0,0,0)*OCBI

#Factor variances are fixed to 1 to allow estimation.
PP ~~ c(1,1,1,1,1)*PP
IRB ~~ c(1,1,1,1,1)*IRB
OCBI ~~ c(1,1,1,1,1)*OCBI
OCBO ~~ c(1,1,1,1,1)*OCBO

#Factor means of latent factors for all groups are fixed at zero to allow identification.
PP ~ c(0,0,0,0,0)*1
IRB ~ c(0,0,0,0,0)*1
OCBI ~ c(0,0,0,0,0)*1
OCBO ~ c(0,0,0,0,0)*1

##Factor covariances labeled.
PP ~~ c(ppirba,ppirbb,ppirbc,ppirbd,ppirbe)*IRB
PP ~~ c(ppocbia,ppocbib,ppocbic,ppocbid,ppocbie)*OCBI
PP ~~ c(ppocboa,ppocbob,ppocboc,ppocbod,ppocboe)*OCBO
IRB ~~ c(irbocbia,irbocbib,irbocbic,irbocbid,irbocbie)*OCBI
IRB ~~ c(irbocboa,irbocbob,irbocboc,irbocbod,irbocboe)*OCBO
OCBI ~~ c(ocbioa,ocbiob,ocbioc,ocbiod,ocbioe)*OCBO

# Add in "Mood" as a single-item latent construct that is an uncorrelated with all factors.
#Mood =~ .95*MOOD + c(mpp1a, mpp1b, mpp1c, mpp1d, mpp1e)*PP1 + c(mpp2a, mpp2b, mpp2c, mpp2d, mpp2e)*PP2 + c(mpp3a,mpp3b,mpp3c,mpp3d,mpp3e)*PP3 + c(mpp4a,mpp4b,mpp4c,mpp4d,mpp4e)*PP4 + c(mpp5a,mpp5b,mpp5c,mpp5d,mpp5e)*PP5 + c(mpp6a,mpp6b,mpp6c,mpp6d,mpp6e)*PP6 + c(mpp7a,mpp7b,mpp7c,mpp7d,mpp7e)*PP7 + c(mpp8a,mpp8b,mpp8c,mpp8d,mpp8e)*PP8 + c(mpp9a,mpp9b,mpp9c,mpp9d,mpp9e)*PP9 + c(mpp10a,mpp10b,mpp10c,mpp10d,mpp10e)*PP10 + c(mirb1a,mirb1b,mirb1c,mirb1d,mirb1e)*IRB1 + c(mirb2a,mirb2b,mirb2c,mirb2d,mirb2e)*IRB2 + c(mirb3a,mirb3b,mirb3c,mirb3d,mirb3e)*IRB3 + c(mirb4a,mirb4b,mirb4c,mirb4d,mirb4e)*IRB4 + c(mirb5a,mirb5b,mirb5c,mirb5d,mirb5e)*IRB5 + c(mirb6a,mirb6b,mirb6c,mirb6d,mirb6e)*IRB6 + c(mirb7a,mirb7b,mirb7c,mirb7d,mirb7e)*IRB7 + c(mocbi1a,mocbi1b,mocbi1c,mocbi1d,mocbi1e)*OCBI1 + c(mocbi2a,mocbi2b,mocbi2c,mocbi2d,mocbi2e)*OCBI2 + c(mocbi3a,mocbi3b,mocbi3c,mocbi3d,mocbi3e)*OCBI3 + c(mocbi4a,mocbi4b,mocbi4c,mocbi4d,mocbi4e)*OCBI4 + c(mocbi5a,mocbi5b,mocbi5c,mocbi5d,mocbi5e)*OCBI5 + c(mocbi6a,mocbi6b,mocbi6c,mocbi6d,mocbi6e)*OCBI6 + c(mocbi7a,mocbi7b,mocbi7c,mocbi7d,mocbi7e)*OCBI7 + c(mocbo1a,mocbo1b,mocbo1c,mocbo1d,mocbo1e)*OCBO1 + c(mocbo2a,mocbo2b,mocbo2c,mocbo2d,mocbo2e)*OCBO2 + c(mocbo3a,mocbo3b,mocbo3c,mocbo3d,mocbo3e)*OCBO3 + c(mocbo4a,mocbo4b,mocbo4c,mocbo4d,mocbo4e)*OCBO4 + c(mocbo5a,mocbo5b,mocbo5c,mocbo5d,mocbo5e)*OCBO5 + c(mocbo6a,mocbo6b,mocbo6c,mocbo6d,mocbo6e)*OCBO6 + c(mocbo7a,mocbo7b,mocbo7c,mocbo7d,mocbo7e)*OCBO7
 # (see Anderson & Gerbing, 1988; Sorbom & Joreskig, 1982; as cited by Petrescu, 2013)
#MOOD ~~ (1.580939)*(1-.85)*MOOD

# Add in PA as a single-item latent construct that is an uncorrelated with all factors.
#PosAff =~ .95*PA
#PA ~~ (0.7321512)*(1-.92)*PA

# Make uncorrelated with NW
#NW ~~ 0*PosAff

# Add in causal sructure for PA and Mood
#Mood ~ PosAff # PA causes mood

# Mood as a proximal cause of method variance
#Mood ~~ 0*PP # Mood uncorrelated with proactive personality
#Mood ~~ 0*IRB # Mood uncorrelated with IRB
#Mood ~~ 0*OCBI # Mood uncorreated with OCBI
#Mood ~~ 0*OCBO # Mood uncorrelated with OCBO
#NW ~~ 0*Mood # Mood uncorrelated with NW

#Compute factor covariance differences 
ppirbab := ppirba-ppirbb # CS Manipulation vs Control
ppirbbc := ppirbc-ppirbb # Item Randomization v. Control
ppirbbd := ppirbd-ppirbb # Filler Scales v. Control
ppirbbe := ppirbe-ppirbb # Scale Rand v. Control
ppocbiab := ppocbia-ppocbib # CS Manipulation vs Control
ppocbibc := ppocbic-ppocbib # Item Randomization v. Control
ppocbibd := ppocbid-ppocbib # Filler Scales v. Control
ppocbibe := ppocbie-ppocbib # Scale Rand v. Control
ppocboab := ppocboa-ppocbob # CS Manipulation vs Control
ppocbobc := ppocboc-ppocbob # Item Randomization v. Control
ppocbobd := ppocbod-ppocbob # Filler Scales v. Control
ppocbobe := ppocboe-ppocbob # Scale Rand v. Control

# Compute differences in factor loadings
pp1bac	:=	pp1a		-		pp1b
pp1cbc	:=	pp1c		-		pp1b
pp1dbc	:=	pp1d		-	 pp1b
pp1ebc	:=	pp1e		-	 pp1b
pp2bac	:=	pp2a		-		pp2b
pp2cbc	:=	pp2c		-		pp2b
pp2dbc	:=	pp2d		-	 pp2b
pp2ebc	:=	pp2e		-	 pp2b
pp3bac	:=	pp3a		-		pp3b
pp3cbc	:=	pp3c		-		pp3b
pp3dbc	:=	pp3d		-	 pp3b
pp3ebc	:=	pp3e		-	 pp3b
pp4bac	:=	pp4a		-		pp4b
pp4cbc	:=	pp4c		-		pp4b
pp4dbc	:=	pp4d		-	 pp4b
pp4ebc	:=	pp4e		-	 pp4b
pp5bac	:=	pp5a		-		pp5b
pp5cbc	:=	pp5c		-		pp5b
pp5dbc	:=	pp5d		-	 pp5b
pp5ebc	:=	pp5e		-	 pp5b
pp6bac	:=	pp6a		-		pp6b
pp6cbc	:=	pp6c		-		pp6b
pp6dbc	:=	pp6d		-	 pp6b
pp6ebc	:=	pp6e		-	 pp6b
pp7bac	:=	pp7a		-		pp7b
pp7cbc	:=	pp7c		-		pp7b
pp7dbc	:=	pp7d		-	 pp7b
pp7ebc	:=	pp7e		-	 pp7b
pp8bac	:=	pp8a		-		pp8b
pp8cbc	:=	pp8c		-		pp8b
pp8dbc	:=	pp8d		-	 pp8b
pp8ebc	:=	pp8e		-	 pp8b
pp9bac	:=	pp9a		-		pp9b
pp9cbc	:=	pp9c		-		pp9b
pp9dbc	:=	pp9d		-	 pp9b
pp9ebc	:=	pp9e		-	 pp9b
pp10bac	:=	pp10a		-		pp10b
pp10cbc	:=	pp10c		-		pp10b
pp10dbc	:=	pp10d		-	 pp10b
pp10ebc	:=	pp10e		-	 pp10b
irb1bac	:=	irb1a		-		irb1b
irb1cbc	:=	irb1c		-		irb1b
irb1dbc	:=	irb1d		-	 irb1b
irb1ebc	:=	irb1e		-	 irb1b
irb2bac	:=	irb2a		-		irb2b
irb2cbc	:=	irb2c		-		irb2b
irb2dbc	:=	irb2d		-	 irb2b
irb2ebc	:=	irb2e		-	 irb2b
irb3bac	:=	irb3a		-		irb3b
irb3cbc	:=	irb3c		-		irb3b
irb3dbc	:=	irb3d		-	 irb3b
irb3ebc	:=	irb3e		-	 irb3b
irb4bac	:=	irb4a		-		irb4b
irb4cbc	:=	irb4c		-		irb4b
irb4dbc	:=	irb4d		-	 irb4b
irb4ebc	:=	irb4e		-	 irb4b
irb5bac	:=	irb5a		-		irb5b
irb5cbc	:=	irb5c		-		irb5b
irb5dbc	:=	irb5d		-	 irb5b
irb5ebc	:=	irb5e		-	 irb5b
irb6bac	:=	irb6a		-		irb6b
irb6cbc	:=	irb6c		-		irb6b
irb6dbc	:=	irb6d		-	 irb6b
irb6ebc	:=	irb6e		-	 irb6b
irb7bac	:=	irb7a		-		irb7b
irb7cbc	:=	irb7c		-		irb7b
irb7dbc	:=	irb7d		-	 irb7b
irb7ebc	:=	irb7e		-	 irb7b
ocbi1bac	:=	ocbi1a		-		ocbi1b
ocbi1cbc	:=	ocbi1c		-		ocbi1b
ocbi1dbc	:=	ocbi1d		-	 ocbi1b
ocbi1ebc	:=	ocbi1e		-	 ocbi1b
ocbi2bac	:=	ocbi2a		-		ocbi2b
ocbi2cbc	:=	ocbi2c		-		ocbi2b
ocbi2dbc	:=	ocbi2d		-	 ocbi2b
ocbi2ebc	:=	ocbi2e		-	 ocbi2b
ocbi3bac	:=	ocbi3a		-		ocbi3b
ocbi3cbc	:=	ocbi3c		-		ocbi3b
ocbi3dbc	:=	ocbi3d		-	 ocbi3b
ocbi3ebc	:=	ocbi3e		-	 ocbi3b
ocbi4bac	:=	ocbi4a		-		ocbi4b
ocbi4cbc	:=	ocbi4c		-		ocbi4b
ocbi4dbc	:=	ocbi4d		-	 ocbi4b
ocbi4ebc	:=	ocbi4e		-	 ocbi4b
ocbi5bac	:=	ocbi5a		-		ocbi5b
ocbi5cbc	:=	ocbi5c		-		ocbi5b
ocbi5dbc	:=	ocbi5d		-	 ocbi5b
ocbi5ebc	:=	ocbi5e		-	 ocbi5b
ocbi6bac	:=	ocbi6a		-		ocbi6b
ocbi6cbc	:=	ocbi6c		-		ocbi6b
ocbi6dbc	:=	ocbi6d		-	 ocbi6b
ocbi6ebc	:=	ocbi6e		-	 ocbi6b
ocbi7bac	:=	ocbi7a		-		ocbi7b
ocbi7cbc	:=	ocbi7c		-		ocbi7b
ocbi7dbc	:=	ocbi7d		-	 ocbi7b
ocbi7ebc	:=	ocbi7e		-	 ocbi7b
ocbo1bac	:=	ocbo1a		-		ocbo1b
ocbo1cbc	:=	ocbo1c		-		ocbo1b
ocbo1dbc	:=	ocbo1d		-	 ocbo1b
ocbo1ebc	:=	ocbo1e		-	 ocbo1b
ocbo2bac	:=	ocbo2a		-		ocbo2b
ocbo2cbc	:=	ocbo2c		-		ocbo2b
ocbo2dbc	:=	ocbo2d		-	 ocbo2b
ocbo2ebc	:=	ocbo2e		-	 ocbo2b
ocbo3bac	:=	ocbo3a		-		ocbo3b
ocbo3cbc	:=	ocbo3c		-		ocbo3b
ocbo3dbc	:=	ocbo3d		-	 ocbo3b
ocbo3ebc	:=	ocbo3e		-	 ocbo3b
ocbo4bac	:=	ocbo4a		-		ocbo4b
ocbo4cbc	:=	ocbo4c		-		ocbo4b
ocbo4dbc	:=	ocbo4d		-	 ocbo4b
ocbo4ebc	:=	ocbo4e		-	 ocbo4b
ocbo5bac	:=	ocbo5a		-		ocbo5b
ocbo5cbc	:=	ocbo5c		-		ocbo5b
ocbo5dbc	:=	ocbo5d		-	 ocbo5b
ocbo5ebc	:=	ocbo5e		-	 ocbo5b
ocbo6bac	:=	ocbo6a		-		ocbo6b
ocbo6cbc	:=	ocbo6c		-		ocbo6b
ocbo6dbc	:=	ocbo6d		-	 ocbo6b
ocbo6ebc	:=	ocbo6e		-	 ocbo6b
ocbo7bac	:=	ocbo7a		-		ocbo7b
ocbo7cbc	:=	ocbo7c		-		ocbo7b
ocbo7dbc	:=	ocbo7d		-	 ocbo7b
ocbo7ebc	:=	ocbo7e		-	 ocbo7b

# Compute average factor loading differences (tests H2)
ppfl.zbac := (pp1bac + pp2bac + pp3bac + pp4bac + pp5bac + pp6bac + pp7bac + pp8bac + pp9bac + pp10bac)/10 # CS Manipulation vs Control
ppfl.zcbc := (pp1cbc + pp2cbc + pp3cbc + pp4cbc + pp5cbc + pp6cbc + pp7cbc + pp8cbc + pp9cbc + pp10cbc)/10 # Item Randomization vs Control
ppfl.zdbc := (pp1dbc + pp2dbc + pp3dbc + pp4dbc + pp5dbc + pp6dbc + pp7dbc + pp8dbc + pp9dbc + pp10dbc)/10 # Filler Scales vs Control
ppfl.zebc := (pp1ebc + pp2ebc + pp3ebc + pp4ebc + pp5ebc + pp6ebc + pp7ebc + pp8ebc + pp9ebc + pp10ebc)/10 # Scale Randomization vs Control
irbfl.zbac := (irb1bac + irb2bac + irb3bac + irb4bac + irb5bac + irb6bac + irb7bac)/7 # CS Manipulation vs Control
irbfl.zcbc := (irb1cbc + irb2cbc + irb3cbc + irb4cbc + irb5cbc + irb6cbc + irb7cbc)/7 # Item Randomization vs Control
irbfl.zdbc := (irb1dbc + irb2dbc + irb3dbc + irb4dbc + irb5dbc + irb6dbc + irb7dbc)/7 # Filler Scales vs Control
irbfl.zebc := (irb1ebc + irb2ebc + irb3ebc + irb4ebc + irb5ebc + irb6ebc + irb7ebc)/7 # Scale Randomization vs Control
ocbifl.zbac := (ocbi1bac + ocbi2bac + ocbi3bac + ocbi4bac + ocbi5bac + ocbi6bac + ocbi7bac)/7 # CS Manipulation vs Control
ocbifl.zcbc := (ocbi1cbc + ocbi2cbc + ocbi3cbc + ocbi4cbc + ocbi5cbc + ocbi6cbc + ocbi7cbc)/7 # Item Randomization vs Control
ocbifl.zdbc := (ocbi1dbc + ocbi2dbc + ocbi3dbc + ocbi4dbc + ocbi5dbc + ocbi6dbc + ocbi7dbc)/7 # Filler Scales vs Control
ocbifl.zebc := (ocbi1ebc + ocbi2ebc + ocbi3ebc + ocbi4ebc + ocbi5ebc + ocbi6ebc + ocbi7ebc)/7 # Scale Randomization vs Control
ocbofl.zbac := (ocbo1bac + ocbo2bac + ocbo3bac + ocbo4bac + ocbo5bac + ocbo6bac + ocbo7bac)/7 # CS Manipulation vs Control
ocbofl.zcbc := (ocbo1cbc + ocbo2cbc + ocbo3cbc + ocbo4cbc + ocbo5cbc + ocbo6cbc + ocbo7cbc)/7 # Item Randomization vs Control
ocbofl.zdbc := (ocbo1dbc + ocbo2dbc + ocbo3dbc + ocbo4dbc + ocbo5dbc + ocbo6dbc + ocbo7dbc)/7 # Filler Scales vs Control
ocbofl.zebc := (ocbo1ebc + ocbo2ebc + ocbo3ebc + ocbo4ebc + ocbo5ebc + ocbo6ebc + ocbo7ebc)/7 # Scale Randomization vs Control
'

m2 <- cfa(m2.cfa, data = data2, group = "COND", estimator = "DWLS", parameterization = "theta", information = "expected", std.lv=TRUE,test = "Satorra-Bentler")
summary(m2, standardized = TRUE, fit.measures = TRUE)
fitm2.m <- fitmeasures(m2)

#Gather stats
csq.ms2 <- round(as.numeric(fitm2.m[c(6)]), digits = 2) ##Chi-Square
df.ms2 <- round(as.numeric(fitm2.m[c(7)]), digits = 2) ##df
cdfratio.ms2 <- round(csq.ms2/df.ms2,2) ## Chi-square / df ratio
p.ms2 <- pvalr(as.numeric(fitm2.m[c(8)]), digits = 2) ##Chi-Square p value
cssf.ms2 <- round(as.numeric(fitm2.m[c(9)]), digits = 2) ##Chisq.scaling factor
cfi.ms2 <- round(as.numeric(fitm2.m[c(27)]), digits = 2) #CFI robust
remsea.ms2 <- round(as.numeric(fitm2.m[c(44)]), digits = 3) ##RMSEA robust
remseal.ms2 <- round(as.numeric(fitm2.m[c(45)]), digits = 3) #RMSEA lower robust
remseau.ms2 <- round(as.numeric(fitm2.m[c(46)]), digits = 3) #RMSEA upper robust
srmr.ms2<- round(as.numeric(fitm2.m[c(50)]), digits = 3) #SRMR
```
```{r Study 2: Build Latent Correlation and Reliability Decomposition Tables, include = FALSE}
# Build table for latent covariances
factors <- c("Proactive Personality","","","","")
table <- as.data.frame(cbind(factors,"Condition","In-Role Behavior","","","","OCBI","","","","OCBO","","",""))
colnames(table)[1] <- "Predictor"
colnames(table)[2] <- "Condition"
colnames(table)[3] <- "In-Role Behavior"
colnames(table)[4] <- "In-Role Behaviorp"
colnames(table)[5] <- "In-Role Behaviorz"
colnames(table)[6] <- "In-Role Behaviorzp"
colnames(table)[7] <- "OCBI"
colnames(table)[8] <- "OCBIp"
colnames(table)[9] <- "OCBIz"
colnames(table)[10] <- "OCBIzp"
colnames(table)[11] <- "OCBO"
colnames(table)[12] <- "OCBOp"
colnames(table)[13] <- "OCBOz"
colnames(table)[14] <- "OCBOzp"
table$Condition <- as.character(table$Condition)
table$`In-Role Behavior` <- as.numeric(table$`In-Role Behavior`)
table$`In-Role Behaviorp` <- as.character(table$`In-Role Behavior`)
table$`OCBI` <- as.numeric(table$`OCBI`)
table$OCBIp <- as.character(table$OCBIp)
table$`OCBO` <- as.numeric(table$`OCBO`)
table$OCBOp <- as.character(table$OCBOp)
table$Condition[1] <- "No Remedies"
table$Condition[2] <- "Cover Story"
table$Condition[3] <- "Filler Scales"
table$Condition[4] <- "Scale Randomization"
table$Condition[5] <- "Item Randomization"

# Extract pvalues and significance levels for mapping purposes.
data <- parameterEstimates(m2, standardized=TRUE) %>% 
  dplyr::filter(op == "~~") %>%
  dplyr::filter(label == "ppirba" | label == "ppirbb" | label == "ppirbc" | label == "ppirbd" | label == "ppirbe" | label == "ppocbia" | label == "ppocbib" | label == "ppocbic" | label == "ppocbid" | label == "ppocbie" | label == "ppocboa" | label == "ppocbob" | label == "ppocboc" | label == "ppocbod" | label == "ppocboe") %>%
  select(label, est, pvalue) %>%
  mutate_if(is.numeric, ~round(., 2)) 
data$p <- stars.pval(data$pvalue)

# Extract the latent covariances, map on significance levels, and place into table.
m2.covariances<- inspect(m2,"std.all")
table$`In-Role Behavior`[1] <- m2.covariances[["Control"]][["psi"]][1,2]  # PP-IRBControl
PPIRB2b <- round(table$`In-Role Behavior`[1],2)
table$`In-Role Behaviorp`[1] <- data %>% dplyr::filter(label == "ppirbb") %>% select(p) 
table$`In-Role Behavior`[2] <- m2.covariances[["CSManipulation"]][["psi"]][1,2]  # PP-IRBCSManipulation
PPIRB2a <- round(table$`In-Role Behavior`[2],2)
table$`In-Role Behaviorp`[2] <- data %>% dplyr::filter(label == "ppirba") %>% select(p) 
table$`In-Role Behavior`[3] <- m2.covariances[["FillerScales"]][["psi"]][1,2]  # PP-IRBFillerScalesItemRand
PPIRB2d <- round(table$`In-Role Behavior`[3],2)
table$`In-Role Behaviorp`[3] <- data %>% dplyr::filter(label == "ppirbd") %>% select(p) 
table$`In-Role Behavior`[4] <- m2.covariances[["ScaleRand"]][["psi"]][1,2]  # PP-IRBScaleRand
PPIRB2e <- round(table$`In-Role Behavior`[4],2)
table$`In-Role Behaviorp`[4] <- data %>% dplyr::filter(label == "ppirbe") %>% select(p) 
table$`In-Role Behavior`[5] <- m2.covariances[["ItemRand"]][["psi"]][1,2]   # PP-IRBItemRand
PPIRB2c <- round(table$`In-Role Behavior`[5],2)
table$`In-Role Behaviorp`[5] <- data %>% dplyr::filter(label == "ppirbc") %>% select(p) 
table$OCBI[1] <- m2.covariances[["Control"]][["psi"]][1,3]  
PPOCBI2b <- round(table$OCBI[1],2)
table$OCBIp[1] <- data %>% dplyr::filter(label == "ppocbib") %>% select(p) 
table$OCBI[2] <- m2.covariances[["CSManipulation"]][["psi"]][1,3]  
PPOCBI2a <- round(table$OCBI[2],2)
table$OCBIp[2] <- data %>% dplyr::filter(label == "ppocbia") %>% select(p) 
table$OCBI[3] <- m2.covariances[["FillerScales"]][["psi"]][1,3]  
PPOCBI2d <- round(table$OCBI[3],2)
table$OCBIp[3] <- data %>% dplyr::filter(label == "ppocbid") %>% select(p) 
table$OCBI[4] <- m2.covariances[["ScaleRand"]][["psi"]][1,3]  
PPOCBI2e <- round(table$OCBI[4],2)
table$OCBIp[4] <- data %>% dplyr::filter(label == "ppocbie") %>% select(p) 
table$OCBI[5] <- m2.covariances[["ItemRand"]][["psi"]][1,3] 
PPOCBI2c <- round(table$OCBI[5],2)
table$OCBIp[5] <- data %>% dplyr::filter(label == "ppocbic") %>% select(p) 
table$OCBO[1] <- m2.covariances[["Control"]][["psi"]][1,4]  
PPOCBO2b <- round(table$OCBO[1],2)
table$OCBOp[1] <- data %>% dplyr::filter(label == "ppocbob") %>% select(p) 
table$OCBO[2] <- m2.covariances[["CSManipulation"]][["psi"]][1,4]  
PPOCBO2a <- round(table$OCBO[2],2)
table$OCBOp[2] <- data %>% dplyr::filter(label == "ppocboa") %>% select(p) 
table$OCBO[3] <- m2.covariances[["FillerScales"]][["psi"]][1,4]
PPOCBO2d <- round(table$OCBO[3],2)
table$OCBOp[3] <- data %>% dplyr::filter(label == "ppocbod") %>% select(p) 
table$OCBO[4] <- m2.covariances[["ScaleRand"]][["psi"]][1,4]  
PPOCBO2e <- round(table$OCBO[4],2)
table$OCBOp[4] <- data %>% dplyr::filter(label == "ppocboe") %>% select(p) 
table$OCBO[5] <- m2.covariances[["ItemRand"]][["psi"]][1,4]
PPOCBO2c <- round(table$OCBO[5],2)
table$OCBOp[5] <- data %>% dplyr::filter(label == "ppocboc") %>% select(p) 

#Extract z-scores and significance levels for mapping purposes.
z <- parameterestimates(m2, standardized = T) %>%
  dplyr::filter(op == ":=") %>%
  dplyr::filter(label == "ppirbab" | label == "ppirbbc" | label == "ppirbbd" | label == "ppirbbe" | label == "ppocbiab" | label == "ppocbibc" | label == "ppocbibd" | label == "ppocbibe" | label == "ppocboab" | label == "ppocbobc" | label == "ppocbobd" | label == "ppocbobe") %>%
  select(label, z, pvalue) %>%
  mutate_if(is.numeric, ~round(., 3))
PPIRB2abz <- z$z[1]
PPIRB2abp <- z$pvalue[1]
PPIRB2bcz <- z$z[2]
PPIRB2bcp <- z$pvalue[2]
PPIRB2bdz <- z$z[3]
PPIRB2bdp <- z$pvalue[3]
PPIRB2bez <- z$z[4]
PPIRB2bep <- z$pvalue[4]
PPOCBI2abz <- z$z[5]
PPOCBI2abp <- z$pvalue[5]
PPOCBI2bcz <- z$z[6]
PPOCBI2bcp <- z$pvalue[6]
PPOCBI2bdz <- z$z[7]
PPOCBI2bdp <- z$pvalue[7]
PPOCBI2bez <- z$z[8]
PPOCBI2bep <- z$pvalue[8]
PPOCBO2abz <- z$z[9]
PPOCBO2abp <- z$pvalue[9]
PPOCBO2bcz <- z$z[10]
PPOCBO2bcp <- z$pvalue[10]
PPOCBO2bdz <- z$z[11]
PPOCBO2bdp <- z$pvalue[11]
PPOCBO2bez <- z$z[12]
PPOCBO2bep <- z$pvalue[12]

z$p <- stars.pval((z$pvalue))
z$z <- paste0("(", format(unlist(z$z)),")")

# Map z scores and pvalue onto table
table$`In-Role Behaviorz` <-  as.character(table$`In-Role Behaviorz` )
table$`In-Role Behaviorz`[2] <-  z$z[1] # CS Manipulation v. Control
table$`In-Role Behaviorz`[5] <-  z$z[2] # Item Randomization v. Control
table$`In-Role Behaviorz`[3] <-  z$z[3] # Filler Scales v. Control
table$`In-Role Behaviorz`[4] <-  z$z[4] # Scale Randomization v. Control
table$`In-Role Behaviorzp` <-  as.character(table$`In-Role Behaviorzp` )
table$`In-Role Behaviorzp`[2] <-  z$p[1]
table$`In-Role Behaviorzp`[5] <-  z$p[2]
table$`In-Role Behaviorzp`[3] <-  z$p[3]
table$`In-Role Behaviorzp`[4] <-  z$p[4]
table$`OCBIz` <-  as.character(table$`OCBIz` )
table$`OCBIz`[2] <-  z$z[5]
table$`OCBIz`[5] <-  z$z[6]
table$`OCBIz`[3] <-  z$z[7]
table$`OCBIz`[4] <-  z$z[8]
table$`OCBIzp` <-  as.character(table$`OCBIzp` )
table$`OCBIzp`[2] <-  z$p[5]
table$`OCBIzp`[5] <-  z$p[6]
table$`OCBIzp`[3] <-  z$p[7]
table$`OCBIzp`[4] <-  z$p[8]
table$`OCBOz` <-  as.character(table$`OCBOz` )
table$`OCBOz`[2] <-  z$z[9]
table$`OCBOz`[5] <-  z$z[10]
table$`OCBOz`[3] <-  z$z[11]
table$`OCBOz`[4] <-  z$z[12]
table$`OCBOzp` <-  as.character(table$`OCBOzp` )
table$`OCBOzp`[2] <-  z$p[9]
table$`OCBOzp`[5] <-  z$p[10]
table$`OCBOzp`[3] <-  z$p[11]
table$`OCBOzp`[4] <-  z$p[12]

# round
table <- table %>% 
  mutate_if(is.numeric, ~round(., 2))

# Combine columns
table$`In-Role Behavior` <- paste(table$`In-Role Behavior`,table$`In-Role Behaviorp`,table$`In-Role Behaviorz`,table$`In-Role Behaviorzp`)
table$OCBI <- paste(table$OCBI,table$OCBIp,table$OCBIz,table$OCBIzp)
table$OCBO <- paste(table$OCBO,table$OCBOp,table$OCBOz,table$OCBOzp)

# Name table 5
table5 <- table %>%
  select(-one_of("In-Role Behaviorp","In-Role Behaviorz","In-Role Behaviorzp","OCBIp","OCBIz","OCBIzp","OCBOp","OCBOz","OCBOzp"))

# Build reliability decomposition table 
factors <- c("Proactive Personality","","","","","In-Role Behavior", "","","","","OCBI","", "","","","OCBO", "", "", "", "")
table <- as.data.frame(cbind(factors,"Condition","Total Reliability","Substantive Reliability",
                             #"Mood Reliability","% Mood Reliability",
                             "Negative Item Wording Reliability","% Negative Item Wording Reliability"))
colnames(table)[1] <- "Latent Variable"
colnames(table)[2] <- "Condition"
colnames(table)[3] <- "Total Reliability"
colnames(table)[4] <- "Substantive Reliability"
#colnames(table)[5] <- "Mood Reliability"
#colnames(table)[6] <- "% Mood Reliability"
colnames(table)[5] <- "Negative Item Wording Reliability"
colnames(table)[6] <- "% Negative Item Wording Reliability"
table$Condition <- as.character(table$Condition)
table$`Total Reliability` <- as.numeric(table$`Total Reliability`)
table$`Substantive Reliability` <- as.numeric(table$`Substantive Reliability`)
#table$`Mood Reliability` <- as.numeric(table$`Mood Reliability`)
#table$`% Mood Reliability` <- as.numeric(table$`% Mood Reliability`)
table$`Negative Item Wording Reliability` <- as.numeric(table$`Negative Item Wording Reliability`)
table$`% Negative Item Wording Reliability` <- as.numeric(table$`% Negative Item Wording Reliability`)
table$Condition[1] <- "No Remedies"
table$Condition[2] <- "Cover Story"
table$Condition[3] <- "Filler Scales"
table$Condition[4] <- "Scale Randomization"
table$Condition[5] <- "Item Randomization"
table$Condition[6] <- "No Remedies"
table$Condition[7] <- "Cover Story"
table$Condition[8] <- "Filler Scales"
table$Condition[9] <- "Scale Randomization"
table$Condition[10] <- "Item Randomization"
table$Condition[11] <- "No Remedies"
table$Condition[12] <- "Cover Story"
table$Condition[13] <- "Filler Scales"
table$Condition[14] <- "Scale Randomization"
table$Condition[15] <- "Item Randomization"
table$Condition[16] <- "No Remedies"
table$Condition[17] <- "Cover Story"
table$Condition[18] <- "Filler Scales"
table$Condition[19] <- "Scale Randomization"
table$Condition[20] <- "Item Randomization"

# Extract the factor loadings
m2.factorloadings <- inspect(m2,"est")

# Focus on Control
PP <- as.data.frame(m2.factorloadings[["Control"]][["lambda"]])  %>%
  abs() %>%
  rownames_to_column %>%
  slice(1:10) %>%
  mutate(Sub.Rel = PP^2)%>%
#  mutate(Mood.Rel = Mood^2)%>%
  mutate(NW.Rel = NW^2) %>%
  as.data.frame() %>%
#  mutate(Error = (1-(Sub.Rel+Mood.Rel+NW.Rel)))
 mutate(Error = (1-(Sub.Rel+NW.Rel)))

table$`Substantive Reliability`[1] <- (sum(PP$PP)^2)/((sum(PP$PP)^2)+(sum(PP$Mood)^2)+sum(PP$Error))
#table$`Mood Reliability`[1] <- (sum(PP$Mood)^2)/((sum(PP$PP)^2)+(sum(PP$Mood)^2)+sum(PP$Error))
table$`Negative Item Wording Reliability`[1] <- (sum(PP$NW)^2)/((sum(PP$PP)^2)+(sum(PP$Mood)^2)+(sum(PP$NW)^2)+sum(PP$Error))
table$`Total Reliability`[1] <- (table$`Substantive Reliability`[1]+
                                   #table$`Mood Reliability`[1]+
                                   table$`Negative Item Wording Reliability`[1])
#table$`% Mood Reliability`[1] <- (table$`Mood Reliability`[1]/table$`Total Reliability`[1])
table$`% Negative Item Wording Reliability`[1] <- (table$`Negative Item Wording Reliability`[1]/table$`Total Reliability`[1])

IRB <- as.data.frame(m2.factorloadings[["Control"]][["lambda"]])  %>%
  abs() %>%
  rownames_to_column %>%
  slice(11:17) %>%
  mutate(Sub.Rel = IRB^2) %>%
#  mutate(Mood.Rel = Mood^2) %>%
  mutate(NW.Rel = NW^2) %>%
  as.data.frame() %>%
  mutate(Error = (1-(Sub.Rel+NW.Rel)))

table$`Substantive Reliability`[6] <- (sum(IRB$IRB)^2)/((sum(IRB$IRB)^2)+(sum(IRB$Mood)^2)+(sum(IRB$NW)^2)+sum(IRB$Error))
#table$`Mood Reliability`[6] <- (sum(IRB$Mood)^2)/((sum(IRB$IRB)^2)+(sum(IRB$Mood)^2)+(sum(IRB$NW)^2)+sum(IRB$Error))
table$`Negative Item Wording Reliability`[6] <- (sum(IRB$NW)^2)/((sum(IRB$IRB)^2)+(sum(IRB$Mood)^2)+(sum(IRB$NW)^2)+sum(IRB$Error))
table$`Total Reliability`[6] <- (table$`Substantive Reliability`[6]+
                                   #table$`Mood Reliability`[6]+
                                   table$`Negative Item Wording Reliability`[6])
#table$`% Mood Reliability`[6] <- (table$`Mood Reliability`[6]/table$`Total Reliability`[6])
table$`% Negative Item Wording Reliability`[6] <- (table$`Negative Item Wording Reliability`[6]/table$`Total Reliability`[6])

OCBI <- as.data.frame(m2.factorloadings[["Control"]][["lambda"]])  %>%
  abs() %>%
  rownames_to_column %>%
  slice(18:24) %>%
  mutate(Sub.Rel = OCBI^2) %>%
#  mutate(Mood.Rel = Mood^2) %>%
  mutate(NW.Rel = NW^2) %>%
  as.data.frame() %>%
  mutate(Error = (1-(Sub.Rel+NW.Rel)))

table$`Substantive Reliability`[11] <- (sum(OCBI$OCBI)^2)/((sum(OCBI$OCBI)^2)+(sum(OCBI$Mood)^2)+(sum(OCBI$NW)^2)+sum(OCBI$Error))
#table$`Mood Reliability`[11] <- (sum(OCBI$Mood)^2)/((sum(OCBI$OCBI)^2)+(sum(OCBI$Mood)^2)+(sum(OCBI$NW)^2)+sum(OCBI$Error))
table$`Negative Item Wording Reliability`[11] <- (sum(OCBI$NW)^2)/((sum(OCBI$OCBI)^2)+(sum(OCBI$Mood)^2)+(sum(OCBI$NW)^2)+sum(OCBI$Error))
table$`Total Reliability`[11] <- (table$`Substantive Reliability`[11]+
                                    #table$`Mood Reliability`[11]+
                                    table$`Negative Item Wording Reliability`[11])
#table$`% Mood Reliability`[11] <- (table$`Mood Reliability`[11]/table$`Total Reliability`[11])
table$`% Negative Item Wording Reliability`[11] <- (table$`Negative Item Wording Reliability`[11]/table$`Total Reliability`[11])

OCBO <- as.data.frame(m2.factorloadings[["Control"]][["lambda"]])  %>%
  abs() %>%
  rownames_to_column %>%
  slice(25:32) %>%
  mutate(Sub.Rel = OCBO^2) %>%
#  mutate(Mood.Rel = Mood^2) %>%
  mutate(NW.Rel = NW^2) %>%
  as.data.frame() %>%
  mutate(Error = (1-(Sub.Rel+NW.Rel)))

table$`Substantive Reliability`[16] <- (sum(OCBO$OCBO)^2)/((sum(OCBO$OCBO)^2)+(sum(OCBO$Mood)^2)+(sum(OCBO$NW)^2)+sum(OCBO$Error))
#table$`Mood Reliability`[16] <- (sum(OCBO$Mood)^2)/((sum(OCBO$OCBO)^2)+(sum(OCBO$Mood)^2)+(sum(OCBO$NW)^2)+sum(OCBO$Error))
table$`Negative Item Wording Reliability`[16] <- (sum(OCBO$NW)^2)/((sum(OCBO$OCBO)^2)+(sum(OCBO$Mood)^2)+(sum(OCBO$NW)^2)+sum(OCBO$Error))
table$`Total Reliability`[16] <- (table$`Substantive Reliability`[16]+
                                    #table$`Mood Reliability`[16]+
                                    table$`Negative Item Wording Reliability`[16])
#table$`% Mood Reliability`[16] <- (table$`Mood Reliability`[16]/table$`Total Reliability`[16])
table$`% Negative Item Wording Reliability`[16] <- (table$`Negative Item Wording Reliability`[16]/table$`Total Reliability`[16])

# Focus on the CSManpulation
PP <- as.data.frame(m2.factorloadings[["CSManipulation"]][["lambda"]])  %>%
  abs() %>%
  rownames_to_column %>%
  slice(1:10) %>%
  mutate(Sub.Rel = PP^2)%>%
#  mutate(Mood.Rel = Mood^2)%>%
  mutate(NW.Rel = NW^2) %>%
  as.data.frame() %>%
  mutate(Error = (1-(Sub.Rel+NW.Rel)))

table$`Substantive Reliability`[2] <- (sum(PP$PP)^2)/((sum(PP$PP)^2)+(sum(PP$Mood)^2)+sum(PP$Error))
#table$`Mood Reliability`[2] <- (sum(PP$Mood)^2)/((sum(PP$PP)^2)+(sum(PP$Mood)^2)+sum(PP$Error))
table$`Negative Item Wording Reliability`[2] <- (sum(PP$NW)^2)/((sum(PP$PP)^2)+(sum(PP$Mood)^2)+(sum(PP$NW)^2)+sum(PP$Error))
table$`Total Reliability`[2] <- (table$`Substantive Reliability`[2]+
                                   #table$`Mood Reliability`[2]+
                                   table$`Negative Item Wording Reliability`[2])
#table$`% Mood Reliability`[2] <- (table$`Mood Reliability`[2]/table$`Total Reliability`[2])
table$`% Negative Item Wording Reliability`[2] <- (table$`Negative Item Wording Reliability`[2]/table$`Total Reliability`[2])

IRB <- as.data.frame(m2.factorloadings[["CSManipulation"]][["lambda"]])  %>%
  abs() %>%
  rownames_to_column %>%
  slice(11:17) %>%
  mutate(Sub.Rel = IRB^2) %>%
#  mutate(Mood.Rel = Mood^2) %>%
  mutate(NW.Rel = NW^2) %>%
  as.data.frame() %>%
  mutate(Error = (1-(Sub.Rel+NW.Rel)))

table$`Substantive Reliability`[7] <- (sum(IRB$IRB)^2)/((sum(IRB$IRB)^2)+(sum(IRB$Mood)^2)+(sum(IRB$NW)^2)+sum(IRB$Error))
#table$`Mood Reliability`[7] <- (sum(IRB$Mood)^2)/((sum(IRB$IRB)^2)+(sum(IRB$Mood)^2)+(sum(IRB$NW)^2)+sum(IRB$Error))
table$`Negative Item Wording Reliability`[7] <- (sum(IRB$NW)^2)/((sum(IRB$IRB)^2)+(sum(IRB$Mood)^2)+(sum(IRB$NW)^2)+sum(IRB$Error))
table$`Total Reliability`[7] <- (table$`Substantive Reliability`[7]+
                                   #table$`Mood Reliability`[7]+
                                   table$`Negative Item Wording Reliability`[7])
#table$`% Mood Reliability`[7] <- (table$`Mood Reliability`[7]/table$`Total Reliability`[7])
table$`% Negative Item Wording Reliability`[7] <- (table$`Negative Item Wording Reliability`[7]/table$`Total Reliability`[7])

OCBI <- as.data.frame(m2.factorloadings[["CSManipulation"]][["lambda"]])  %>%
  abs() %>%
  rownames_to_column %>%
  slice(18:24) %>%
  mutate(Sub.Rel = OCBI^2) %>%
#  mutate(Mood.Rel = Mood^2) %>%
  mutate(NW.Rel = NW^2) %>%
  as.data.frame() %>%
  mutate(Error = (1-(Sub.Rel+NW.Rel)))

table$`Substantive Reliability`[12] <- (sum(OCBI$OCBI)^2)/((sum(OCBI$OCBI)^2)+(sum(OCBI$Mood)^2)+(sum(OCBI$NW)^2)+sum(OCBI$Error))
#table$`Mood Reliability`[12] <- (sum(OCBI$Mood)^2)/((sum(OCBI$OCBI)^2)+(sum(OCBI$Mood)^2)+(sum(OCBI$NW)^2)+sum(OCBI$Error))
table$`Negative Item Wording Reliability`[12] <- (sum(OCBI$NW)^2)/((sum(OCBI$OCBI)^2)+(sum(OCBI$Mood)^2)+(sum(OCBI$NW)^2)+sum(OCBI$Error))
table$`Total Reliability`[12] <- (table$`Substantive Reliability`[12]+
                                    #table$`Mood Reliability`[12]+
                                    table$`Negative Item Wording Reliability`[12])
#table$`% Mood Reliability`[12] <- (table$`Mood Reliability`[12]/table$`Total Reliability`[12])
table$`% Negative Item Wording Reliability`[12] <- (table$`Negative Item Wording Reliability`[12]/table$`Total Reliability`[12])

OCBO <- as.data.frame(m2.factorloadings[["CSManipulation"]][["lambda"]])  %>%
  abs() %>%
  rownames_to_column %>%
  slice(25:32) %>%
  mutate(Sub.Rel = OCBO^2) %>%
#  mutate(Mood.Rel = Mood^2) %>%
  mutate(NW.Rel = NW^2) %>%
  as.data.frame() %>%
  mutate(Error = (1-(Sub.Rel+NW.Rel)))

table$`Substantive Reliability`[17] <- (sum(OCBO$OCBO)^2)/((sum(OCBO$OCBO)^2)+(sum(OCBO$Mood)^2)+(sum(OCBO$NW)^2)+sum(OCBO$Error))
#table$`Mood Reliability`[17] <- (sum(OCBO$Mood)^2)/((sum(OCBO$OCBO)^2)+(sum(OCBO$Mood)^2)+(sum(OCBO$NW)^2)+sum(OCBO$Error))
table$`Negative Item Wording Reliability`[17] <- (sum(OCBO$NW)^2)/((sum(OCBO$OCBO)^2)+(sum(OCBO$Mood)^2)+(sum(OCBO$NW)^2)+sum(OCBO$Error))
table$`Total Reliability`[17] <- (table$`Substantive Reliability`[17]+
                                    #table$`Mood Reliability`[17]+
                                    table$`Negative Item Wording Reliability`[17])
#table$`% Mood Reliability`[17] <- (table$`Mood Reliability`[17]/table$`Total Reliability`[17])
table$`% Negative Item Wording Reliability`[17] <- (table$`Negative Item Wording Reliability`[17]/table$`Total Reliability`[17])

# Focus on the FillerScales
PP <- as.data.frame(m2.factorloadings[["FillerScales"]][["lambda"]])  %>%
  abs() %>%
  rownames_to_column %>%
  slice(1:10) %>%
  mutate(Sub.Rel = PP^2)%>%
#  mutate(Mood.Rel = Mood^2)%>%
  mutate(NW.Rel = NW^2) %>%
  as.data.frame() %>%
  mutate(Error = (1-(Sub.Rel+NW.Rel)))

table$`Substantive Reliability`[3] <- (sum(PP$PP)^2)/((sum(PP$PP)^2)+(sum(PP$Mood)^2)+sum(PP$Error))
#table$`Mood Reliability`[3] <- (sum(PP$Mood)^2)/((sum(PP$PP)^2)+(sum(PP$Mood)^2)+sum(PP$Error))
table$`Negative Item Wording Reliability`[3] <- (sum(PP$NW)^2)/((sum(PP$PP)^2)+(sum(PP$Mood)^2)+(sum(PP$NW)^2)+sum(PP$Error))
table$`Total Reliability`[3] <- (table$`Substantive Reliability`[3]+
                                   #table$`Mood Reliability`[3]+
                                   table$`Negative Item Wording Reliability`[3])
#table$`% Mood Reliability`[3] <- (table$`Mood Reliability`[3]/table$`Total Reliability`[3])
table$`% Negative Item Wording Reliability`[3] <- (table$`Negative Item Wording Reliability`[3]/table$`Total Reliability`[3])

IRB <- as.data.frame(m2.factorloadings[["FillerScales"]][["lambda"]])  %>%
  abs() %>%
  rownames_to_column %>%
  slice(11:17) %>%
  mutate(Sub.Rel = IRB^2) %>%
#  mutate(Mood.Rel = Mood^2) %>%
  mutate(NW.Rel = NW^2) %>%
  as.data.frame() %>%
  mutate(Error = (1-(Sub.Rel+NW.Rel)))

table$`Substantive Reliability`[8] <- (sum(IRB$IRB)^2)/((sum(IRB$IRB)^2)+(sum(IRB$Mood)^2)+(sum(IRB$NW)^2)+sum(IRB$Error))
#table$`Mood Reliability`[8] <- (sum(IRB$Mood)^2)/((sum(IRB$IRB)^2)+(sum(IRB$Mood)^2)+(sum(IRB$NW)^2)+sum(IRB$Error))
table$`Negative Item Wording Reliability`[8] <- (sum(IRB$NW)^2)/((sum(IRB$IRB)^2)+(sum(IRB$Mood)^2)+(sum(IRB$NW)^2)+sum(IRB$Error))
table$`Total Reliability`[8] <- (table$`Substantive Reliability`[8]+
                                   #table$`Mood Reliability`[8]+
                                   table$`Negative Item Wording Reliability`[8])
#table$`% Mood Reliability`[8] <- (table$`Mood Reliability`[8]/table$`Total Reliability`[8])
table$`% Negative Item Wording Reliability`[8] <- (table$`Negative Item Wording Reliability`[8]/table$`Total Reliability`[8])

OCBI <- as.data.frame(m2.factorloadings[["FillerScales"]][["lambda"]])  %>%
  abs() %>%
  rownames_to_column %>%
  slice(18:24) %>%
  mutate(Sub.Rel = OCBI^2) %>%
#  mutate(Mood.Rel = Mood^2) %>%
  mutate(NW.Rel = NW^2) %>%
  as.data.frame() %>%
  mutate(Error = (1-(Sub.Rel+NW.Rel)))

table$`Substantive Reliability`[13] <- (sum(OCBI$OCBI)^2)/((sum(OCBI$OCBI)^2)+(sum(OCBI$Mood)^2)+(sum(OCBI$NW)^2)+sum(OCBI$Error))
#table$`Mood Reliability`[13] <- (sum(OCBI$Mood)^2)/((sum(OCBI$OCBI)^2)+(sum(OCBI$Mood)^2)+(sum(OCBI$NW)^2)+sum(OCBI$Error))
table$`Negative Item Wording Reliability`[13] <- (sum(OCBI$NW)^2)/((sum(OCBI$OCBI)^2)+(sum(OCBI$Mood)^2)+(sum(OCBI$NW)^2)+sum(OCBI$Error))
table$`Total Reliability`[13] <- (table$`Substantive Reliability`[13]+
                                    #table$`Mood Reliability`[13]+
                                    table$`Negative Item Wording Reliability`[13])
#table$`% Mood Reliability`[13] <- (table$`Mood Reliability`[13]/table$`Total Reliability`[13])
table$`% Negative Item Wording Reliability`[13] <- (table$`Negative Item Wording Reliability`[13]/table$`Total Reliability`[13])

OCBO <- as.data.frame(m2.factorloadings[["FillerScales"]][["lambda"]])  %>%
  abs() %>%
  rownames_to_column %>%
  slice(25:32) %>%
  mutate(Sub.Rel = OCBO^2) %>%
#  mutate(Mood.Rel = Mood^2) %>%
  mutate(NW.Rel = NW^2) %>%
  as.data.frame() %>%
  mutate(Error = (1-(Sub.Rel+NW.Rel)))

table$`Substantive Reliability`[18] <- (sum(OCBO$OCBO)^2)/((sum(OCBO$OCBO)^2)+(sum(OCBO$Mood)^2)+(sum(OCBO$NW)^2)+sum(OCBO$Error))
#table$`Mood Reliability`[18] <- (sum(OCBO$Mood)^2)/((sum(OCBO$OCBO)^2)+(sum(OCBO$Mood)^2)+(sum(OCBO$NW)^2)+sum(OCBO$Error))
table$`Negative Item Wording Reliability`[18] <- (sum(OCBO$NW)^2)/((sum(OCBO$OCBO)^2)+(sum(OCBO$Mood)^2)+(sum(OCBO$NW)^2)+sum(OCBO$Error))
table$`Total Reliability`[18] <- (table$`Substantive Reliability`[18]+
                                    #table$`Mood Reliability`[18]+
                                    table$`Negative Item Wording Reliability`[18])
#table$`% Mood Reliability`[18] <- (table$`Mood Reliability`[18]/table$`Total Reliability`[18])
table$`% Negative Item Wording Reliability`[18] <- (table$`Negative Item Wording Reliability`[18]/table$`Total Reliability`[18])

# Focus on the FillerScales
PP <- as.data.frame(m2.factorloadings[["ScaleRand"]][["lambda"]])  %>%
  abs() %>%
  rownames_to_column %>%
  slice(1:10) %>%
  mutate(Sub.Rel = PP^2)%>%
#  mutate(Mood.Rel = Mood^2)%>%
  mutate(NW.Rel = NW^2) %>%
  as.data.frame() %>%
  mutate(Error = (1-(Sub.Rel+NW.Rel)))

table$`Substantive Reliability`[4] <- (sum(PP$PP)^2)/((sum(PP$PP)^2)+(sum(PP$Mood)^2)+sum(PP$Error))
#table$`Mood Reliability`[4] <- (sum(PP$Mood)^2)/((sum(PP$PP)^2)+(sum(PP$Mood)^2)+sum(PP$Error))
table$`Negative Item Wording Reliability`[4] <- (sum(PP$NW)^2)/((sum(PP$PP)^2)+(sum(PP$Mood)^2)+(sum(PP$NW)^2)+sum(PP$Error))
table$`Total Reliability`[4] <- (table$`Substantive Reliability`[4]+
                                   #table$`Mood Reliability`[4]+
                                   table$`Negative Item Wording Reliability`[4])
#table$`% Mood Reliability`[4] <- (table$`Mood Reliability`[4]/table$`Total Reliability`[4])
table$`% Negative Item Wording Reliability`[4] <- (table$`Negative Item Wording Reliability`[4]/table$`Total Reliability`[4])

IRB <- as.data.frame(m2.factorloadings[["ScaleRand"]][["lambda"]])  %>%
  abs() %>%
  rownames_to_column %>%
  slice(11:17) %>%
  mutate(Sub.Rel = IRB^2) %>%
#  mutate(Mood.Rel = Mood^2) %>%
  mutate(NW.Rel = NW^2) %>%
  as.data.frame() %>%
  mutate(Error = (1-(Sub.Rel+NW.Rel)))

table$`Substantive Reliability`[9] <- (sum(IRB$IRB)^2)/((sum(IRB$IRB)^2)+(sum(IRB$Mood)^2)+(sum(IRB$NW)^2)+sum(IRB$Error))
#table$`Mood Reliability`[9] <- (sum(IRB$Mood)^2)/((sum(IRB$IRB)^2)+(sum(IRB$Mood)^2)+(sum(IRB$NW)^2)+sum(IRB$Error))
table$`Negative Item Wording Reliability`[9] <- (sum(IRB$NW)^2)/((sum(IRB$IRB)^2)+(sum(IRB$Mood)^2)+(sum(IRB$NW)^2)+sum(IRB$Error))
table$`Total Reliability`[9] <- (table$`Substantive Reliability`[9]+
                                   #table$`Mood Reliability`[9]+
                                   table$`Negative Item Wording Reliability`[9])
#table$`% Mood Reliability`[9] <- (table$`Mood Reliability`[9]/table$`Total Reliability`[9])
table$`% Negative Item Wording Reliability`[9] <- (table$`Negative Item Wording Reliability`[9]/table$`Total Reliability`[9])

OCBI <- as.data.frame(m2.factorloadings[["ScaleRand"]][["lambda"]])  %>%
  abs() %>%
  rownames_to_column %>%
  slice(18:24) %>%
  mutate(Sub.Rel = OCBI^2) %>%
#  mutate(Mood.Rel = Mood^2) %>%
  mutate(NW.Rel = NW^2) %>%
  as.data.frame() %>%
  mutate(Error = (1-(Sub.Rel+NW.Rel)))

table$`Substantive Reliability`[14] <- (sum(OCBI$OCBI)^2)/((sum(OCBI$OCBI)^2)+(sum(OCBI$Mood)^2)+(sum(OCBI$NW)^2)+sum(OCBI$Error))
#table$`Mood Reliability`[14] <- (sum(OCBI$Mood)^2)/((sum(OCBI$OCBI)^2)+(sum(OCBI$Mood)^2)+(sum(OCBI$NW)^2)+sum(OCBI$Error))
table$`Negative Item Wording Reliability`[14] <- (sum(OCBI$NW)^2)/((sum(OCBI$OCBI)^2)+(sum(OCBI$Mood)^2)+(sum(OCBI$NW)^2)+sum(OCBI$Error))
table$`Total Reliability`[14] <- (table$`Substantive Reliability`[14]+
                                    #table$`Mood Reliability`[14]+
                                    table$`Negative Item Wording Reliability`[14])
#table$`% Mood Reliability`[14] <- (table$`Mood Reliability`[14]/table$`Total Reliability`[14])
table$`% Negative Item Wording Reliability`[14] <- (table$`Negative Item Wording Reliability`[14]/table$`Total Reliability`[14])

OCBO <- as.data.frame(m2.factorloadings[["ScaleRand"]][["lambda"]])  %>%
  abs() %>%
  rownames_to_column %>%
  slice(25:32) %>%
  mutate(Sub.Rel = OCBO^2) %>%
#  mutate(Mood.Rel = Mood^2) %>%
  mutate(NW.Rel = NW^2) %>%
  as.data.frame() %>%
  mutate(Error = (1-(Sub.Rel+NW.Rel)))

table$`Substantive Reliability`[19] <- (sum(OCBO$OCBO)^2)/((sum(OCBO$OCBO)^2)+(sum(OCBO$Mood)^2)+(sum(OCBO$NW)^2)+sum(OCBO$Error))
#table$`Mood Reliability`[19] <- (sum(OCBO$Mood)^2)/((sum(OCBO$OCBO)^2)+(sum(OCBO$Mood)^2)+(sum(OCBO$NW)^2)+sum(OCBO$Error))
table$`Negative Item Wording Reliability`[19] <- (sum(OCBO$NW)^2)/((sum(OCBO$OCBO)^2)+(sum(OCBO$Mood)^2)+(sum(OCBO$NW)^2)+sum(OCBO$Error))
table$`Total Reliability`[19] <- (table$`Substantive Reliability`[19]+
                                    #table$`Mood Reliability`[19]+
                                    table$`Negative Item Wording Reliability`[19])
#table$`% Mood Reliability`[19] <- (table$`Mood Reliability`[19]/table$`Total Reliability`[19])
table$`% Negative Item Wording Reliability`[19] <- (table$`Negative Item Wording Reliability`[19]/table$`Total Reliability`[19])

# Focus on the ItemRand
PP <- as.data.frame(m2.factorloadings[["ItemRand"]][["lambda"]])  %>%
  abs() %>%
  rownames_to_column %>%
  slice(1:10) %>%
  mutate(Sub.Rel = PP^2)%>%
#  mutate(Mood.Rel = Mood^2)%>%
  mutate(NW.Rel = NW^2) %>%
  as.data.frame() %>%
  mutate(Error = (1-(Sub.Rel+NW.Rel)))

table$`Substantive Reliability`[5] <- (sum(PP$PP)^2)/((sum(PP$PP)^2)+(sum(PP$Mood)^2)+sum(PP$Error))
#table$`Mood Reliability`[5] <- (sum(PP$Mood)^2)/((sum(PP$PP)^2)+(sum(PP$Mood)^2)+sum(PP$Error))
table$`Negative Item Wording Reliability`[5] <- (sum(PP$NW)^2)/((sum(PP$PP)^2)+(sum(PP$Mood)^2)+(sum(PP$NW)^2)+sum(PP$Error))
table$`Total Reliability`[5] <- (table$`Substantive Reliability`[5]+
                                   #table$`Mood Reliability`[5]+
                                   table$`Negative Item Wording Reliability`[5])
#table$`% Mood Reliability`[5] <- (table$`Mood Reliability`[5]/table$`Total Reliability`[5])
table$`% Negative Item Wording Reliability`[5] <- (table$`Negative Item Wording Reliability`[5]/table$`Total Reliability`[5])

IRB <- as.data.frame(m2.factorloadings[["ItemRand"]][["lambda"]])  %>%
  abs() %>%
  rownames_to_column %>%
  slice(11:17) %>%
  mutate(Sub.Rel = IRB^2) %>%
#  mutate(Mood.Rel = Mood^2) %>%
  mutate(NW.Rel = NW^2) %>%
  as.data.frame() %>%
  mutate(Error = (1-(Sub.Rel+NW.Rel)))

table$`Substantive Reliability`[10] <- (sum(IRB$IRB)^2)/((sum(IRB$IRB)^2)+(sum(IRB$Mood)^2)+(sum(IRB$NW)^2)+sum(IRB$Error))
#table$`Mood Reliability`[10] <- (sum(IRB$Mood)^2)/((sum(IRB$IRB)^2)+(sum(IRB$Mood)^2)+(sum(IRB$NW)^2)+sum(IRB$Error))
table$`Negative Item Wording Reliability`[10] <- (sum(IRB$NW)^2)/((sum(IRB$IRB)^2)+(sum(IRB$Mood)^2)+(sum(IRB$NW)^2)+sum(IRB$Error))
table$`Total Reliability`[10] <- (table$`Substantive Reliability`[10]+
                                    #table$`Mood Reliability`[10]+
                                    table$`Negative Item Wording Reliability`[10])
#table$`% Mood Reliability`[10] <- (table$`Mood Reliability`[10]/table$`Total Reliability`[10])
table$`% Negative Item Wording Reliability`[10] <- (table$`Negative Item Wording Reliability`[10]/table$`Total Reliability`[10])

OCBI <- as.data.frame(m2.factorloadings[["ItemRand"]][["lambda"]])  %>%
  abs() %>%
  rownames_to_column %>%
  slice(18:24) %>%
  mutate(Sub.Rel = OCBI^2) %>%
#  mutate(Mood.Rel = Mood^2) %>%
  mutate(NW.Rel = NW^2) %>%
  as.data.frame() %>%
  mutate(Error = (1-(Sub.Rel+NW.Rel)))

table$`Substantive Reliability`[15] <- (sum(OCBI$OCBI)^2)/((sum(OCBI$OCBI)^2)+(sum(OCBI$Mood)^2)+(sum(OCBI$NW)^2)+sum(OCBI$Error))
#table$`Mood Reliability`[15] <- (sum(OCBI$Mood)^2)/((sum(OCBI$OCBI)^2)+(sum(OCBI$Mood)^2)+(sum(OCBI$NW)^2)+sum(OCBI$Error))
table$`Negative Item Wording Reliability`[15] <- (sum(OCBI$NW)^2)/((sum(OCBI$OCBI)^2)+(sum(OCBI$Mood)^2)+(sum(OCBI$NW)^2)+sum(OCBI$Error))
table$`Total Reliability`[15] <- (table$`Substantive Reliability`[15]+
                                    #table$`Mood Reliability`[15]+
                                    table$`Negative Item Wording Reliability`[15])
#table$`% Mood Reliability`[15] <- (table$`Mood Reliability`[15]/table$`Total Reliability`[15])
table$`% Negative Item Wording Reliability`[15] <- (table$`Negative Item Wording Reliability`[15]/table$`Total Reliability`[15])

OCBO <- as.data.frame(m2.factorloadings[["ItemRand"]][["lambda"]])  %>%
  abs() %>%
  rownames_to_column %>%
  slice(25:32) %>%
  mutate(Sub.Rel = OCBO^2) %>%
#  mutate(Mood.Rel = Mood^2) %>%
  mutate(NW.Rel = NW^2) %>%
  as.data.frame() %>%
  mutate(Error = (1-(Sub.Rel+NW.Rel)))

table$`Substantive Reliability`[20] <- (sum(OCBO$OCBO)^2)/((sum(OCBO$OCBO)^2)+(sum(OCBO$Mood)^2)+(sum(OCBO$NW)^2)+sum(OCBO$Error))
#table$`Mood Reliability`[20] <- (sum(OCBO$Mood)^2)/((sum(OCBO$OCBO)^2)+(sum(OCBO$Mood)^2)+(sum(OCBO$NW)^2)+sum(OCBO$Error))
table$`Negative Item Wording Reliability`[20] <- (sum(OCBO$NW)^2)/((sum(OCBO$OCBO)^2)+(sum(OCBO$Mood)^2)+(sum(OCBO$NW)^2)+sum(OCBO$Error))
table$`Total Reliability`[20] <- (table$`Substantive Reliability`[20]+
                                    #table$`Mood Reliability`[20]+
                                    table$`Negative Item Wording Reliability`[20])
#table$`% Mood Reliability`[20] <- (table$`Mood Reliability`[20]/table$`Total Reliability`[20])
table$`% Negative Item Wording Reliability`[20] <- (table$`Negative Item Wording Reliability`[20]/table$`Total Reliability`[20])

# Round columsn to nearest 100th
table <- table %>% 
  mutate_if(is.numeric, ~round(., 2))

# Call table 6
table6 <- table

# Extract parameter estimates
pe <- parameterestimates(m2)

## PP
### CS Manipulation v Control
ppfl.zbac <- pe %>% 
  dplyr::filter(label == "ppfl.zbac") %>%
  select(z)
ppfl.zbac <- round(ppfl.zbac,3)
ppfl.zbac <- as.numeric(ppfl.zbac[1])
ppfl.pbac <- pe %>% 
  dplyr::filter(label == "ppfl.zbac") %>%
  select(pvalue)
ppfl.pbac <- round(ppfl.pbac,3)
ppfl.pbac <- as.numeric(ppfl.pbac[1])
### ItemRand v Control
ppfl.zcbc <- pe %>% 
  dplyr::filter(label == "ppfl.zcbc") %>%
  select(z)
ppfl.zcbc <- round(ppfl.zcbc,3)
ppfl.zcbc <- as.numeric(ppfl.zcbc[1])
ppfl.pcbc <- pe %>% 
  dplyr::filter(label == "ppfl.zcbc") %>%
  select(pvalue)
ppfl.pcbc <- round(ppfl.pcbc,3)
ppfl.pcbc <- as.numeric(ppfl.pcbc[1])
### FillerScales v Control
ppfl.zdbc	 <- pe %>% 
  dplyr::filter(label == "ppfl.zdbc") %>%
  select(z)
ppfl.zdbc	 <- round(ppfl.zdbc	,3)
ppfl.zdbc	 <- as.numeric(ppfl.zdbc[1])
ppfl.pdbc	 <- pe %>% 
  dplyr::filter(label == "ppfl.zdbc") %>%
  select(pvalue)
ppfl.pdbc <- round(ppfl.pdbc,3)
ppfl.pdbc	 <- as.numeric(ppfl.pdbc[1])

### ScaleRand v Control
ppfl.zebc	 <- pe %>% 
  dplyr::filter(label == "ppfl.zebc") %>%
  select(z)
ppfl.zebc	 <- round(ppfl.zebc	,3)
ppfl.zebc	 <- as.numeric(ppfl.zebc[1])
ppfl.pebc	 <- pe %>% 
  dplyr::filter(label == "ppfl.zebc") %>%
  select(pvalue)
ppfl.pebc <- round(ppfl.pebc,3)
ppfl.pebc	 <- as.numeric(ppfl.pebc[1])

## irb
### CS Manipulation v Control
irbfl.zbac <- pe %>% 
  dplyr::filter(label == "irbfl.zbac") %>%
  select(z)
irbfl.zbac <- round(irbfl.zbac,3)
irbfl.zbac <- as.numeric(irbfl.zbac[1])
irbfl.pbac <- pe %>% 
  dplyr::filter(label == "irbfl.zbac") %>%
  select(pvalue)
irbfl.pbac <- round(irbfl.pbac,3)
irbfl.pbac <- as.numeric(irbfl.pbac[1])

### ItemRand v Control
irbfl.zcbc <- pe %>% 
  dplyr::filter(label == "irbfl.zcbc") %>%
  select(z)
irbfl.zcbc <- round(irbfl.zcbc,3)
irbfl.zcbc <- as.numeric(irbfl.zcbc[1])
irbfl.pcbc <- pe %>% 
  dplyr::filter(label == "irbfl.zcbc") %>%
  select(pvalue)
irbfl.pcbc <- round(irbfl.pcbc,3)
irbfl.pcbc <- as.numeric(irbfl.pcbc[1])

### FillerScales v Control
irbfl.zdbc	 <- pe %>% 
  dplyr::filter(label == "irbfl.zdbc") %>%
  select(z)
irbfl.zdbc	 <- round(irbfl.zdbc	,3)
irbfl.zdbc	 <- as.numeric(irbfl.zdbc[1])
irbfl.pdbc	 <- pe %>% 
  dplyr::filter(label == "irbfl.zdbc") %>%
  select(pvalue)
irbfl.pdbc <- round(irbfl.pdbc,3)
irbfl.pdbc	 <- as.numeric(irbfl.pdbc[1])

### ScaleRand v Control
irbfl.zebc	 <- pe %>% 
  dplyr::filter(label == "irbfl.zebc") %>%
  select(z)
irbfl.zebc	 <- round(irbfl.zebc	,3)
irbfl.zebc	 <- as.numeric(irbfl.zebc[1])
irbfl.pebc	 <- pe %>% 
  dplyr::filter(label == "irbfl.zebc") %>%
  select(pvalue)
irbfl.pebc <- round(irbfl.pebc,3)
irbfl.pebc	 <- as.numeric(irbfl.pebc[1])

## ocbi
### CS Manipulation v Control
ocbifl.zbac <- pe %>% 
  dplyr::filter(label == "ocbifl.zbac") %>%
  select(z)
ocbifl.zbac <- round(ocbifl.zbac,3)
ocbifl.zbac <- as.numeric(ocbifl.zbac[1])
ocbifl.pbac <- pe %>% 
  dplyr::filter(label == "ocbifl.zbac") %>%
  select(pvalue)
ocbifl.pbac <- round(ocbifl.pbac,3)
ocbifl.pbac <- as.numeric(ocbifl.pbac[1])

### ItemRand v Control
ocbifl.zcbc <- pe %>% 
  dplyr::filter(label == "ocbifl.zcbc") %>%
  select(z)
ocbifl.zcbc <- round(ocbifl.zcbc,3)
ocbifl.zcbc <- as.numeric(ocbifl.zcbc[1])
ocbifl.pcbc <- pe %>% 
  dplyr::filter(label == "ocbifl.zcbc") %>%
  select(pvalue)
ocbifl.pcbc <- round(ocbifl.pcbc,3)
ocbifl.pcbc <- as.numeric(ocbifl.pcbc[1])

### FillerScales v Control
ocbifl.zdbc	 <- pe %>% 
  dplyr::filter(label == "ocbifl.zdbc") %>%
  select(z)
ocbifl.zdbc	 <- round(ocbifl.zdbc	,3)
ocbifl.zdbc	 <- as.numeric(ocbifl.zdbc[1])
ocbifl.pdbc	 <- pe %>% 
  dplyr::filter(label == "ocbifl.zdbc") %>%
  select(pvalue)
ocbifl.pdbc <- round(ocbifl.pdbc,3)
ocbifl.pdbc	 <- as.numeric(ocbifl.pdbc[1])

### ScaleRand v Control
ocbifl.zebc	 <- pe %>% 
  dplyr::filter(label == "ocbifl.zebc") %>%
  select(z)
ocbifl.zebc	 <- round(ocbifl.zebc	,3)
ocbifl.zebc	 <- as.numeric(ocbifl.zebc[1])
ocbifl.pebc	 <- pe %>% 
  dplyr::filter(label == "ocbifl.zebc") %>%
  select(pvalue)
ocbifl.pebc <- round(ocbifl.pebc,3)
ocbifl.pebc	 <- as.numeric(ocbifl.pebc[1])

## ocbo
### CS Manipulation v Control
ocbofl.zbac <- pe %>% 
  dplyr::filter(label == "ocbofl.zbac") %>%
  select(z)
ocbofl.zbac <- round(ocbofl.zbac,3)
ocbofl.zbac <- as.numeric(ocbofl.zbac[1])
ocbofl.pbac <- pe %>% 
  dplyr::filter(label == "ocbofl.zbac") %>%
  select(pvalue)
ocbofl.pbac <- round(ocbofl.pbac,3)
ocbofl.pbac <- as.numeric(ocbofl.pbac[1])

### ItemRand v Control
ocbofl.zcbc <- pe %>% 
  dplyr::filter(label == "ocbofl.zcbc") %>%
  select(z)
ocbofl.zcbc <- round(ocbofl.zcbc,3)
ocbofl.zcbc <- as.numeric(ocbofl.zcbc[1])
ocbofl.pcbc <- pe %>% 
  dplyr::filter(label == "ocbofl.zcbc") %>%
  select(pvalue)
ocbofl.pcbc <- round(ocbofl.pcbc,3)
ocbofl.pcbc <- as.numeric(ocbofl.pcbc[1])

### FillerScales v Control
ocbofl.zdbc	 <- pe %>% 
  dplyr::filter(label == "ocbofl.zdbc") %>%
  select(z)
ocbofl.zdbc	 <- round(ocbofl.zdbc	,3)
ocbofl.zdbc	 <- as.numeric(ocbofl.zdbc[1])
ocbofl.pdbc	 <- pe %>% 
  dplyr::filter(label == "ocbofl.zdbc") %>%
  select(pvalue)
ocbofl.pdbc <- round(ocbofl.pdbc,3)
ocbofl.pdbc	 <- as.numeric(ocbofl.pdbc[1])

### ScaleRand v Control
ocbofl.zebc	 <- pe %>% 
  dplyr::filter(label == "ocbofl.zebc") %>%
  select(z)
ocbofl.zebc	 <- round(ocbofl.zebc	,3)
ocbofl.zebc	 <- as.numeric(ocbofl.zebc[1])
ocbofl.pebc	 <- pe %>% 
  dplyr::filter(label == "ocbofl.zebc") %>%
  select(pvalue)
ocbofl.pebc <- round(ocbofl.pebc,3)
ocbofl.pebc	 <- as.numeric(ocbofl.pebc[1])
```
```{r Table 3, results = 'asis', tab.cap = NULL, echo = FALSE, include = FALSE}
table5 %>%
  knitr::kable(format = 'html', booktabs=T ,escape = F) %>%
  kable_styling(latex_options = c(
   # "scale_down",
    "threeparttable",
    "hold_position"
    ),
                 repeat_header_method = c("replace")) %>%
  kableExtra::footnote(general = ". p < .10; * p < .05; ** p < .01; *** p < .001. ",
  "Values in parentheses are z-scores that describe the difference between the estimated correlations.") %>%
  landscape() %>%
  kable_styling(full_width = F) %>%
  save_kable("Table3.html")
```
```{r Table 4, results = 'asis', tab.cap = NULL, echo = FALSE, include = FALSE}
table6 %>%
  knitr::kable(format = 'html', booktabs=T ,escape = T) %>%
  kable_styling(latex_options = c("scale_down", "hold_position"),
                 repeat_header_method = c("replace")) %>%
  kableExtra::footnote(general = "Total reliability is a sum of the substantive and method reliability components (see Williams et al., 2010). ") %>%
  landscape() %>%
  kable_styling(full_width = T) %>%
  save_kable("Table4.html")
```

Though our initial model provided a close fit to the data, the estimates we derived were nonsensical. In certain conditions, mood explained a majority of the variance in substantive measures, which could be attributed to assumptions regarding the reliability of our one-item mood measure. Though informed by conservative values [@PetrescuMarketingresearchusing2013], these conservative estimates may be less applicable to our data. Rather than engage in a search for more ideal sample-specific estimates of mood reliability, we decided to drop both mood and positive affectivity from our analysis. Therefore, our results should be viewed as unadjusted any momentary mood effects. This revised model had a close fit ($\chi^2$~(`r df.ms2`)~ = `r csq.ms2`, *p* `r p.ms2`, $\chi^2$/df = `r cdfratio.ms2`, CFI = `r cfi.ms2`, RMSEA = `r remsea.ms2`, SRMR = `r srmr.ms2`) and so we moved to interpreting the results. 

Table 3 contains the estimates for the latent construct correlations. Unexpectedly, the PP-IRB correlations were, compared to the non-remedied condition (Φ = `r PPIRB2b`), all consistently stronger in the remedied conditions (all zs > 3.42, all *p*s < .001). Conversely, the PP–OCBI correlations were generally, though inconsistently, smaller. Compared to the no remedy condition (Φ = `r PPOCBI2b`), the correlations linking PP to OCBI were smaller in the item randomization condition (Φ = `r PPOCBI2c`, z = `r PPOCBI2bcz`, *p* = `r PPOCBI2bcp`), the filler scale condition (Φ = `r PPOCBI2d`, z = `r PPOCBI2bdz`, *p* `r pvalr(PPOCBI2bdp)`), and the scale randomization condition (Φ = `r PPOCBI2e`, z = `r PPOCBI2bez`, *p* = `r PPOCBI2bep`). There was no significant effect of the cover story manipulation on PP–OCBI correlation (Φ = `r PPOCBI2a`, z = `r PPOCBI2abz`, *p* = `r round(PPOCBI2abp,2)`). As for the PP–OCBO correlation, compared to the no remedy condition (Φ = `r PPOCBO2b`), only the cover story manipulation (Φ = `r PPOCBO2a`, z = `r PPOCBO2abz`, *p* = `r PPOCBO2abp`) affected this correlation, increasing it. The remaining remedies did not significantly affect this correlation (filler scales: Φ = `r PPOCBO2d`, z = `r PPOCBO2bdz`, *p* = `r PPOCBO2bdp`, scale randomization: Φ = `r PPOCBO2e`, z = `r PPOCBO2bez`, *p* = `r PPOCBO2bep`). Overall, this study provides evidence that bears complexly upon hypothesis 1 (i.e., some corroborating evidence, some negating evidence, and no evidence depending on the specific comparison in question). 

# - - - - - - - - - - - 
# Insert Tables 3 and 4 about here
# - - - - - - - - - - - 

Table 4 contains the reliability decomposition for our study variables. While the substantive reliability estimates were generally descriptively smaller in the remedied conditions compared to the non-remedied condition (consistent with hypothesis 2), focusing in on the substantive factor loadings alone revealed a complex pattern. For the PP measure, compared to the no remedies condition the substantive factor loadings were weaker across all remedies (all zs < -5.34, all *p*s < .001). For the IRB measure, compared to the no remedies condition the substantive factor loadings were stronger in the cover story condition (z = `r irbfl.zbac`, *p* = `r irbfl.pbac`) and weaker in the item randomization condition (z = `r irbfl.zcbc`, *p* `r pvalr(irbfl.pcbc)`). The substantive factor loadings in the other conditions were not statistically different from their counterparts in the no remedy condition (*p*'s > .73). For the OCBI measure, compared to the no remedies condition the substantive factor loadings were smaller in the item randomization, filler scale, and scale randomization conditions (zs < -3.85, *p*s < .001). However, the evidence that a cover story affected assessments of OCBI, though in the trend that was anticipated, was not statistically significant (z = `r ocbifl.zbac`, *p* = `r ocbifl.pbac`). For the OCBO measurement model, compared to the no remedies condition the substantive factor loadings were smaller in the scale randomization condition (z = `r ocbofl.zebc`, *p* `r pvalr(ocbofl.pebc)`) but larger in the cover story condition (z = `r ocbofl.zbac`, *p* `r pvalr(ocbofl.pbac)`). There was no evidence that randomizing items or using filler scales affected the OCBO measurement model (*p*'s > .145). Importantly, all of these observed effects were quite small in magnitude (standardized factor loadings differed by an absolute value of .13 at most), suggesting marginal but potentially unique impacts. Again, in the results testing hypothesis 2, evidence from the current study indicates complex results (i.e., some corroborating evidence, some negating evidence, and no evidence depending on the specific comparison in question). 

# Discussion Study 2

Study 2 provided evidence that using a specific proximal remedy can influence one's substantive conclusions in unforeseen ways. While some effects were anticipated, many were not. We suspected that since a specific remedy should address a specific method effect (e.g., the cover story should eliminate demand effects; item randomization should eliminate serial item-order effects), the estimated substantive effects would be relatively weaker compared to a condition where no remedy was applied. The reliability decomposition for Study 2 favors this view. Our results overall also suggests that using any single remedy may be insufficient for controlling method variance.

Recall from our discussion of study 1 that a substantive reliability estimate is a ratio of the trait variance (as estimated by substantive factor loadings) to all sources of modeled variance (trait, measured method, and error) reflected by the corresponding measurement models [@WilliamsMethodVarianceMarker2010]. In Study 2, the proactive personality and OCBI measures were modeled as reflecting only two sources of variance, and so we observed smaller substantive reliability estimates across all remedied conditions in an incremental fashion. Unfortunately, for measures that are contaminated by measurable method effects (e.g., negative item wording), the interpretation thereof is more complicated. While we generally see weaker substantive reliability for the IRB and OCBO measures, certain remedied substantive factor loadings are *larger* than the non-remedied factor loadings. This would suggest that CMV, which would be present in the non-remedied condition, can *weaken* estimates. However, the opposite was observed in Study 1. Such conflicting findings suggests, to us, that our understanding of the procedural remedies, and method variance in general, may be overly simplistic. 

To make sense of findings from Studies 1 and 2, we revisited the literature to understand when CMV might weaken or strengthen an estimate. Simulation research into method variance by @coteMeasurementErrorTheory1988, @siemsenCommonMethodBias2010, and @williamsMethodVarianceOrganizational1994 suggests that when measures reflect both substantive *and* method variance (e.g., negative item wording, mood, consistency motif), the results will be much more difficult to interpret. Cumulatively, these scholars point out that while the presence of method variance should be of concern - thus motivating the use of procedural remedies to purify one's results - researchers should be mindful of the following: (i) the correlation between the substantive constructs of interest, (ii) the correlation among method constructs of interest, and more importantly, and (iii) the ratio between the method and trait correlations. In brief, these scholars point out that when method constructs are weakly correlated and this correlation is weaker relative to the correlation among substantive constructs, then estimates will be weakened. Conversely, when method constructs are correlated and this correlation is stronger relative to the correlation among substantive constructs, then estimates will be strengthened. In other words, not only does contamination matter (hence the need for remedies), but the ratio of the interrelation between method and substantive constructs (both measured and not) determines whether inflation, attenuation, or no method bias will occur. 

Placing our results into this broader literature does help us make sense of our findings. Since procedural remedies only address relatively unique causes of method variance, interpreting one's observed estimates will require holding certain assumptions regarding key quantities, such as (i) how the remedy affects the correlation among the relevant method constructs, (ii) what the correlation is linking the substantive constructs, and (iii) the ratio of the two. Unfortunately, these are quantities that are difficult to know, which brings us to our conclusion regarding study 2: when method variance plays a role in a study, it will be difficult to form a reliable interpretation regarding the effect that has been estimated. In practice, this conclusion implies that when method variance can explain one's findings, bundling remedies would be the most advisable course of action. This conclusion also highlights a need for research into the procedural remedies and method constructs that are affected by these remedies, which we will elaborate upon in our general discussion section.

# Study 3 - Efficacy of Introducing a 1-Week Separation Between Predictor and Outcome Measures

As observed in Study 1, mood played a contaminating role in our observations. This is to be expected as data were gathered using a single-source at a single point in time. With Study 3, we examined how the contaminating role played by momentary mood might be reduced. Specifically, we examined the efficacy of introducing a one-week temporal separation of measurement between the administration of our predictor and outcome measures. Conventionally this involves administering predictor measures first and, after a time lag of one week or so, administering the outcome measures. In sum, temporal separation of measurement is designed to reduce if not eliminate any method effects that are relevant to a single-source single time-point design, such as momentary mood, that affect estimates of the correlation among scales.

Recall that findings from Study 2 suggest that the impact of a single remedy on one's substantive estimates can be difficult to know in advance unless the relative interplay between the method and substantive constructs is understood. Findings from Study 1 and 2 suggest that some proximal method effects (i.e., item context effects, scale-order effects) play a role even when data are gathered via a temporal separation. Though we would expect smaller estimates of correlation (consistent with hypothesis 1), any unremedied method effects at play would still contaminate the measurement model, inflating estimates of substantive reliability across all conditions. This should inform our expectations regarding Study 3. While the substantive reliability estimates should be relatively similar across both conditions, for the non-remedied condition, however, the total reliability (i.e., substantive reliability + method reliability) estimated for our outcome measures (i.e., IRB and OCB) should be larger because these outcome measures should contain additional method variance attributable to mood, which has not been remedied. Overall, the latent construct correlations in the remedied condition should generally be weaker (which would support Hypothesis 1), the amount of mood-related variance contaminating the measurement model should also be lower. This latter claim we articulate with Hypothesis 3:

*Hypothesis 3: Inserting a 1-week temporal separation between the administration of predictor and outcome measures will weaken the contaminating role of mood in the substantive outcome variables.*

# Method - Study 3

## Sample, procedure, and analytical approach
```{r Load Study 3 Data, include=FALSE}
data3 <- read_sav("Data/Study 3.sav")

#Address the misnamed OCB variables. Fix OCBO ordering to align with other datasets.
colnames(data3)[colnames(data3)=="OCBI7"] <- "OCBO1new"
colnames(data3)[colnames(data3)=="OCBO7"] <- "OCBI7new"
colnames(data3)[colnames(data3)=="OCBO6"] <- "OCBO7"
colnames(data3)[colnames(data3)=="OCBO5"] <- "OCBO6"
colnames(data3)[colnames(data3)=="OCBO4"] <- "OCBO5"
colnames(data3)[colnames(data3)=="OCBO3"] <- "OCBO4"
colnames(data3)[colnames(data3)=="OCBO2"] <- "OCBO3"
colnames(data3)[colnames(data3)=="OCBO1"] <- "OCBO2"
colnames(data3)[colnames(data3)=="OCBO1new"] <- "OCBO1"
colnames(data3)[colnames(data3)=="OCBI7new"] <- "OCBI7"

#Re-order variables.
data3 <- data3[c(2,3,87,88,89,4:13,30:35,43,37:42,36,44:50,60:79,86)]

#Analyses revealed that some cases of data had been imputed. Missing data appears to have been dealt with by simply imputing an average of sorts. This could be problematic (see Enders, C. K. (2003). Using the expectation maximization algorithm to estimate coefficient alpha for scales with item-level missing data. Psychological Methods, 8(3), 322-337.; Enders, C. K. (2010). Applied missing data analysis: New York, NY: The Guilford Press. Additionally, and more importantly, the required estimation methods are not maximum likelihood but are ordinal, further requiring the each response category be represented (i.e., averages do not fall into a particular response category).  Reversing this decision:
data3$PA2[data3$PA2==3.23] <- NA
data3$PA5[data3$PA5==3.58] <- NA
data3$PA6[data3$PA6==3.59] <- NA
data3$PA8[data3$PA8==3.77] <- NA
data3$PA9[data3$PA9==3.75] <- NA
data3$PA10[data3$PA10==3.57] <- NA
data3$PA3[data3$PA3==3.44] <- NA
data3$PA4[data3$PA4==3.56] <- NA
data3$PA7[data3$PA7==3.45] <- NA
data3$NA9[data3$NA9==1.87] <- NA
data3$NA2[data3$NA2==3.23] <- NA
data3$NA3[data3$NA3==1.72] <- NA
data3$NA6[data3$NA6==2.17] <- NA
data3$NA7[data3$NA7==1.57] <- NA
data3$NA8[data3$NA8==2.11] <- NA
data3$PP1[data3$PP1==3.49] <- NA
data3$PP3[data3$PP3==3.92] <- NA
data3$PP3[data3$PP3==0] <- NA
data3$PP8[data3$PP8==3.57] <- NA
data3$PP9[data3$PP9==3.53] <- NA
data3$PP10[data3$PP10==3.45] <- NA
data3$IRB1[data3$IRB1==4.47] <- NA
data3$IRB2[data3$IRB2==4.47] <- NA
data3$IRB4[data3$IRB4==4.49] <- NA
data3$IRB5[data3$IRB5==3.59] <- NA
data3$IRB5[data3$IRB5==3.96] <- NA
data3$IRB6[data3$IRB6==1.88] <- NA
data3$IRB7[data3$IRB7==1.65] <- NA
data3$IRB7[data3$IRB7==0] <- NA
data3$OCBI3[data3$OCBI3==3.7] <- NA
data3$OCBI3[data3$OCBI3==0] <- NA
data3$OCBI5[data3$OCBI5==3.99] <- NA
data3$OCBI7[data3$OCBI7==4.06] <- NA
data3$OCBO2[data3$OCBO2==4.24] <- NA
data3$OCBO4[data3$OCBO4==1.92] <- NA
data3$OCBO4[data3$OCBO4==0] <- NA
data3$OCBO5[data3$OCBO5==2.12] <- NA
data3$OCBO5[data3$OCBO5==0] <- NA
data3$OCBO6[data3$OCBO6==2.12] <- NA
data3$OCBO7[data3$OCBO7==3.89] <- NA

#Examine missingness pattern.
#MDAplot <-  aggr(data3, col=mdc(1:2), numbers=TRUE, sortVars=TRUE, labels=names(data3), cex.axis=.7, gap=3, ylab=c("Proportion of missingness","Missingness Pattern"))
#Delete cases who did not complete the didn't complete the demographic questionnaire.
data3 <- data3[-c(38,94,141,153,161,165,170,183,188,190,193,195,196,231,300),]
#MDAplot <-  aggr(data3, col=mdc(1:2), numbers=TRUE, sortVars=TRUE, labels=names(data3), cex.axis=.7, gap=3, ylab=c("Proportion of missingness","Missingness Pattern"))
#MDA <- LittleMCAR(data3[c(6:31,33:56)]) #Only 50 items can be analyzed, so one item with full data was dropped from the analysis. 

##Thirty six missing data patterns and a just significant LittleMCAR test suggest that data may not be MCAR. Though imputation is used to fill in gaps, this should be taken as a limitation.

#Count of individuals who agree to participate.
N1 <- as.numeric(freq_vect(data3$COND)[1,"Count"])  #Cross-sectional/Control
N2 <- as.numeric(freq_vect(data3$COND)[2,"Count"])  #Temporal Separation of Measurement

#Missing data analysis using 'sapply(data3, function(x) sum(is.na(x)))' which likert data were missing. 
init <- mice(data3, maxit = 0)
meth <- init$method
predM <- init$predictorMatrix
meth[c("PP1")]="norm"
meth[c("PP3")]="norm"
meth[c("PP8")]="norm"
meth[c("PP9")]="norm"
meth[c("PP10")]="norm"
meth[c("OCBI3")]="norm"
meth[c("OCBI5")]="norm"
meth[c("OCBO1")]="norm"
meth[c("OCBO3")]="norm"
meth[c("OCBO4")]="norm"
meth[c("OCBO5")]="norm"
meth[c("OCBO6")]="norm"
meth[c("OCBO7")]="norm"
meth[c("IRB2")]="norm"
meth[c("IRB4")]="norm"
meth[c("IRB5")]="norm"
meth[c("IRB6")]="norm"
meth[c("IRB7")]="norm"
meth[c("PA1")]="norm"
meth[c("PA2")]="norm"
meth[c("PA3")]="norm"
meth[c("PA4")]="norm"
meth[c("PA5")]="norm"
meth[c("PA6")]="norm"
meth[c("PA7")]="norm"
meth[c("PA8")]="norm"
meth[c("PA9")]="norm"
meth[c("PA10")]="norm"
meth[c("NA1")]="norm"
meth[c("NA2")]="norm"
meth[c("NA3")]="norm"
meth[c("NA4")]="norm"
meth[c("NA5")]="norm"
meth[c("NA6")]="norm"
meth[c("NA7")]="norm"
meth[c("NA8")]="norm"
meth[c("NA9")]="norm"
meth[c("NA10")]="norm"
imputed <- mice(data3, method=meth, predictorMatrix=predM, m=5)
data3 <- complete(imputed)
#Round imputed data to nearest whole number for estimation purposes. 
#data3$PP1 <- ceiling(data3$PP1)
#data3$PP3 <- ceiling(data3$PP3)
#data3$PP8 <- ceiling(data3$PP8)
#data3$PP9 <- ceiling(data3$PP9)
#data3$PP10 <- ceiling(data3$PP10)
#data3$OCBI3 <- ceiling(data3$OCBI3)
#data3$OCBI5 <- ceiling(data3$OCBI5)
#data3$OCBO1 <- ceiling(data3$OCBO1)
#data3$OCBO3 <- ceiling(data3$OCBO3)
#data3$OCBO4 <- ceiling(data3$OCBO4)
#data3$OCBO5 <- ceiling(data3$OCBO5)
#data3$OCBO6 <- ceiling(data3$OCBO6)
#data3$OCBO7 <- ceiling(data3$OCBO7)
#data3$IRB2 <- ceiling(data3$IRB2)
#data3$IRB4 <- ceiling(data3$IRB4)
#data3$IRB5 <- ceiling(data3$IRB5)
#data3$IRB6 <- ceiling(data3$IRB6)
#data3$IRB7 <- ceiling(data3$IRB7)
#data3$PA1 <- ceiling(data3$PA1)
#data3$PA2 <- ceiling(data3$PA2)
#data3$PA3 <- ceiling(data3$PA3)
#data3$PA4 <- ceiling(data3$PA4)
#data3$PA5 <- ceiling(data3$PA5)
#data3$PA6 <- ceiling(data3$PA6)
#data3$PA7 <- ceiling(data3$PA7)
#data3$PA8 <- ceiling(data3$PA8)
#data3$PA9 <- ceiling(data3$PA9)
#data3$PA10 <- ceiling(data3$PA10)
#data3$NA1 <- ceiling(data3$NA1)
#data3$NA2 <- ceiling(data3$NA2)
#data3$NA3 <- ceiling(data3$NA3)
#data3$NA4 <- ceiling(data3$NA4)
#data3$NA5 <- ceiling(data3$NA5)
#data3$NA6 <- ceiling(data3$NA6)
#data3$NA7 <- ceiling(data3$NA7)
#data3$NA8 <- ceiling(data3$NA8)
#data3$NA9 <- ceiling(data3$NA9)
#data3$NA10 <- ceiling(data3$NA10)

#Setup demographics.
#Rename variables.
colnames(data3)[colnames(data3)=="SEX"] <- "GENDER"

#Calculate descriptives
M <- mean(data3$AGE, na.rm = TRUE)
SD <- sd(data3$AGE, na.rm = TRUE)

#Calculate frequencies
##Recode single "f" to female. 
data3$GENDER[data3$GENDER=="f"] <- 1
female.n <- as.numeric(freq_vect(data3$GENDER)[2,"Count"])
female.p <- as.numeric(freq_vect(data3$GENDER)[2,"Percentage"])
white.n <- as.numeric(freq_vect(data3$RACE)[2,"Count"])
white.p <- as.numeric(freq_vect(data3$RACE)[2,"Percentage"])
fulltime.n <- as.numeric(freq_vect(data3$PartFullTime)[2,"Count"])
fulltime.p <- as.numeric(freq_vect(data3$PartFullTime)[2,"Percentage"])
```
```{r Study 3: Cronbach Alphas, include = FALSE}
#Cronbach Alphas
my.scales <- scoreItems(my.keys.list,data3)
PP.alpha <- round(my.scales[["alpha"]][1],2)
IRB.alpha <- round(my.scales[["alpha"]][2],2)
OCBI.alpha <- round(my.scales[["alpha"]][3],2)
OCBO.alpha <- round(my.scales[["alpha"]][4],2)
PA.alpha <- round(my.scales[["alpha"]][5],2)
```
```{r Study 3: Build for Descriptive Statistics, include=FALSE}
#Create Descriptives Table
scales <- my.scales[["scores"]]
scales <- cbind(data3[c(1,3,4)],scales,data3[c("MOOD_T1")])
table7 <- apa.cor.table(scales, table.number = 1, landscape = FALSE)
```
```{r Study 3: Table 7: Descriptives, results="asis"}
#table7
```
```{r Study 3: Construct Validity for Affect, include = FALSE}
mood <- cfa(mood.cfa, data = data3, estimator = "DWLS", parameterization = "theta", information = "expected",  std.lv=TRUE, test = "Satorra-Bentler")
summary(mood, standardized = TRUE, fit.measures =TRUE)
affect <- as.data.frame(predict(mood))

#Bind to data
data3 <- cbind(data3,affect[c(1)])
```

Participants were recruited using a Survey Monkey panel and randomly assigned to a condition where they received all measures at the same time (i.e., control) or a condition whereby a temporal separation of one week was used to divide the administration of predictor and outcome measures. No inattentive responding measures were used, so our results should be viewed as not accounting for inattentive response bias. All respondents received $5 to complete this survey. Fifteen respondents did not answer demographic questions and were deleted. For the temporal remedy condition (n = `r apa(N2,0,T)`), after one week, the in-role behavior and OCB measures were administered along with a measure of positive and negative affectivity and the demographics questionnaire. Additionally, anticipating a significant amount of attrition in temporal separation condition, we requested twice as many individuals participate in the temporal separation condition to end up with roughly equal sample sizes. For the cross-sectional data (n = `r apa(N1,0,T)`), respondents completed all measures at the same point in time. Notwithstanding missing demographic data, the sample was predominantly Caucasian, and the average age was *M* = `r apa(M,2,T)` (*SD* = `r apa(SD,2,T)`). The majority (n = `r apa(fulltime.n,0,T)`) of respondents worked full-time. The same measures and analytical approach used in study 1 and 2 were used in study 3 (i.e., reduced mood effects were assessed via estimating the differences in mood contamination across measurement models and calculating the appropriate z-scores). Cronbach alphas ranged from `r apa(OCBO.alpha)` for OCBO to `r apa(PA.alpha)` for positive affectivity. 

# Results - Study 3
```{r Study 3: Model Testing, include = FALSE}
m3.cfa <- ' 
# Substantive factors
PP =~ c(pp1a, pp1b)*PP1 + c(pp2a, pp2b)*PP2 + c(pp3a,pp3b)*PP3 + c(pp4a,pp4b)*PP4 + c(pp5a,pp5b)*PP5 + c(pp6a,pp6b)*PP6 + c(pp7a,pp7b)*PP7 + c(pp8a,pp8b)*PP8 + c(pp9a,pp9b)*PP9 + c(pp10a,pp10b)*PP10
IRB =~ c(irb1a,irb1b)*IRB1 + c(irb2a,irb2b)*IRB2 + c(irb3a,irb3b)*IRB3 + c(irb4a,irb4b)*IRB4 + c(irb5a,irb5b)*IRB5 + c(irb6a,irb6b)*IRB6 + c(irb7a,irb7b)*IRB7
OCBI =~ c(ocbi1a,ocbi1b)*OCBI1 + c(ocbi2a,ocbi2b)*OCBI2 + c(ocbi3a,ocbi3b)*OCBI3 + c(ocbi4a,ocbi4b)*OCBI4 + c(ocbi5a,ocbi5b)*OCBI5 + c(ocbi6a,ocbi6b)*OCBI6 + c(ocbi7a,ocbi7b)*OCBI7
OCBO =~ c(ocbo1a,ocbo1b)*OCBO1 + c(ocbo2a,ocbo2b)*OCBO2 + c(ocbo3a,ocbo3b)*OCBO3 + c(ocbo4a,ocbo4b)*OCBO4 + c(ocbo5a,ocbo5b)*OCBO5 + c(ocbo6a,ocbo6b)*OCBO6 + c(ocbo7a,ocbo7b)*OCBO7

# Negative Item Content Factor
NW =~ c(nw1a, nw1b)*IRB6 + c(nw2a, nw2b)*IRB7 + c(nw3a, nw3b)*OCBO3 + c(nw4a, nw4b)*OCBO4 + c(nw5a, nw5b)*OCBO5
NW ~~ c(1,1)*NW
NW ~ c(0,0)*1
NW ~~ c(0,0)*IRB
NW ~~ c(0,0)*OCBO
NW ~~ c(0,0)*PP
NW ~~ c(0,0)*OCBI

#Factor variances are fixed to 1 to allow estimation.
###Substantive factors
PP ~~ c(1,1)*PP
IRB ~~ c(1,1)*IRB
OCBI ~~ c(1,1)*OCBI
OCBO ~~ c(1,1)*OCBO

##Factor covariances labels 
PP ~~ c(PPIRB1,PPIRB2)*IRB
PP ~~ c(PPOCBI1,PPOCBI2)*OCBI
PP ~~ c(PPOCBO1,PPOCBO2)*OCBO
IRB ~~ c(IRBOCBI1,IRBOCBI2)*OCBI
IRB ~~ c(IRBOCBO1,IRBOCBO2)*OCBO
OCBI ~~ c(OCBIO1,OCBIO2)*OCBO

#Factor means of both groups are fixed at zero to allow identification.
##Substantive factors
PP ~ c(0,0)*1
IRB ~ c(0,0)*1
OCBI ~ c(0,0)*1
OCBO ~ c(0,0)*1

# Add in "Mood" as a single-item latent construct that is an uncorrelated with all factors.
Mood =~ .95*MOOD_T1 + c(mpp1a, mpp1b)*PP1 + c(mpp2a, mpp2b)*PP2 + c(mpp3a,mpp3b)*PP3 + c(mpp4a,mpp4b)*PP4 + c(mpp5a,mpp5b)*PP5 + c(mpp6a,mpp6b)*PP6 + c(mpp7a,mpp7b)*PP7 + c(mpp8a,mpp8b)*PP8 + c(mpp9a,mpp9b)*PP9 + c(mpp10a,mpp10b)*PP10 + c(mirb1a,mirb1b)*IRB1 + c(mirb2a,mirb2b)*IRB2 + c(mirb3a,mirb3b)*IRB3 + c(mirb4a,mirb4b)*IRB4 + c(mirb5a,mirb5b)*IRB5 + c(mirb6a,mirb6b)*IRB6 + c(mirb7a,mirb7b)*IRB7 + c(mocbi1a,mocbi1b)*OCBI1 + c(mocbi2a,mocbi2b)*OCBI2 + c(mocbi3a,mocbi3b)*OCBI3 + c(mocbi4a,mocbi4b)*OCBI4 + c(mocbi5a,mocbi5b)*OCBI5 + c(mocbi6a,mocbi6b)*OCBI6 + c(mocbi7a,mocbi7b)*OCBI7 + c(mocbo1a,mocbo1b)*OCBO1 + c(mocbo2a,mocbo2b)*OCBO2 + c(mocbo3a,mocbo3b)*OCBO3 + c(mocbo4a,mocbo4b)*OCBO4 + c(mocbo5a,mocbo5b)*OCBO5 + c(mocbo6a,mocbo6b)*OCBO6 + c(mocbo7a,mocbo7b)*OCBO7
 # (see Anderson & Gerbing, 1988; Sorbom & Joreskig, 1982; as cited by Petrescu, 2013)
MOOD_T1 ~~ (1.837648)*(1-.85)*MOOD_T1

# Add in PA as a single-item latent construct that is an uncorrelated with all factors.
PosAff =~ .95*PA
PA ~~ (0.9279188)*(1-.92)*PA

# Make uncorrelated with NW
NW ~~ 0*PosAff

# Add in causal sructure for PA and Mood
Mood ~ PosAff # PA causes mood

# Mood as a proximal cause of method variance
Mood ~~ 0*PP # Mood uncorrelated with proactive personality
Mood ~~ 0*IRB # Mood uncorrelated with IRB
Mood ~~ 0*OCBI # Mood uncorreated with OCBI
Mood ~~ 0*OCBO # Mood uncorrelated with OCBO
NW ~~ 0*Mood # Mood uncorrelated with NW

# Estimate differences in key parameters (factor loadings and covariances). 
# Compute differences in factor loadings
pp1c	:=	pp1b		-		pp1a
pp2c	:=	pp2b		-		pp2a
pp3c	:=	pp3b		-	pp3a
pp4c	:=	pp4b		-	pp4a
pp5c	:=	pp5b		-	pp5a
pp6c	:=	pp6b		-		pp6a
pp7c	:=	pp7b		-		pp7a
pp8c	:=	pp8b		-		pp8a
pp9c	:=	pp9b		-		pp9a
pp10c	:=	pp10b		-		pp10a
irb1c	:=	irb1b		-		irb1a
irb2c	:=	irb2b		-		irb2a
irb3c	:=	irb3b		-		irb3a
irb4c	:=	irb4b		-		irb4a
irb5c	:=	irb5b		-		irb5a
irb6c	:=	irb6b		-		irb6a
irb7c	:=	irb7b		-  irb7a
ocbi1c	:=	ocbi1b		- ocbi1a
ocbi2c	:=	ocbi2b		-	ocbi2a
ocbi3c	:=	ocbi3b		-	ocbi3a
ocbi4c	:=	ocbi4b		-	ocbi4a
ocbi5c	:=	ocbi5b		- ocbi5a
ocbi6c	:=	ocbi6b		- ocbi6a
ocbi7c	:=	ocbi7b		- ocbi7a
ocbo1c	:=	ocbo1b		-	ocbo1a
ocbo2c	:=	ocbo2b		-	ocbo2a
ocbo3c	:=	ocbo3b		-	ocbo3a
ocbo4c	:=	ocbo4b		-	ocbo4a
ocbo5c	:=	ocbo5b		- ocbo5a
ocbo6c	:=	ocbo6b		- ocbo6a
ocbo7c	:=	ocbo7b		- ocbo7a

# Compute covariance differences
PPIRB3 := PPIRB2-PPIRB1
PPOCBI3 := PPOCBI2-PPOCBI1
PPOCBO3 := PPOCBO2-PPOCBO1

# Compute average factor loading differences (tests H2)
ppfl.z := (pp1c + pp2c + pp3c + pp4c + pp5c + pp6c + pp7c + pp8c + pp9c + pp10c)/10
irbfl.z := (irb1c + irb2c + irb3c + irb4c + irb5c + irb6c + irb7c)/7
ocbifl.z := (ocbi1c + ocbi2c + ocbi3c + ocbi4c + ocbi5c + ocbi6c + ocbi7c)/7
ocbofl.z := (ocbo1c + ocbo2c + ocbo3c + ocbo4c + ocbo5c + ocbo6c + ocbo7c)/7

# Compute differences in mood effect loadings
mpp1c	:=	mpp1b		-		mpp1a
mpp2c	:=	mpp2b		-		mpp2a
mpp3c	:=	mpp3b		-	mpp3a
mpp4c	:=	mpp4b		-	mpp4a
mpp5c	:=	mpp5b		-	mpp5a
mpp6c	:=	mpp6b		-		mpp6a
mpp7c	:=	mpp7b		-		mpp7a
mpp8c	:=	mpp8b		-		mpp8a
mpp9c	:=	mpp9b		-		mpp9a
mpp10c	:=	mpp10b		-		mpp10a
mirb1c	:=	mirb1b		-		mirb1a
mirb2c	:=	mirb2b		-		mirb2a
mirb3c	:=	mirb3b		-		mirb3a
mirb4c	:=	mirb4b		-		mirb4a
mirb5c	:=	mirb5b		-		mirb5a
mirb6c	:=	mirb6b		-		mirb6a
mirb7c	:=	mirb7b		-  mirb7a
mocbi1c	:=	mocbi1b		- mocbi1a
mocbi2c	:=	mocbi2b		-	mocbi2a
mocbi3c	:=	mocbi3b		-	mocbi3a
mocbi4c	:=	mocbi4b		-	mocbi4a
mocbi5c	:=	mocbi5b		- mocbi5a
mocbi6c	:=	mocbi6b		- mocbi6a
mocbi7c	:=	mocbi7b		- mocbi7a
mocbo1c	:=	mocbo1b		-	mocbo1a
mocbo2c	:=	mocbo2b		-	mocbo2a
mocbo3c	:=	mocbo3b		-	mocbo3a
mocbo4c	:=	mocbo4b		-	mocbo4a
mocbo5c	:=	mocbo5b		- mocbo5a
mocbo6c	:=	mocbo6b		- mocbo6a
mocbo7c	:=	mocbo7b		- mocbo7a

# Compute average factor loading differences (tests H3)
mood.ppfl.z := (mpp1c + mpp2c + mpp3c + mpp4c + mpp5c + mpp6c + mpp7c + mpp8c + mpp9c + mpp10c)/10
mood.irbfl.z := (mirb1c + mirb2c + mirb3c + mirb4c + mirb5c + mirb6c + mirb7c)/7
mood.ocbifl.z := (mocbi1c + mocbi2c + mocbi3c + mocbi4c + mocbi5c + mocbi6c + mocbi7c)/7
mood.ocbofl.z := (mocbo1c + mocbo2c + mocbo3c + mocbo4c + mocbo5c + mocbo6c + mocbo7c)/7
'

m3 <- cfa(m3.cfa, data = data3, group = "COND", estimator = "DWLS", parameterization = "theta", information = "expected",  std.lv=TRUE,test = "Satorra-Bentler")
summary(m3, standardized = TRUE, fit.measures = TRUE)
mod.m <- modindices(m3, minimum.value = 10, sort = TRUE)
fitm3.m <- fitmeasures(m3)
mod.m <- modindices(m3, minimum.value = 10, sort = TRUE)
fitm3.m <- fitmeasures(m3)

#Gather stats
csq.ms3 <- round(as.numeric(fitm3.m[c(6)]), digits = 2) ##Chi-Square
df.ms3 <- round(as.numeric(fitm3.m[c(7)]), digits = 2) ##df
cdfratio.ms3 <- round(csq.ms3/df.ms3,2) ## Chi-square / df ratio
p.ms3 <- pvalr(as.numeric(fitm3.m[c(8)]), digits = 2) ##Chi-Square p value
cssf.ms3 <- round(as.numeric(fitm3.m[c(9)]), digits = 2) ##Chisq.scaling factor
cfi.ms3 <- round(as.numeric(fitm3.m[c(27)]), digits = 2) #CFI robust
remsea.ms3 <- round(as.numeric(fitm3.m[c(44)]), digits = 3) ##RMSEA robust
remseal.ms3 <- round(as.numeric(fitm3.m[c(45)]), digits = 3) #RMSEA lower robust
remseau.ms3 <- round(as.numeric(fitm3.m[c(46)]), digits = 3) #RMSEA upper robust
srmr.ms3<- round(as.numeric(fitm3.m[c(50)]), digits = 3) #SRMR
```
```{r Study 3: Build Latent Correlation and Reliability Decomposition Tables, include = FALSE}
# Build table for latent covariances
factors <- c("Proactive Personality","")
table <- as.data.frame(cbind(factors,"Condition","In-Role Behavior","","","","OCBI","","","","OCBO","","",""))
colnames(table)[1] <- "Predictor"
colnames(table)[2] <- "Condition"
colnames(table)[3] <- "In-Role Behavior"
colnames(table)[4] <- "In-Role Behaviorp"
colnames(table)[5] <- "In-Role Behaviorz"
colnames(table)[6] <- "In-Role Behaviorzp"
colnames(table)[7] <- "OCBI"
colnames(table)[8] <- "OCBIp"
colnames(table)[9] <- "OCBIz"
colnames(table)[10] <- "OCBIzp"
colnames(table)[11] <- "OCBO"
colnames(table)[12] <- "OCBOp"
colnames(table)[13] <- "OCBOz"
colnames(table)[14] <- "OCBOzp"
table$Condition <- as.character(table$Condition)
table$`In-Role Behavior` <- as.numeric(table$`In-Role Behavior`)
table$`In-Role Behaviorp` <- as.character(table$`In-Role Behavior`)
table$`OCBI` <- as.numeric(table$`OCBI`)
table$OCBIp <- as.character(table$OCBIp)
table$`OCBO` <- as.numeric(table$`OCBO`)
table$OCBOp <- as.character(table$OCBOp)
table$Condition[1] <- "No Remedies"
table$Condition[2] <- "One-Week Separation"

# Extract pvalues and significance levels for mapping purposes.
data <- parameterEstimates(m3, standardized=TRUE)%>% 
  dplyr::filter(op == "~~") %>%
  dplyr::filter(label == "PPIRB1" | label == "PPOCBI1" | label == "PPOCBO1" | label == "PPIRB2" | label == "PPOCBI2" | label == "PPOCBO2") %>%
  select(label, est, pvalue) %>%
  mutate_if(is.numeric, ~round(., 2)) 
data$p <- stars.pval(data$pvalue)

# Extract the latent covariances loadings, map on significance levels, and place into table.
m3.covariances<- inspect(m3,"std.all")
table$`In-Role Behavior`[1] <- m3.covariances[["0"]][["psi"]][1,2]  # PP-IRBnr
PPIRB3a <- round(table$`In-Role Behavior`[1],2)
table$`In-Role Behaviorp`[1] <- data %>% dplyr::filter(label == "PPIRB1") %>% select(p) 
table$`In-Role Behavior`[2] <- m3.covariances[["1"]][["psi"]][1,2]  # PP-IRBr
PPIRB3b <- round(table$`In-Role Behavior`[2],2)
table$`In-Role Behaviorp`[2] <- data %>% dplyr::filter(label == "PPIRB2") %>% select(p) 
table$OCBI[1] <- m3.covariances[["0"]][["psi"]][1,3]  # PP-OCBInr
PPOCBI3a <- round(table$OCBI[1],2)
table$OCBIp[1] <- data %>% dplyr::filter(label == "PPOCBI1") %>% select(p) 
table$OCBI[2] <- m3.covariances[["1"]][["psi"]][1,3]  # PP-OCBIr
PPOCBI3b <- round(table$OCBI[2], 2)
table$OCBIp[2] <- data %>% dplyr::filter(label == "PPOCBI2") %>% select(p) 
table$OCBO[1] <- m3.covariances[["0"]][["psi"]][1,4]  # PP-OCBOnr
PPOCBO3a <- round(table$OCBO[1],2)
table$OCBOp[1] <- data %>% dplyr::filter(label == "PPOCBO1") %>% select(p) 
table$OCBO[2] <- m3.covariances[["1"]][["psi"]][1,4]  # PP-OCBOr
PPOCBO3b <- round(table$OCBO[2],2)
table$OCBOp[2] <- data %>% dplyr::filter(label == "PPOCBO2") %>% select(p) 

#Extract z-scores and significance levels for mapping purposes.
z <- parameterestimates(m3, standardized = T) %>%
  dplyr::filter(op == ":=") %>%
  dplyr::filter(label == "PPIRB3" | label == "PPOCBI3" | label == "PPOCBO3") %>%
  select(label, z, pvalue) %>%
  mutate_if(is.numeric, ~round(., 3))
PPIRB3z <- z$z[1]
PPIRB3zp <- z$pvalue[1]
PPOCBI3z <- z$z[2]
PPOCBI3zp <- z$pvalue[2] %>% pvalr()
PPOCBO3z <- z$z[3]
PPOCBO3zp <- z$pvalue[3] %>% pvalr()
z$p <- stars.pval((z$pvalue))
z$z <- paste0("(", format(unlist(z$z)),")")

# Map z scores and pvalue onto table
table$`In-Role Behaviorz` <-  as.character(table$`In-Role Behaviorz` )
table$`In-Role Behaviorz`[2] <-  z$z[1]
table$`In-Role Behaviorzp` <-  as.character(table$`In-Role Behaviorzp` )
table$`In-Role Behaviorzp`[2] <-  z$p[1]
table$`OCBIz` <-  as.character(table$`OCBIz` )
table$`OCBIz`[2] <-  z$z[2]
table$`OCBIzp` <-  as.character(table$`OCBIzp` )
table$`OCBIzp`[2] <-  z$p[2]
table$`OCBOz` <-  as.character(table$`OCBOz` )
table$`OCBOz`[2] <-  z$z[3]
table$`OCBOzp` <-  as.character(table$`OCBOzp` )
table$`OCBOzp`[2] <-  z$p[3]

# round
table <- table %>% 
  mutate_if(is.numeric, ~round(., 2))

# Combine columns
table$`In-Role Behavior` <- paste(table$`In-Role Behavior`,table$`In-Role Behaviorp`,table$`In-Role Behaviorz`,table$`In-Role Behaviorzp`)
table$OCBI <- paste(table$OCBI,table$OCBIp,table$OCBIz,table$OCBIzp)
table$OCBO <- paste(table$OCBO,table$OCBOp,table$OCBOz,table$OCBOzp)

# Name table 8
table8 <- table %>%
  select(-one_of("In-Role Behaviorp","In-Role Behaviorz","In-Role Behaviorzp","OCBIp","OCBIz","OCBIzp","OCBOp","OCBOz","OCBOzp"))

# Build reliability decomposition table 
factors <- c("Proactive Personality","","In-Role Behavior", "","OCBI","", "OCBO", "")
table <- as.data.frame(cbind(factors,"Condition","Total Reliability","Substantive Reliability","Mood Reliability","% Mood Reliability","Negative Item Wording Reliability","% Negative Item Wording Reliability"))
colnames(table)[1] <- "Latent Variable"
colnames(table)[2] <- "Condition"
colnames(table)[3] <- "Total Reliability"
colnames(table)[4] <- "Substantive Reliability"
colnames(table)[5] <- "Mood Reliability"
colnames(table)[6] <- "% Mood Reliability"
colnames(table)[7] <- "Negative Item Wording Reliability"
colnames(table)[8] <- "% Negative Item Wording Reliability"
table$Condition <- as.character(table$Condition)
table$`Total Reliability` <- as.numeric(table$`Total Reliability`)
table$`Substantive Reliability` <- as.numeric(table$`Substantive Reliability`)
table$`Mood Reliability` <- as.numeric(table$`Mood Reliability`)
table$`% Mood Reliability` <- as.numeric(table$`% Mood Reliability`)
table$`Negative Item Wording Reliability` <- as.numeric(table$`Negative Item Wording Reliability`)
table$`% Negative Item Wording Reliability` <- as.numeric(table$`% Negative Item Wording Reliability`)
table$Condition[1] <- "No Remedies"
table$Condition[2] <- "One-Week Separation"
table$Condition[3] <- "No Remedies"
table$Condition[4] <- "One-Week Separation"
table$Condition[5] <- "No Remedies"
table$Condition[6] <- "One-Week Separation"
table$Condition[7] <- "No Remedies"
table$Condition[8] <- "One-Week Separation"

# Extract the factor loadings
m3.factorloadings <- inspect(m3,"est")

# Focus on the remedied group
PP <- as.data.frame(m3.factorloadings[["1"]][["lambda"]])  %>%
  abs() %>%
  rownames_to_column %>%
  slice(1:10) %>%
  mutate(Sub.Rel = PP^2)%>%
  mutate(Mood.Rel = Mood^2)%>%
  mutate(NW.Rel = NW^2) %>%
  as.data.frame() %>%
  mutate(Error = (1-(Sub.Rel+Mood.Rel+NW.Rel)))

table$`Substantive Reliability`[2] <- (sum(PP$PP)^2)/((sum(PP$PP)^2)+(sum(PP$Mood)^2)+sum(PP$Error))
table$`Mood Reliability`[2] <- (sum(PP$Mood)^2)/((sum(PP$PP)^2)+(sum(PP$Mood)^2)+sum(PP$Error))
table$`Negative Item Wording Reliability`[2] <- (sum(PP$NW)^2)/((sum(PP$PP)^2)+(sum(PP$Mood)^2)+(sum(PP$NW)^2)+sum(PP$Error))
table$`Total Reliability`[2] <- (table$`Substantive Reliability`[2]+table$`Mood Reliability`[2]+table$`Negative Item Wording Reliability`[2])
table$`% Mood Reliability`[2] <- (table$`Mood Reliability`[2]/table$`Total Reliability`[2])
table$`% Negative Item Wording Reliability`[2] <- (table$`Negative Item Wording Reliability`[2]/table$`Total Reliability`[2])

IRB <- as.data.frame(m3.factorloadings[["1"]][["lambda"]])  %>%
  abs() %>%
  rownames_to_column %>%
  slice(11:17) %>%
  mutate(Sub.Rel = IRB^2) %>%
  mutate(Mood.Rel = Mood^2) %>%
  mutate(NW.Rel = NW^2) %>%
  as.data.frame() %>%
  mutate(Error = (1-(Sub.Rel+Mood.Rel+NW.Rel)))

table$`Substantive Reliability`[4] <- (sum(IRB$IRB)^2)/((sum(IRB$IRB)^2)+(sum(IRB$Mood)^2)+(sum(IRB$NW)^2)+sum(IRB$Error))
table$`Mood Reliability`[4] <- (sum(IRB$Mood)^2)/((sum(IRB$IRB)^2)+(sum(IRB$Mood)^2)+(sum(IRB$NW)^2)+sum(IRB$Error))
table$`Negative Item Wording Reliability`[4] <- (sum(IRB$NW)^2)/((sum(IRB$IRB)^2)+(sum(IRB$Mood)^2)+(sum(IRB$NW)^2)+sum(IRB$Error))
table$`Total Reliability`[4] <- (table$`Substantive Reliability`[4]+table$`Mood Reliability`[4]+table$`Negative Item Wording Reliability`[4])
table$`% Mood Reliability`[4] <- (table$`Mood Reliability`[4]/table$`Total Reliability`[4])
table$`% Negative Item Wording Reliability`[4] <- (table$`Negative Item Wording Reliability`[4]/table$`Total Reliability`[4])

OCBI <- as.data.frame(m3.factorloadings[["1"]][["lambda"]])  %>%
  abs() %>%
  rownames_to_column %>%
  slice(18:24) %>%
  mutate(Sub.Rel = OCBI^2) %>%
  mutate(Mood.Rel = Mood^2) %>%
  mutate(NW.Rel = NW^2) %>%
  as.data.frame() %>%
  mutate(Error = (1-(Sub.Rel+Mood.Rel+NW.Rel)))

table$`Substantive Reliability`[6] <- (sum(OCBI$OCBI)^2)/((sum(OCBI$OCBI)^2)+(sum(OCBI$Mood)^2)+(sum(OCBI$NW)^2)+sum(OCBI$Error))
table$`Mood Reliability`[6] <- (sum(OCBI$Mood)^2)/((sum(OCBI$OCBI)^2)+(sum(OCBI$Mood)^2)+(sum(OCBI$NW)^2)+sum(OCBI$Error))
table$`Negative Item Wording Reliability`[6] <- (sum(OCBI$NW)^2)/((sum(OCBI$OCBI)^2)+(sum(OCBI$Mood)^2)+(sum(OCBI$NW)^2)+sum(OCBI$Error))
table$`Total Reliability`[6] <- (table$`Substantive Reliability`[6]+table$`Mood Reliability`[6]+table$`Negative Item Wording Reliability`[6])
table$`% Mood Reliability`[6] <- (table$`Mood Reliability`[6]/table$`Total Reliability`[6])
table$`% Negative Item Wording Reliability`[6] <- (table$`Negative Item Wording Reliability`[6]/table$`Total Reliability`[6])

OCBO <- as.data.frame(m3.factorloadings[["1"]][["lambda"]])  %>%
  abs() %>%
  rownames_to_column %>%
  slice(25:32) %>%
  mutate(Sub.Rel = OCBO^2) %>%
  mutate(Mood.Rel = Mood^2) %>%
  mutate(NW.Rel = NW^2) %>%
  as.data.frame() %>%
  mutate(Error = (1-(Sub.Rel+Mood.Rel+NW.Rel)))

table$`Substantive Reliability`[8] <- (sum(OCBO$OCBO)^2)/((sum(OCBO$OCBO)^2)+(sum(OCBO$Mood)^2)+(sum(OCBO$NW)^2)+sum(OCBO$Error))
table$`Mood Reliability`[8] <- (sum(OCBO$Mood)^2)/((sum(OCBO$OCBO)^2)+(sum(OCBO$Mood)^2)+(sum(OCBO$NW)^2)+sum(OCBO$Error))
table$`Negative Item Wording Reliability`[8] <- (sum(OCBO$NW)^2)/((sum(OCBO$OCBO)^2)+(sum(OCBO$Mood)^2)+(sum(OCBO$NW)^2)+sum(OCBO$Error))
table$`Total Reliability`[8] <- (table$`Substantive Reliability`[8]+table$`Mood Reliability`[8]+table$`Negative Item Wording Reliability`[8])
table$`% Mood Reliability`[8] <- (table$`Mood Reliability`[8]/table$`Total Reliability`[8])
table$`% Negative Item Wording Reliability`[8] <- (table$`Negative Item Wording Reliability`[8]/table$`Total Reliability`[8])

# Focus on non-remedied
PP <- as.data.frame(m3.factorloadings[["0"]][["lambda"]])  %>%
  abs() %>%
  rownames_to_column %>%
  slice(1:10) %>%
  mutate(Sub.Rel = PP^2)%>%
  mutate(Mood.Rel = Mood^2)%>%
  mutate(NW.Rel = NW^2) %>%
  as.data.frame() %>%
  mutate(Error = (1-(Sub.Rel+Mood.Rel+NW.Rel)))

table$`Substantive Reliability`[1] <- (sum(PP$PP)^2)/((sum(PP$PP)^2)+(sum(PP$Mood)^2)+sum(PP$Error))
table$`Mood Reliability`[1] <- (sum(PP$Mood)^2)/((sum(PP$PP)^2)+(sum(PP$Mood)^2)+sum(PP$Error))
table$`Negative Item Wording Reliability`[1] <- (sum(PP$NW)^2)/((sum(PP$PP)^2)+(sum(PP$Mood)^2)+(sum(PP$NW)^2)+sum(PP$Error))
table$`Total Reliability`[1] <- (table$`Substantive Reliability`[1]+table$`Mood Reliability`[1]+table$`Negative Item Wording Reliability`[1])
table$`% Mood Reliability`[1] <- (table$`Mood Reliability`[1]/table$`Total Reliability`[1])
table$`% Negative Item Wording Reliability`[1] <- (table$`Negative Item Wording Reliability`[1]/table$`Total Reliability`[1])

IRB <- as.data.frame(m3.factorloadings[["0"]][["lambda"]])  %>%
  abs() %>%
  rownames_to_column %>%
  slice(11:17) %>%
  mutate(Sub.Rel = IRB^2) %>%
  mutate(Mood.Rel = Mood^2) %>%
  mutate(NW.Rel = NW^2) %>%
  as.data.frame() %>%
  mutate(Error = (1-(Sub.Rel+Mood.Rel+NW.Rel)))

table$`Substantive Reliability`[3] <- (sum(IRB$IRB)^2)/((sum(IRB$IRB)^2)+(sum(IRB$Mood)^2)+(sum(IRB$NW)^2)+sum(IRB$Error))
table$`Mood Reliability`[3] <- (sum(IRB$Mood)^2)/((sum(IRB$IRB)^2)+(sum(IRB$Mood)^2)+(sum(IRB$NW)^2)+sum(IRB$Error))
table$`Negative Item Wording Reliability`[3] <- (sum(IRB$NW)^2)/((sum(IRB$IRB)^2)+(sum(IRB$Mood)^2)+(sum(IRB$NW)^2)+sum(IRB$Error))
table$`Total Reliability`[3] <- (table$`Substantive Reliability`[3]+table$`Mood Reliability`[3]+table$`Negative Item Wording Reliability`[3])
table$`% Mood Reliability`[3] <- (table$`Mood Reliability`[3]/table$`Total Reliability`[3])
table$`% Negative Item Wording Reliability`[3] <- (table$`Negative Item Wording Reliability`[3]/table$`Total Reliability`[3])

OCBI <- as.data.frame(m3.factorloadings[["0"]][["lambda"]])  %>%
  abs() %>%
  rownames_to_column %>%
  slice(18:24) %>%
  mutate(Sub.Rel = OCBI^2) %>%
  mutate(Mood.Rel = Mood^2) %>%
  mutate(NW.Rel = NW^2) %>%
  as.data.frame() %>%
  mutate(Error = (1-(Sub.Rel+Mood.Rel+NW.Rel)))

table$`Substantive Reliability`[5] <- (sum(OCBI$OCBI)^2)/((sum(OCBI$OCBI)^2)+(sum(OCBI$Mood)^2)+(sum(OCBI$NW)^2)+sum(OCBI$Error))
table$`Mood Reliability`[5] <- (sum(OCBI$Mood)^2)/((sum(OCBI$OCBI)^2)+(sum(OCBI$Mood)^2)+(sum(OCBI$NW)^2)+sum(OCBI$Error))
table$`Negative Item Wording Reliability`[5] <- (sum(OCBI$NW)^2)/((sum(OCBI$OCBI)^2)+(sum(OCBI$Mood)^2)+(sum(OCBI$NW)^2)+sum(OCBI$Error))
table$`Total Reliability`[5] <- (table$`Substantive Reliability`[5]+table$`Mood Reliability`[5]+table$`Negative Item Wording Reliability`[5])
table$`% Mood Reliability`[5] <- (table$`Mood Reliability`[5]/table$`Total Reliability`[5])
table$`% Negative Item Wording Reliability`[5] <- (table$`Negative Item Wording Reliability`[5]/table$`Total Reliability`[5])

OCBO <- as.data.frame(m3.factorloadings[["0"]][["lambda"]])  %>%
  abs() %>%
  rownames_to_column %>%
  slice(25:32) %>%
  mutate(Sub.Rel = OCBO^2) %>%
  mutate(Mood.Rel = Mood^2) %>%
  mutate(NW.Rel = NW^2) %>%
  as.data.frame() %>%
  mutate(Error = (1-(Sub.Rel+Mood.Rel+NW.Rel)))

table$`Substantive Reliability`[7] <- (sum(OCBO$OCBO)^2)/((sum(OCBO$OCBO)^2)+(sum(OCBO$Mood)^2)+(sum(OCBO$NW)^2)+sum(OCBO$Error))
table$`Mood Reliability`[7] <- (sum(OCBO$Mood)^2)/((sum(OCBO$OCBO)^2)+(sum(OCBO$Mood)^2)+(sum(OCBO$NW)^2)+sum(OCBO$Error))
table$`Negative Item Wording Reliability`[7] <- (sum(OCBO$NW)^2)/((sum(OCBO$OCBO)^2)+(sum(OCBO$Mood)^2)+(sum(OCBO$NW)^2)+sum(OCBO$Error))
table$`Total Reliability`[7] <- (table$`Substantive Reliability`[7]+table$`Mood Reliability`[7]+table$`Negative Item Wording Reliability`[7])
table$`% Mood Reliability`[7] <- (table$`Mood Reliability`[7]/table$`Total Reliability`[7])
table$`% Negative Item Wording Reliability`[7] <- (table$`Negative Item Wording Reliability`[7]/table$`Total Reliability`[7])

# Round columsn to nearest 100th
table <- table %>% 
  mutate_if(is.numeric, ~round(., 2))

# Call table 9
table9 <- table

# Extract parameter estimates
pe <- parameterestimates(m3)
## PP
pp3z <- pe %>% 
  dplyr::filter(label == "ppfl.z") %>%
  select(z)
pp3z <- round(pp3z,3)
pp3z <- as.numeric(pp3z[1])
pp3p <- pe %>% 
  dplyr::filter(label == "ppfl.z") %>%
  select(pvalue)
pp3p <- round(pp3p,3)
pp3p <- as.numeric(pp3p[1])
## IRB
irb3z <- pe %>% 
  dplyr::filter(label == "irbfl.z") %>%
  select(z)
irb3z <- round(irb3z,3)
irb3z <- as.numeric(irb3z[1])
irb3p <- pe %>% 
  dplyr::filter(label == "irbfl.z") %>%
  select(pvalue)
irb3p <- round(irb3p,3)
irb3p <- as.numeric(irb3p[1])

## OCBI
ocbi3z <- pe %>% 
  dplyr::filter(label == "ocbifl.z") %>%
  select(z)
ocbi3z <- round(ocbi3z,3)
ocbi3z <- as.numeric(ocbi3z[1])
ocbi3p <- pe %>% 
  dplyr::filter(label == "ocbifl.z") %>%
  select(pvalue)
ocbi3p <- round(ocbi3p,3)
ocbi3p <- as.numeric(ocbi3p[1])

## OCBO
ocbo3z <- pe %>% 
  dplyr::filter(label == "ocbofl.z") %>%
  select(z)
ocbo3z <- round(ocbo3z,3)
ocbo3z <- as.numeric(ocbo3z[1])
ocbo3p <- pe %>% 
  dplyr::filter(label == "ocbofl.z") %>%
  select(pvalue)
ocbo3p <- round(ocbo3p,3)
ocbo3p <- as.numeric(ocbo3p[1])

## Mood
mood.ocbofl.z <- pe %>% 
  dplyr::filter(label == "mood.ocbofl.z") %>%
  select(z)
mood.ocbofl.z <- round(mood.ocbofl.z,3)
mood.ocbofl.z <- as.numeric(mood.ocbofl.z[1])
mood.ocbofl.p <- pe %>% 
  dplyr::filter(label == "mood.ocbofl.z") %>%
  select(pvalue)
mood.ocbofl.p <- round(mood.ocbofl.p,3)
mood.ocbofl.p <- as.numeric(mood.ocbofl.p[1])
```
```{r Table 5}
table8 %>%
  knitr::kable(format = 'html', booktabs=T ,escape = T) %>%
  kable_styling(latex_options = c("scale_down", "hold_position"),
                 repeat_header_method = c("replace")) %>%
  kableExtra::footnote(general = "* p < .05; ** p < .01; *** p < .001. ") %>%
  landscape() %>%
  kable_styling(full_width = T) %>%
  save_kable("Table5.html")
```
```{r Table 6}
table9 %>%
  knitr::kable(format = 'html', booktabs=T ,escape = T) %>%
  kable_styling(latex_options = c("scale_down", "hold_position"),
                 repeat_header_method = c("replace")) %>%
  kableExtra::footnote(general = "Total reliability is a sum of the substantive and method reliability components (see Williams et al., 2010). ") %>%
  landscape() %>%
  kable_styling(full_width = T) %>%
  save_kable("Table6.html")
```

Our proposed model had a close fit ($\chi^2$~(`r df.ms3`)~ = `r csq.ms3`, *p* `r p.ms3`, $\chi^2$/df = `r cdfratio.ms3`, CFI = `r cfi.ms3`, RMSEA = `r remsea.ms3`, SRMR = `r srmr.ms3`) and so we moved to interpreting the results. Table 5 contains the estimates for the latent construct correlations. A consistent trend emerged. Specifically, all of the latent construct correlations were larger under the non-remedied condition relative to the remedied condition. The PP–IRB correlation was smaller in the remedied condition compared to the non-remedied condition (Φs: `r PPIRB3b` vs. `r PPIRB3a`, z = `r PPIRB3z`, *p* = `r PPIRB3zp`). The same was also observed for the PP–OCBI correlation (Φs: `r PPOCBI3b` vs. `r PPOCBI3a`, z = `r PPOCBI3z`, *p* `r PPOCBI3zp`) as well as the PP–OCBO correlation (Φs: `r PPOCBO3b` vs. `r PPOCBO3a`, z = `r PPOCBO3z`, *p* `r PPOCBO3zp`). Overall, these findings provide strong support for hypothesis 1. 

# - - - - - - - - - - - 
# Insert Tables 5 and 6 about here
# - - - - - - - - - - - 

Table 6 contains the reliability decomposition for our key study variables. The pattern of substantive reliability estimates was not consistent with our expectations. Substantive reliability estimates were roughly equivalent for PP and IRB across conditions, however these estimates were larger for the OCB measures in the remedied condition. The mood reliability estimates were roughly equivalent across the measures of PP, IRB, and OCBI. However, mood was clearly a contaminant for the OCBO measure across both conditions comprising 41% and 31% of the reliable variance in the no remedies and remedied conditions, respectively. The contamination of negative item wording effects were also complex as these effects seemed stronger for the IRB measure in the no remedies condition (21%) compared to the one-week separation condition (8%). The opposite pattern appears to be the case for the OCBO measure as 26% of the reliable variance was attributable to negative item wording in the no remedies condition, while 19% was attributable to negative item wording in the one-week separation condition.

Comparing the substantive factor loadings across conditions revealed relatively small and possibly negligible differences. Unexpectedly, we observed smaller substantive factor loadings for the proactive personality in the remedied condition (z = `r pp3z`, *p* = `r pp3p`). We also observed smaller loadings for the IRB (z = `r irb3z`, *p* = `r irb3p`) measure in this condition. Similarly surprising, the substantive factor loadings were higher for the OCBI measure in the remedied condition (z = `r ocbi3z`, *p* = `r ocbi3p`). No difference was observed for the substantive factor loadings for the OCBO measure (z = `r ocbo3z`, *p* = `r ocbo3p`). Overall, this provides mixed evidence for hypothesis 2. 

Regarding hypothesis 3, mood was a relatively weaker contaminant for the OCBO measure in the remedied condition (z = `r mood.ocbofl.z`, *p* `r pvalr(mood.ocbofl.p)`). No other differences in mood contamination were observed across our outcomes (*p*'s > .05). Overall, this provides mixed evidence for hypothesis 3. 

# Discussion - Study 3

In our third study, we found that predictor and outcome data gathered using a one-week temporal separation of measurement resulted in weaker substantive predictor-outcome relations. Indeed, for the proactive personality-OCBO correlation, this relationship was rendered non-significant, suggesting that proximal causes of method variance addressed by a one-week temporal separation can explain these relationships for same-source self-report designs. Though the reductions in Study 3 were not as drastic as Study 1, this could be due to using a single remedy (i.e., temporal separation). As study 2 and 3 suggested, when only a single remedy is used, other method effects may still affect one's estimates. 

Regarding how the one-week temporal separation remedy affected the reliability of our measures, it is difficult to see an overarching pattern in our findings for reasons spelled out earlier (i.e., interplay of the method effects that remain). However, what is clear is that a temporal separation of measurement of one-week can address method effects such as mood when it is a contaminant. In our case, this applied to our measure of OCBO. Even though this provides weak support for hypothesis 3, it reinforces arguments by @spectorNewPerspectiveMethod2017 that method variance can be measure-specific. Why mood is a contaminant for self-reports of OCBO as opposed to other measures of performance is an interesting question to investigate further. 

Overall, study 3 provided evidence that a one-week temporal separation of measurement results in effect size estimates that differ from single time-point designs, some of which can be explained by reducing the contaminating role of mood. 

# General Discussion

We contribute to the literature by providing the first experimental evidence regarding the efficacy of procedural remedies for method variance. Our three studies demonstrate the extent to which decisions regarding using proximal remedies for method variance can affect substantive estimates. Our findings for Study 1 and 3 were dramatic in that the remedies weakened the substantive correlations, many of which were statistically indistinguishable from zero. These findings suggest that method variance may explain one's substantive correlations of interest when a same-source self-report design is used. Decomposing the reliability of our measures helped reveal how method variance can play a complex role within a study. 

The results of our investigation speak directly to the efficacy of select procedural remedies for method variance. We find that bundling proximal remedies or introducing a temporal separation of one week, by reducing the influence of method effects, can weaken estimates of substantive correlations. We also found that a temporal separation of one-week can reduce contamination in a measure. Though we did not test a longer time lag, it stands to reason that a longer separation may further reduce if not eliminate contamination from proximal causes (e.g. mood). Of course, researchers should pick a time lag that is relevant for the context in question as excessively long time lags may deleteriously affect the ability to capture substantive relationships. 

The results of our three studies add to the growing literature on method variance and can help social scientists understand the nature and likelihood of this type of error. As noted, these substantive variables were chosen specifically because they are likely to be susceptible to CMV. The findings across the three data sets presented here provide evidence that method effects are likely biasing these relationships in same source data and that procedural remedies can reduce the magnitude of correlations. Results from study 1 and 3 do align well with the extant literature in that researchers should aim to purify their measures of method variance as much as can be feasibly accomplished. Specifically, remedies should be bundled to address as many method variance explanations as possible. 

## Implications for the Proactive Personality Literature

Notably, scholars who study proactive personality may view our findings with great skepticism. They may (we think inaccurately) view our findings as suggesting either that proactive individuals are neither good nor bad workers or that our method variance remedies distort estimates of this relationship. Indeed, without further investigation, it is possible that proximal remedies for method variance such as the ones we examined here might produce a distorted estimate of the substantive correlations of interest (e.g., PP-IRB). Given the wealth of evidence suggesting that proactive personality traits are related to a host of desirable outcomes for organizations [see @FullerJr.Changedrivennature2009], our findings require further clarification. 

First, we do not believe that our results refute the possibility of meaningful correlations between proactive personality and job performance outcomes. Rather, we submit that meaningful correlations between these factors would be more reliably revealed by other designs, such as distinct- or multiple-source designs, with the latter being the least likely to introduce method bias [@coteMeasurementErrorTheory1988; @spectorNewPerspectiveMethod2017; @williamsMethodVarianceOrganizational1994]. We emphasize the difference between distinct- and multi-source designs because the two are often conflated in the literature [see  @FullerJr.Changedrivennature2009]. Distinct-source designs involve gathering measurements on separate factors from distinct sources (e.g., predictor data from employees, outcome data from supervisors), which can introduce uncommon method variance [e.g., potentially impression management, halo and observational biases; see @LanceUseindependentmeasures2015; @spectorNewPerspectiveMethod2017] that weakens substantive estimates. Multi-source designs (i.e., gathering multiple perspectives on both predictor and outcome factors using multiple sources, such as in a 360) should produce more reliable substantive estimates by addressing multiple method effects simultaneously.

Second, it might be argued that our remedied estimates systematically underestimate the effects in question. This reasoning implies that remedied estimates could be less accurate than non-remedied estimates. For this reasoning to be true, non-remedied designs would have to experience a symmetry between method variance and substantive variance [@coteMeasurementErrorTheory1988; @siemsenCommonMethodBias2010; @williamsMethodVarianceOrganizational1994] for method bias is negligible. Alternatively, measurement unreliability would have to play a role [@LanceMethodEffectsMeasurement2010]. Unfortunately, it is not clear how frequently such symmetry occurs in practice and whether or not this occurred here. Also, the conditions that must apply for unreliability to offset method variance remain relatively unknown. 

Though these concerns do highlight a need for deeper thinking regarding how method-substantive symmetry and measure reliability interplay with procedural remedies, we urge those researchers to consider that a coincidence of symmetry or measurement unreliability may be besides the point entirely. While using a design confounded by method variance could produce an estimate that is closer to the true value, it would be better scientific practice to demonstrate this empirically. As procedural remedies for method variance are intended to address *methodological* explanations for a relationship, a simple test is remedying method effects and seeing if the results change. A similar point has been made previously by @SpectorMethodVarianceOrganizational2006, who argued that once researchers identify a correlation between two factors, they should then seek to eliminate remaining explanations for this correlation, such as method effects [see also @antonakisMakingCausalClaims2010]. If a remedied estimate deviates more strongly from a true estimate than a non-remedied estimate, the former should be considered more more credible because the substantive hypothesis in question has undergone a more rigorous test. In the absence of such evidence, however, contaminated estimates can reasonably be viewed as less credible than non-remedied estimates. 

## Limitations and Future Research 

The primary limitation of the studies presented here is the same one that plagues any investigation of method effects in same source data — that the true correlations among the substantive and method variables, and therefore the symmetry among these factors, is relatively unknown. Thus, while we can make assumptions about the degree to which the change in magnitude of correlations among substantive variables is due to a reduction in method variance, we cannot conclude that method variance is the sole difference in these correlations. Indeed, particularly in Study 3, the possibility of the influence of unmeasured variables in the week between independent and dependent variables were measured, cannot be discounted. 

Our second limitation is that we studied only single-source designs. Research utilizing multi-source remedies to study phenomena like those examined here are needed. Such studies would shed light on how method variance affects substantive conclusions of interest. This is particularly relevant for scholars who study the role of proactive personality at work, as our findings call into question conclusions that are based on single-source data that do not involve the use of proximal remedies for method variance.

A final limitation of our studies was that we investigated only mood as a potentially contaminating method effect. In the context of our substantive variables, mood was a nuisance variable, but in other research, could very likely be a meaningful construct. Yet, it is notable that we did not manipulate momentary mood directly to control method variance attributable to mood, so future research should examine such effects and whether or not they are truly contaminants. As mood might facilitate recall [@bowerMoodMemory1981; @isenPositiveAffectFactor1991], inducing individuals to consider “how they typically feel” during work on average might help individuals to more accurately describe their behaviors.

## A Call for Theory Development Regarding Method Variance

We suspect that most researchers devote modest attention to the interplay between method and substantive constructs when conducting a study, seeking rather to use designs, such as distinct-source designs, that appear sufficient to purify estimates of obvious causes of common method variance (e.g., self-report bias). While such designs (compared to self-report designs) should more reliably detect effects if they are present, they will also produce attenuated estimates unless uncommon method variance is addressed [@LanceUseindependentmeasures2015; @spectorNewPerspectiveMethod2017]. Also, there are instances when method variance is ignorable. Simulation work by @siemsenCommonMethodBias2010 suggests that when multiple factors are contaminated by common method variance but examined in a multivariate manner for linear effects (e.g., multiple regression analysis with multiple predictors), CMV impacting the regression slopes can be negligible (i.e., a remedy may not be required in these contexts). However, theorized interaction effects (linear and nonlinear), though they cannot be produced by method variance, could be weakened in such instances (i.e., a remedy may be required in these contexts to produce more reliable effect size estimates). Unless the symmetry between method and substantive constructs within a given design is known, it will remain unclear when method variance is ignorable (so no remedy is required) and when it is not ignorable (and so a remedy is required). 

While it widely agreed upon that scholars take steps to purify one's study of method variance, given that addressing every possible cause of method bias is a gargantuan feat for any single study, research is needed to clarify when impure estimates may or may not be reliable. For a design (e.g., same-source single time-point) that uses only a select few remedies (and so other method effects plausibly remain), researchers will need guidance regarding how method variance could be biasing results. Interestingly, our findings revealed that the procedural remedies, used singly, may produce estimates that are difficult to interpret. This may occur because a single remedy affects the symmetry between method and substantive factors at play in ways that are difficult to identify [see @coteMeasurementErrorTheory1988; @siemsenCommonMethodBias2010; @williamsMethodVarianceOrganizational1994]. 

Seeing this need, we call for the development of a formal theory of method variance that clarifies how procedural remedies for method variance can render method effects negligible or not. There is a vast literature on this topic to draw upon [@coteMeasurementErrorTheory1988; @LanceUseindependentmeasures2015; @PodsakoffCommonmethodbiases2003; @siemsenCommonMethodBias2010; @spectorNewPerspectiveMethod2017; @williamsMethodVarianceOrganizational1994] to synthesize such a formal theory. We see great value in this work because such a theory would resolve the long-standing debate regarding the nature of method variance. As scholars fall into various positions regarding method variance [e.g., it exists but is negligible, it is non-negligible; see @RichardsonTaleThreePerspectives2009], such a theory would explain when (and which) remedies render method bias a plausible alternative explanation as well as when it is ignorable. Adversarial collaborations in developing such a theory would represent a meaningful contribution to social science research writ large.

In sum, more research focused on how remedies affect the interrelation between method and substantive constructs would clarify when remedies are and are not useful as well as which remedies should be used. Such a theory would also be empirically verifiable by showing how results should (or should not) change when a procedural remedy has been applied. 


\newpage

# References
```{r create_r-references}
r_refs(file = "r-references.bib")
```
\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}
